From 3c577ba96bd2a3a34f874c9efc6da5cf2c97c886 Mon Sep 17 00:00:00 2001
From: Shengzhou Liu <Shengzhou.Liu@nxp.com>
Date: Mon, 29 Oct 2018 12:59:22 +0800
Subject: [PATCH] Upgrade arm64 4.9-lts kernel patches

Signed-off-by: Shengzhou Liu <Shengzhou.Liu@nxp.com>
---
 .../configs/arm64-all/arm64-all.config        |    26 +-
 ...DK-18.09-lts-4.9-kernel-patch-to-ONL.patch | 26953 ++++++++++++++++
 .../any/kernels/4.9-lts/patches/series.arm64  |     2 +-
 3 files changed, 26975 insertions(+), 6 deletions(-)
 create mode 100644 packages/base/any/kernels/4.9-lts/patches/0017-Sync-LSDK-18.09-lts-4.9-kernel-patch-to-ONL.patch

diff --git a/packages/base/any/kernels/4.9-lts/configs/arm64-all/arm64-all.config b/packages/base/any/kernels/4.9-lts/configs/arm64-all/arm64-all.config
index ed4d9cab..389aaa6d 100644
--- a/packages/base/any/kernels/4.9-lts/configs/arm64-all/arm64-all.config
+++ b/packages/base/any/kernels/4.9-lts/configs/arm64-all/arm64-all.config
@@ -37,6 +37,7 @@ CONFIG_PGTABLE_LEVELS=4
 CONFIG_DEFCONFIG_LIST="/lib/modules/$UNAME_RELEASE/.config"
 CONFIG_IRQ_WORK=y
 CONFIG_BUILDTIME_EXTABLE_SORT=y
+CONFIG_THREAD_INFO_IN_TASK=y
 
 #
 # General setup
@@ -78,6 +79,7 @@ CONFIG_HANDLE_DOMAIN_IRQ=y
 # CONFIG_IRQ_DOMAIN_DEBUG is not set
 CONFIG_IRQ_FORCED_THREADING=y
 CONFIG_SPARSE_IRQ=y
+# CONFIG_GENERIC_IRQ_DEBUGFS is not set
 CONFIG_ARCH_CLOCKSOURCE_DATA=y
 CONFIG_GENERIC_TIME_VSYSCALL=y
 CONFIG_GENERIC_CLOCKEVENTS=y
@@ -140,7 +142,7 @@ CONFIG_FAIR_GROUP_SCHED=y
 # CONFIG_CFS_BANDWIDTH is not set
 # CONFIG_RT_GROUP_SCHED is not set
 CONFIG_CGROUP_PIDS=y
-# CONFIG_CGROUP_FREEZER is not set
+CONFIG_CGROUP_FREEZER=y
 CONFIG_CGROUP_HUGETLB=y
 CONFIG_CPUSETS=y
 CONFIG_PROC_PID_CPUSET=y
@@ -426,6 +428,7 @@ CONFIG_ARM64_ERRATUM_832075=y
 CONFIG_ARM64_ERRATUM_834220=y
 CONFIG_ARM64_ERRATUM_845719=y
 CONFIG_ARM64_ERRATUM_843419=y
+CONFIG_ARM64_ERRATUM_1024718=y
 CONFIG_CAVIUM_ERRATUM_22375=y
 CONFIG_CAVIUM_ERRATUM_23144=y
 CONFIG_CAVIUM_ERRATUM_23154=y
@@ -509,10 +512,15 @@ CONFIG_SECCOMP=y
 CONFIG_PARAVIRT=y
 # CONFIG_PARAVIRT_TIME_ACCOUNTING is not set
 CONFIG_KEXEC=y
+CONFIG_CRASH_DUMP=y
 CONFIG_XEN_DOM0=y
 CONFIG_XEN=y
 CONFIG_FORCE_MAX_ZONEORDER=11
+CONFIG_UNMAP_KERNEL_AT_EL0=y
+CONFIG_HARDEN_BRANCH_PREDICTOR=y
+CONFIG_ARM64_SSBD=y
 # CONFIG_ARMV8_DEPRECATED is not set
+# CONFIG_ARM64_SW_TTBR0_PAN is not set
 
 #
 # ARMv8.1 architectural features
@@ -630,6 +638,7 @@ CONFIG_ARM_BIG_LITTLE_CPUFREQ=y
 CONFIG_ARM_TEGRA20_CPUFREQ=y
 CONFIG_ARM_TEGRA124_CPUFREQ=y
 # CONFIG_ACPI_CPPC_CPUFREQ is not set
+CONFIG_QORIQ_CPUFREQ=y
 CONFIG_NET=y
 CONFIG_NET_INGRESS=y
 
@@ -983,7 +992,7 @@ CONFIG_DNS_RESOLVER=y
 CONFIG_NETLINK_DIAG=y
 # CONFIG_MPLS is not set
 # CONFIG_HSR is not set
-# CONFIG_NET_SWITCHDEV is not set
+CONFIG_NET_SWITCHDEV=y
 # CONFIG_NET_L3_MASTER_DEV is not set
 # CONFIG_QRTR is not set
 # CONFIG_NET_NCSI is not set
@@ -1096,6 +1105,8 @@ CONFIG_CMA_ALIGNMENT=8
 # CONFIG_TEGRA_ACONNECT is not set
 CONFIG_UNIPHIER_SYSTEM_BUS=y
 CONFIG_VEXPRESS_CONFIG=y
+CONFIG_FSL_MC_BUS=y
+CONFIG_FSL_MC_RESTOOL=y
 # CONFIG_CONNECTOR is not set
 CONFIG_MTD=y
 # CONFIG_MTD_TESTS is not set
@@ -1771,6 +1782,7 @@ CONFIG_RAVB=y
 CONFIG_NET_VENDOR_RDC=y
 # CONFIG_R6040 is not set
 CONFIG_NET_VENDOR_ROCKER=y
+# CONFIG_ROCKER is not set
 CONFIG_NET_VENDOR_SAMSUNG=y
 # CONFIG_SXGBE_ETH is not set
 CONFIG_NET_VENDOR_SEEQ=y
@@ -2489,7 +2501,8 @@ CONFIG_GPIO_MAX77620=y
 # USB GPIO expanders
 #
 # CONFIG_W1 is not set
-# CONFIG_POWER_AVS is not set
+CONFIG_POWER_AVS=y
+CONFIG_ROCKCHIP_IODOMAIN=y
 CONFIG_POWER_RESET=y
 # CONFIG_POWER_RESET_GPIO is not set
 # CONFIG_POWER_RESET_GPIO_RESTART is not set
@@ -4047,16 +4060,16 @@ CONFIG_STAGING=y
 # CONFIG_GS_FPGABOOT is not set
 # CONFIG_COMMON_CLK_XLNX_CLKWZRD is not set
 # CONFIG_FB_TFT is not set
-CONFIG_FSL_MC_BUS=y
 CONFIG_FSL_MC_DPIO=y
 # CONFIG_FSL_QBMAN_DEBUG is not set
-CONFIG_FSL_MC_RESTOOL=y
 CONFIG_FSL_DPAA2=y
 CONFIG_FSL_DPAA2_ETH=y
 # CONFIG_FSL_DPAA2_ETH_USE_ERR_QUEUE is not set
+CONFIG_FSL_DPAA2_ETH_DEBUGFS=y
 CONFIG_FSL_DPAA2_MAC=y
 # CONFIG_FSL_DPAA2_MAC_NETDEVS is not set
 CONFIG_FSL_DPAA2_EVB=y
+CONFIG_FSL_DPAA2_ETHSW=y
 # CONFIG_MOST is not set
 # CONFIG_KS7010 is not set
 # CONFIG_GREYBUS is not set
@@ -4562,6 +4575,7 @@ CONFIG_ARM_GIC_MAX_NR=1
 CONFIG_ARM_GIC_V2M=y
 CONFIG_ARM_GIC_V3=y
 CONFIG_ARM_GIC_V3_ITS=y
+CONFIG_ARM_GIC_V3_ITS_FSL_MC=y
 CONFIG_ALPINE_MSI=y
 CONFIG_DW_APB_ICTL=y
 CONFIG_HISILICON_IRQ_MBIGEN=y
@@ -4793,6 +4807,7 @@ CONFIG_FAT_DEFAULT_IOCHARSET="iso8859-1"
 #
 CONFIG_PROC_FS=y
 # CONFIG_PROC_KCORE is not set
+CONFIG_PROC_VMCORE=y
 CONFIG_PROC_SYSCTL=y
 CONFIG_PROC_PAGE_MONITOR=y
 CONFIG_PROC_CHILDREN=y
@@ -5127,6 +5142,7 @@ CONFIG_KEYS=y
 CONFIG_KEYS_COMPAT=y
 # CONFIG_PERSISTENT_KEYRINGS is not set
 # CONFIG_BIG_KEYS is not set
+# CONFIG_SECURE_KEYS is not set
 # CONFIG_ENCRYPTED_KEYS is not set
 # CONFIG_KEY_DH_OPERATIONS is not set
 # CONFIG_SECURITY_DMESG_RESTRICT is not set
diff --git a/packages/base/any/kernels/4.9-lts/patches/0017-Sync-LSDK-18.09-lts-4.9-kernel-patch-to-ONL.patch b/packages/base/any/kernels/4.9-lts/patches/0017-Sync-LSDK-18.09-lts-4.9-kernel-patch-to-ONL.patch
new file mode 100644
index 00000000..cbbbfa2a
--- /dev/null
+++ b/packages/base/any/kernels/4.9-lts/patches/0017-Sync-LSDK-18.09-lts-4.9-kernel-patch-to-ONL.patch
@@ -0,0 +1,26953 @@
+From e12249ba890785dc893ae6ee9f68fed995c1548c Mon Sep 17 00:00:00 2001
+From: Shengzhou Liu <Shengzhou.Liu@nxp.com>
+Date: Sat, 27 Oct 2018 11:50:12 +0200
+Subject: [PATCH] Sync LSDK-18.09 lts-4.9 kernel patch to ONL
+
+---
+ arch/arm/include/asm/kvm_mmu.h                |   41 +-
+ .../arm64/boot/dts/freescale/fsl-ls1012a.dtsi |   43 +-
+ .../boot/dts/freescale/fsl-ls1043-post.dtsi   |    3 +-
+ .../dts/freescale/fsl-ls1043a-rdb-sdk.dts     |    2 +
+ .../boot/dts/freescale/fsl-ls1043a-rdb.dts    |   39 +-
+ .../arm64/boot/dts/freescale/fsl-ls1043a.dtsi |   47 +-
+ .../boot/dts/freescale/fsl-ls1046-post.dtsi   |    2 +-
+ .../dts/freescale/fsl-ls1046a-rdb-sdk.dts     |   32 +
+ .../boot/dts/freescale/fsl-ls1046a-rdb.dts    |   39 +-
+ .../arm64/boot/dts/freescale/fsl-ls1046a.dtsi |   54 +-
+ .../boot/dts/freescale/fsl-ls1088a-rdb.dts    |   38 +-
+ .../arm64/boot/dts/freescale/fsl-ls1088a.dtsi |  224 +--
+ .../boot/dts/freescale/fsl-ls2080a-rdb.dts    |   38 +-
+ .../arm64/boot/dts/freescale/fsl-ls2080a.dtsi |   38 +-
+ .../boot/dts/freescale/fsl-ls2088a-rdb.dts    |   38 +-
+ .../arm64/boot/dts/freescale/fsl-ls2088a.dtsi |   38 +-
+ .../boot/dts/freescale/fsl-ls208xa-qds.dtsi   |   38 +-
+ .../boot/dts/freescale/fsl-ls208xa-rdb.dtsi   |   38 +-
+ .../arm64/boot/dts/freescale/fsl-ls208xa.dtsi |   44 +-
+ .../dts/freescale/qoriq-bman-portals-sdk.dtsi |   55 +
+ .../dts/freescale/qoriq-bman-portals.dtsi     |   77 +
+ .../boot/dts/freescale/qoriq-dpaa-eth.dtsi    |   24 +
+ .../dts/freescale/qoriq-fman3-0-10g-0.dtsi    |    6 +-
+ .../dts/freescale/qoriq-fman3-0-10g-1.dtsi    |    6 +-
+ .../dts/freescale/qoriq-fman3-0-1g-0.dtsi     |    2 +-
+ .../dts/freescale/qoriq-fman3-0-1g-1.dtsi     |    2 +-
+ .../dts/freescale/qoriq-fman3-0-1g-2.dtsi     |    2 +-
+ .../dts/freescale/qoriq-fman3-0-1g-3.dtsi     |    2 +-
+ .../dts/freescale/qoriq-fman3-0-1g-4.dtsi     |    2 +-
+ .../dts/freescale/qoriq-fman3-0-1g-5.dtsi     |    2 +-
+ .../boot/dts/freescale/qoriq-fman3-0.dtsi     |    2 +-
+ .../dts/freescale/qoriq-qman-portals-sdk.dtsi |   38 +
+ .../dts/freescale/qoriq-qman-portals.dtsi     |   87 ++
+ arch/arm64/mm/dma-mapping.c                   |    2 +-
+ crypto/tcrypt.c                               |   18 +-
+ drivers/bus/Kconfig                           |    3 +
+ drivers/bus/Makefile                          |    4 +
+ drivers/bus/fsl-mc/Kconfig                    |   23 +
+ drivers/bus/fsl-mc/Makefile                   |   22 +
+ drivers/bus/fsl-mc/dpbp.c                     |  186 +++
+ drivers/bus/fsl-mc/dpcon.c                    |  222 +++
+ drivers/bus/fsl-mc/dpmcp.c                    |   99 ++
+ drivers/bus/fsl-mc/dprc-driver.c              |  815 +++++++++++
+ drivers/bus/fsl-mc/dprc.c                     |  575 ++++++++
+ drivers/bus/fsl-mc/fsl-mc-allocator.c         |  655 +++++++++
+ drivers/bus/fsl-mc/fsl-mc-bus.c               | 1151 +++++++++++++++
+ drivers/bus/fsl-mc/fsl-mc-iommu.c             |   78 +
+ drivers/bus/fsl-mc/fsl-mc-msi.c               |  285 ++++
+ drivers/bus/fsl-mc/fsl-mc-private.h           |  223 +++
+ drivers/bus/fsl-mc/fsl-mc-restool.c           |  219 +++
+ drivers/bus/fsl-mc/mc-io.c                    |  268 ++++
+ drivers/bus/fsl-mc/mc-sys.c                   |  296 ++++
+ drivers/crypto/caam/caamalg_desc.c            |   80 +-
+ drivers/crypto/caam/caamalg_qi.c              |   28 +-
+ drivers/crypto/caam/caamalg_qi2.c             |  879 +++++-------
+ drivers/crypto/caam/caamalg_qi2.h             |   27 +-
+ drivers/crypto/caam/caampkc.c                 |   20 +-
+ drivers/crypto/caam/compat.h                  |    1 +
+ drivers/crypto/caam/ctrl.c                    |    8 +-
+ drivers/crypto/caam/dpseci.c                  |   78 +-
+ drivers/crypto/caam/dpseci.h                  |   44 +-
+ drivers/crypto/caam/dpseci_cmd.h              |  100 +-
+ drivers/crypto/caam/jr.c                      |    3 +-
+ drivers/crypto/caam/qi.c                      |    2 +-
+ drivers/dma/dpaa2-qdma/dpaa2-qdma.c           |   78 +-
+ drivers/dma/dpaa2-qdma/dpaa2-qdma.h           |   47 +-
+ drivers/dma/dpaa2-qdma/dpdmai.c               |  123 +-
+ drivers/iommu/arm-smmu.c                      |    2 +-
+ drivers/iommu/iommu.c                         |   21 +
+ drivers/mmc/host/sdhci-of-esdhc.c             |   85 +-
+ .../net/ethernet/freescale/dpaa/dpaa_eth.c    |  300 ++--
+ .../net/ethernet/freescale/dpaa/dpaa_eth.h    |    2 +
+ .../ethernet/freescale/dpaa/dpaa_eth_sysfs.c  |    3 +
+ .../ethernet/freescale/dpaa/dpaa_ethtool.c    |  122 +-
+ drivers/net/ethernet/freescale/fec_main.c     |   32 +-
+ drivers/net/ethernet/freescale/fman/Makefile  |   12 +-
+ drivers/net/ethernet/freescale/fman/fman.c    |  102 +-
+ drivers/net/ethernet/freescale/fman/fman.h    |   77 +
+ .../net/ethernet/freescale/fman/fman_dtsec.c  |  139 +-
+ .../net/ethernet/freescale/fman/fman_dtsec.h  |    1 +
+ .../net/ethernet/freescale/fman/fman_keygen.c |  783 ++++++++++
+ .../net/ethernet/freescale/fman/fman_keygen.h |   46 +
+ .../net/ethernet/freescale/fman/fman_memac.c  |   32 +-
+ .../net/ethernet/freescale/fman/fman_memac.h  |    1 +
+ .../net/ethernet/freescale/fman/fman_port.c   |   69 +-
+ .../net/ethernet/freescale/fman/fman_port.h   |    7 +
+ .../net/ethernet/freescale/fman/fman_tgec.c   |   33 +-
+ .../net/ethernet/freescale/fman/fman_tgec.h   |    1 +
+ drivers/net/ethernet/freescale/fman/mac.c     |  151 +-
+ drivers/net/ethernet/freescale/fman/mac.h     |    8 +-
+ drivers/net/ethernet/freescale/fsl_pq_mdio.c  |    9 +-
+ drivers/net/ethernet/freescale/gianfar.c      |   22 +-
+ drivers/net/ethernet/freescale/gianfar_ptp.c  |    3 +-
+ .../net/ethernet/freescale/sdk_dpaa/Kconfig   |    1 -
+ .../ethernet/freescale/sdk_dpaa/dpaa_eth.c    |    8 +-
+ .../ethernet/freescale/sdk_dpaa/dpaa_eth.h    |    8 +-
+ .../freescale/sdk_dpaa/dpaa_eth_ceetm.c       |  178 ++-
+ .../freescale/sdk_dpaa/dpaa_eth_ceetm.h       |    4 +-
+ .../freescale/sdk_dpaa/dpaa_eth_common.c      |    9 -
+ .../freescale/sdk_dpaa/dpaa_eth_common.h      |    6 +-
+ .../ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c |   87 +-
+ .../net/ethernet/freescale/sdk_dpaa/mac-api.c |   38 +-
+ drivers/net/ethernet/freescale/sdk_dpaa/mac.c |    4 +-
+ .../sdk_fman/Peripherals/FM/MAC/dtsec.c       |   57 +-
+ .../sdk_fman/src/wrapper/lnxwrp_fm.c          |    6 +-
+ .../sdk_fman/src/wrapper/lnxwrp_fm_port.c     |   44 +-
+ drivers/net/ethernet/freescale/ucc_geth.c     |    8 +-
+ drivers/pci/quirks.c                          |   72 +-
+ drivers/soc/fsl/layerscape/ftm_alarm.c        |    9 +-
+ drivers/staging/fsl-dpaa2/Kconfig             |   11 +-
+ drivers/staging/fsl-dpaa2/ethernet/Makefile   |    1 +
+ drivers/staging/fsl-dpaa2/ethernet/TODO       |   18 +
+ .../fsl-dpaa2/ethernet/dpaa2-eth-ceetm.c      | 1253 +++++++++++++++++
+ .../fsl-dpaa2/ethernet/dpaa2-eth-ceetm.h      |  182 +++
+ .../fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c    |    6 +-
+ .../fsl-dpaa2/ethernet/dpaa2-eth-trace.h      |    1 +
+ .../staging/fsl-dpaa2/ethernet/dpaa2-eth.c    | 1212 ++++++++--------
+ .../staging/fsl-dpaa2/ethernet/dpaa2-eth.h    |  196 ++-
+ .../fsl-dpaa2/ethernet/dpaa2-ethtool.c        |  237 ++--
+ drivers/staging/fsl-dpaa2/ethernet/dpni-cmd.h |   70 +-
+ drivers/staging/fsl-dpaa2/ethernet/dpni.c     |  315 ++++-
+ drivers/staging/fsl-dpaa2/ethernet/dpni.h     |  195 ++-
+ drivers/staging/fsl-dpaa2/ethsw/Makefile      |    3 +
+ drivers/staging/fsl-dpaa2/ethsw/dpsw-cmd.h    |   48 +-
+ drivers/staging/fsl-dpaa2/ethsw/dpsw.c        |  155 +-
+ drivers/staging/fsl-dpaa2/ethsw/dpsw.h        |   41 +-
+ drivers/staging/fsl-dpaa2/ethsw/ethsw.c       |   45 +-
+ drivers/staging/fsl-dpaa2/evb/dpdmux.c        |   63 +-
+ drivers/staging/fsl-dpaa2/evb/evb.c           |   13 +-
+ drivers/staging/fsl-dpaa2/mac/dpmac.c         |   35 +-
+ drivers/staging/fsl-dpaa2/mac/mac.c           |   29 +-
+ drivers/staging/fsl-dpaa2/rtc/dprtc.c         |   48 +-
+ drivers/staging/fsl-dpaa2/rtc/rtc.c           |    3 +-
+ drivers/staging/fsl-mc/Kconfig                |    1 +
+ drivers/staging/fsl-mc/Makefile               |    1 +
+ drivers/staging/fsl-mc/bus/Kconfig            |   22 +-
+ drivers/staging/fsl-mc/bus/Makefile           |   21 +-
+ drivers/staging/fsl-mc/bus/dpio/Makefile      |    3 +-
+ drivers/staging/fsl-mc/bus/dpio/dpio-cmd.h    |   28 +-
+ drivers/staging/fsl-mc/bus/dpio/dpio-driver.c |   38 +-
+ .../staging/fsl-mc/bus/dpio/dpio-service.c    |  171 ++-
+ drivers/staging/fsl-mc/bus/dpio/dpio.c        |   46 +-
+ drivers/staging/fsl-mc/bus/dpio/dpio.h        |   28 +-
+ .../staging/fsl-mc/bus/dpio/qbman-portal.c    |  110 +-
+ .../staging/fsl-mc/bus/dpio/qbman-portal.h    |   54 +-
+ drivers/staging/fsl-mc/include/dpaa2-fd.h     |   33 +-
+ drivers/staging/fsl-mc/include/dpaa2-global.h |   27 +-
+ drivers/staging/fsl-mc/include/dpaa2-io.h     |   41 +-
+ drivers/staging/fsl-mc/include/dpopr.h        |    2 +
+ drivers/staging/fsl_qbman/fsl_usdpaa_irq.c    |    2 +-
+ drivers/staging/fsl_qbman/qman_high.c         |   29 +-
+ drivers/staging/fsl_qbman/qman_low.h          |    5 +-
+ drivers/usb/dwc3/dwc3-pci.c                   |    5 +-
+ drivers/vfio/fsl-mc/vfio_fsl_mc.c             |   15 +-
+ drivers/vfio/fsl-mc/vfio_fsl_mc_intr.c        |    2 +-
+ include/keys/secure-type.h                    |   33 +
+ include/kvm/arm_psci.h                        |   63 +
+ include/linux/fsl/guts.h                      |  126 +-
+ include/linux/fsl/mc.h                        | 1025 ++++++++++++++
+ include/linux/fsl/svr.h                       |   97 ++
+ include/linux/iommu.h                         |    2 +
+ include/linux/irqdomain.h                     |   13 +-
+ include/net/switchdev.h                       |    8 +-
+ include/uapi/linux/fsl_mc.h                   |   31 +
+ kernel/irq/irqdomain.c                        |  171 ++-
+ kernel/irq/msi.c                              |    6 +-
+ net/bridge/br.c                               |    4 +-
+ security/keys/Kconfig                         |   12 +
+ security/keys/Makefile                        |    5 +
+ security/keys/encrypted-keys/Makefile         |    2 +
+ security/keys/encrypted-keys/encrypted.c      |   44 +-
+ security/keys/encrypted-keys/encrypted.h      |   13 +
+ .../keys/encrypted-keys/masterkey_secure.c    |   37 +
+ security/keys/secure_key.c                    |  339 +++++
+ security/keys/securekey_desc.c                |  608 ++++++++
+ security/keys/securekey_desc.h                |  141 ++
+ 176 files changed, 14388 insertions(+), 3663 deletions(-)
+ create mode 100644 arch/arm64/boot/dts/freescale/qoriq-bman-portals-sdk.dtsi
+ create mode 100644 arch/arm64/boot/dts/freescale/qoriq-bman-portals.dtsi
+ create mode 100644 arch/arm64/boot/dts/freescale/qoriq-qman-portals-sdk.dtsi
+ create mode 100644 arch/arm64/boot/dts/freescale/qoriq-qman-portals.dtsi
+ create mode 100644 drivers/bus/fsl-mc/Kconfig
+ create mode 100644 drivers/bus/fsl-mc/Makefile
+ create mode 100644 drivers/bus/fsl-mc/dpbp.c
+ create mode 100644 drivers/bus/fsl-mc/dpcon.c
+ create mode 100644 drivers/bus/fsl-mc/dpmcp.c
+ create mode 100644 drivers/bus/fsl-mc/dprc-driver.c
+ create mode 100644 drivers/bus/fsl-mc/dprc.c
+ create mode 100644 drivers/bus/fsl-mc/fsl-mc-allocator.c
+ create mode 100644 drivers/bus/fsl-mc/fsl-mc-bus.c
+ create mode 100644 drivers/bus/fsl-mc/fsl-mc-iommu.c
+ create mode 100644 drivers/bus/fsl-mc/fsl-mc-msi.c
+ create mode 100644 drivers/bus/fsl-mc/fsl-mc-private.h
+ create mode 100644 drivers/bus/fsl-mc/fsl-mc-restool.c
+ create mode 100644 drivers/bus/fsl-mc/mc-io.c
+ create mode 100644 drivers/bus/fsl-mc/mc-sys.c
+ create mode 100644 drivers/net/ethernet/freescale/fman/fman_keygen.c
+ create mode 100644 drivers/net/ethernet/freescale/fman/fman_keygen.h
+ create mode 100644 drivers/staging/fsl-dpaa2/ethernet/TODO
+ create mode 100644 drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-ceetm.c
+ create mode 100644 drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-ceetm.h
+ create mode 100644 include/keys/secure-type.h
+ create mode 100644 include/kvm/arm_psci.h
+ create mode 100644 include/linux/fsl/mc.h
+ create mode 100644 include/linux/fsl/svr.h
+ create mode 100644 include/uapi/linux/fsl_mc.h
+ create mode 100644 security/keys/encrypted-keys/masterkey_secure.c
+ create mode 100644 security/keys/secure_key.c
+ create mode 100644 security/keys/securekey_desc.c
+ create mode 100644 security/keys/securekey_desc.h
+
+diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h
+index a58bbaa3..99406b80 100644
+--- a/arch/arm/include/asm/kvm_mmu.h
++++ b/arch/arm/include/asm/kvm_mmu.h
+@@ -28,6 +28,13 @@
+  */
+ #define kern_hyp_va(kva)	(kva)
+ 
++/* Contrary to arm64, there is no need to generate a PC-relative address */
++#define hyp_symbol_addr(s)						\
++	({								\
++		typeof(s) *addr = &(s);					\
++		addr;							\
++	})
++
+ /*
+  * KVM_MMU_CACHE_MIN_PAGES is the number of stage2 page table translation levels.
+  */
+@@ -48,7 +55,8 @@ void stage2_unmap_vm(struct kvm *kvm);
+ int kvm_alloc_stage2_pgd(struct kvm *kvm);
+ void kvm_free_stage2_pgd(struct kvm *kvm);
+ int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,
+-			  phys_addr_t pa, unsigned long size, bool writable);
++			  phys_addr_t pa, unsigned long size, bool writable,
++			  pgprot_t prot);
+ 
+ int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run);
+ 
+@@ -223,6 +231,37 @@ static inline unsigned int kvm_get_vmid_bits(void)
+ 	return 8;
+ }
+ 
++/*
++ * We are not in the kvm->srcu critical section most of the time, so we take
++ * the SRCU read lock here. Since we copy the data from the user page, we
++ * can immediately drop the lock again.
++ */
++static inline int kvm_read_guest_lock(struct kvm *kvm,
++				      gpa_t gpa, void *data, unsigned long len)
++{
++	int srcu_idx = srcu_read_lock(&kvm->srcu);
++	int ret = kvm_read_guest(kvm, gpa, data, len);
++
++	srcu_read_unlock(&kvm->srcu, srcu_idx);
++
++	return ret;
++}
++
++static inline void *kvm_get_hyp_vector(void)
++{
++	return kvm_ksym_ref(__kvm_hyp_vector);
++}
++
++static inline int kvm_map_vectors(void)
++{
++	return 0;
++}
++
++static inline int hyp_map_aux_data(void)
++{
++	return 0;
++}
++
+ #endif	/* !__ASSEMBLY__ */
+ 
+ #endif /* __ARM_KVM_MMU_H__ */
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1012a.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls1012a.dtsi
+index 071fa88b..2fe44324 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1012a.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1012a.dtsi
+@@ -1,45 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for Freescale Layerscape-1012A family SoC.
+  *
+  * Copyright 2016 Freescale Semiconductor, Inc.
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ #include <dt-bindings/interrupt-controller/arm-gic.h>
+@@ -246,7 +210,7 @@
+ 		dcfg: dcfg@1ee0000 {
+ 			compatible = "fsl,ls1012a-dcfg",
+ 				     "syscon";
+-			reg = <0x0 0x1ee0000 0x0 0x10000>;
++			reg = <0x0 0x1ee0000 0x0 0x1000>;
+ 			big-endian;
+ 		};
+ 
+@@ -375,12 +339,13 @@
+ 		};
+ 
+ 		i2c0: i2c@2180000 {
+-			compatible = "fsl,vf610-i2c";
++			compatible = "fsl,vf610-i2c", "fsl,ls1012a-vf610-i2c";
+ 			#address-cells = <1>;
+ 			#size-cells = <0>;
+ 			reg = <0x0 0x2180000 0x0 0x10000>;
+ 			interrupts = <0 56 IRQ_TYPE_LEVEL_HIGH>;
+ 			clocks = <&clockgen 4 3>;
++			fsl-scl-gpio = <&gpio0 13 0>;
+ 			status = "disabled";
+ 		};
+ 
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1043-post.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls1043-post.dtsi
+index 169e1714..6082ae02 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1043-post.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1043-post.dtsi
+@@ -1,9 +1,8 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 device tree nodes for ls1043
+  *
+  * Copyright 2015-2016 Freescale Semiconductor Inc.
+- *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ &soc {
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1043a-rdb-sdk.dts b/arch/arm64/boot/dts/freescale/fsl-ls1043a-rdb-sdk.dts
+index ac4b9a41..d9248da2 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1043a-rdb-sdk.dts
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1043a-rdb-sdk.dts
+@@ -45,6 +45,8 @@
+  */
+ 
+ #include "fsl-ls1043a-rdb.dts"
++#include "qoriq-qman-portals-sdk.dtsi"
++#include "qoriq-bman-portals-sdk.dtsi"
+ 
+ &bman_fbpr {
+ 	compatible = "fsl,bman-fbpr";
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1043a-rdb.dts b/arch/arm64/boot/dts/freescale/fsl-ls1043a-rdb.dts
+index f92ae325..3d09566d 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1043a-rdb.dts
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1043a-rdb.dts
+@@ -1,47 +1,10 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for Freescale Layerscape-1043A family SoC.
+  *
+  * Copyright 2014-2015 Freescale Semiconductor, Inc.
+  *
+  * Mingkai Hu <Mingkai.hu@freescale.com>
+- *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ /dts-v1/;
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1043a.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls1043a.dtsi
+index 4a9566e1..f4643725 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1043a.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1043a.dtsi
+@@ -1,50 +1,14 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for Freescale Layerscape-1043A family SoC.
+  *
+  * Copyright 2014-2015 Freescale Semiconductor, Inc.
+  *
+  * Mingkai Hu <Mingkai.hu@freescale.com>
+- *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ #include <dt-bindings/thermal/thermal.h>
++#include <dt-bindings/interrupt-controller/arm-gic.h>
+ 
+ / {
+ 	compatible = "fsl,ls1043a";
+@@ -442,7 +406,7 @@
+ 		};
+ 
+ 		i2c0: i2c@2180000 {
+-			compatible = "fsl,vf610-i2c";
++			compatible = "fsl,vf610-i2c", "fsl,ls1043a-vf610-i2c";
+ 			#address-cells = <1>;
+ 			#size-cells = <0>;
+ 			reg = <0x0 0x2180000 0x0 0x10000>;
+@@ -452,6 +416,7 @@
+ 			dmas = <&edma0 1 39>,
+ 			       <&edma0 1 38>;
+ 			dma-names = "tx", "rx";
++			fsl-scl-gpio = <&gpio4 12 0>;
+ 			status = "disabled";
+ 		};
+ 
+@@ -882,5 +847,5 @@
+ 	};
+ };
+ 
+-#include "qoriq-qman1-portals.dtsi"
+-#include "qoriq-bman1-portals.dtsi"
++#include "qoriq-qman-portals.dtsi"
++#include "qoriq-bman-portals.dtsi"
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1046-post.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls1046-post.dtsi
+index f5017dba..d6caaea5 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1046-post.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1046-post.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 device tree nodes for ls1046
+  *
+  * Copyright 2015-2016 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ &soc {
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1046a-rdb-sdk.dts b/arch/arm64/boot/dts/freescale/fsl-ls1046a-rdb-sdk.dts
+index f84e49ba..06a7a8df 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1046a-rdb-sdk.dts
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1046a-rdb-sdk.dts
+@@ -45,6 +45,8 @@
+  */
+ 
+ #include "fsl-ls1046a-rdb.dts"
++#include "qoriq-qman-portals-sdk.dtsi"
++#include "qoriq-bman-portals-sdk.dtsi"
+ 
+ &bman_fbpr {
+ 	compatible = "fsl,bman-fbpr";
+@@ -81,3 +83,33 @@
+ &fman0 {
+ 	compatible = "fsl,fman", "simple-bus";
+ };
++
++&mdio9 {
++	pcsphy6: ethernet-phy@0 {
++		backplane-mode = "10gbase-kr";
++		compatible = "ethernet-phy-ieee802.3-c45";
++		reg = <0x0>;
++		fsl,lane-handle = <&serdes1>;
++		fsl,lane-reg = <0x8C0 0x40>;   /* lane D */
++	};
++};
++
++&mdio10 {
++	pcsphy7: ethernet-phy@0 {
++		backplane-mode = "10gbase-kr";
++		compatible = "ethernet-phy-ieee802.3-c45";
++		reg = <0x0>;
++		fsl,lane-handle = <&serdes1>;
++		fsl,lane-reg = <0x880 0x40>;   /* lane C */
++	};
++};
++
++/* Update MAC connections to backplane PHYs
++ * &mac9 {
++ *	phy-handle = <&pcsphy6>;
++ *};
++ *
++ *&mac10 {
++ *	phy-handle = <&pcsphy7>;
++ *};
++*/
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1046a-rdb.dts b/arch/arm64/boot/dts/freescale/fsl-ls1046a-rdb.dts
+index be9b62ca..c8c07a3e 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1046a-rdb.dts
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1046a-rdb.dts
+@@ -1,47 +1,10 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for Freescale Layerscape-1046A family SoC.
+  *
+  * Copyright 2016 Freescale Semiconductor, Inc.
+  *
+  * Mingkai Hu <mingkai.hu@nxp.com>
+- *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ /dts-v1/;
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1046a.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls1046a.dtsi
+index 83f2c386..ad96eab7 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1046a.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1046a.dtsi
+@@ -1,47 +1,10 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for Freescale Layerscape-1046A family SoC.
+  *
+  * Copyright 2016 Freescale Semiconductor, Inc.
+  *
+  * Mingkai Hu <mingkai.hu@nxp.com>
+- *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ #include <dt-bindings/interrupt-controller/arm-gic.h>
+@@ -407,7 +370,7 @@
+ 		};
+ 
+ 		i2c0: i2c@2180000 {
+-			compatible = "fsl,vf610-i2c";
++			compatible = "fsl,vf610-i2c", "fsl,ls1046a-vf610-i2c";
+ 			#address-cells = <1>;
+ 			#size-cells = <0>;
+ 			reg = <0x0 0x2180000 0x0 0x10000>;
+@@ -416,6 +379,7 @@
+ 			dmas = <&edma0 1 39>,
+ 			       <&edma0 1 38>;
+ 			dma-names = "tx", "rx";
++			fsl-scl-gpio = <&gpio3 12 0>;
+ 			status = "disabled";
+ 		};
+ 
+@@ -440,12 +404,13 @@
+ 		};
+ 
+ 		i2c3: i2c@21b0000 {
+-			compatible = "fsl,vf610-i2c";
++			compatible = "fsl,vf610-i2c", "fsl,ls1046a-vf610-i2c";
+ 			#address-cells = <1>;
+ 			#size-cells = <0>;
+ 			reg = <0x0 0x21b0000 0x0 0x10000>;
+ 			interrupts = <GIC_SPI 59 IRQ_TYPE_LEVEL_HIGH>;
+ 			clocks = <&clockgen 4 1>;
++			fsl-scl-gpio = <&gpio3 12 0>;
+ 			status = "disabled";
+ 		};
+ 
+@@ -767,6 +732,11 @@
+ 					<0000 0 0 4 &gic GIC_SPI 154 IRQ_TYPE_LEVEL_HIGH>;
+ 		};
+ 
++		serdes1: serdes@1ea0000 {
++			reg = <0x0 0x1ea0000 0 0x00002000>;
++			compatible = "fsl,serdes-10g";
++		};
++
+ 	};
+ 
+ 	reserved-memory {
+@@ -802,5 +772,5 @@
+ 	};
+ };
+ 
+-#include "qoriq-qman1-portals.dtsi"
+-#include "qoriq-bman1-portals.dtsi"
++#include "qoriq-qman-portals.dtsi"
++#include "qoriq-bman-portals.dtsi"
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1088a-rdb.dts b/arch/arm64/boot/dts/freescale/fsl-ls1088a-rdb.dts
+index a4cbc2d5..f563c159 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1088a-rdb.dts
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1088a-rdb.dts
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree file for NXP LS1088A RDB Board.
+  *
+@@ -5,43 +6,6 @@
+  *
+  * Harninder Rai <harninder.rai@nxp.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ /dts-v1/;
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls1088a.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls1088a.dtsi
+index 9aa44835..037418ba 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls1088a.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls1088a.dtsi
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for NXP Layerscape-1088A family SoC.
+  *
+@@ -5,43 +6,6 @@
+  *
+  * Harninder Rai <harninder.rai@nxp.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ #include <dt-bindings/interrupt-controller/arm-gic.h>
+ #include <dt-bindings/thermal/thermal.h>
+@@ -174,70 +138,6 @@
+ 			     <1 10 IRQ_TYPE_LEVEL_LOW>;/* Hypervisor PPI */
+ 	};
+ 
+-	fsl_mc: fsl-mc@80c000000 {
+-		compatible = "fsl,qoriq-mc";
+-		reg = <0x00000008 0x0c000000 0 0x40>,	 /* MC portal base */
+-		      <0x00000000 0x08340000 0 0x40000>; /* MC control reg */
+-		msi-parent = <&its>;
+-		iommu-map = <0 &smmu 0 0>;      /* This is fixed-up by u-boot */
+-		#address-cells = <3>;
+-		#size-cells = <1>;
+-
+-		/*
+-		 * Region type 0x0 - MC portals
+-		 * Region type 0x1 - QBMAN portals
+-		 */
+-		ranges = <0x0 0x0 0x0 0x8 0x0c000000 0x4000000
+-			  0x1 0x0 0x0 0x8 0x18000000 0x8000000>;
+-
+-		dpmacs {
+-			#address-cells = <1>;
+-			#size-cells = <0>;
+-
+-			dpmac1: dpmac@1 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <1>;
+-			};
+-			dpmac2: dpmac@2 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <2>;
+-			};
+-			dpmac3: dpmac@3 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <3>;
+-			};
+-			dpmac4: dpmac@4 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <4>;
+-			};
+-			dpmac5: dpmac@5 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <5>;
+-			};
+-			dpmac6: dpmac@6 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <6>;
+-			};
+-			dpmac7: dpmac@7 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <7>;
+-			};
+-			dpmac8: dpmac@8 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <8>;
+-			};
+-			dpmac9: dpmac@9 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <9>;
+-			};
+-			dpmac10: dpmac@10 {
+-				compatible = "fsl,qoriq-mc-dpmac";
+-				reg = <0xa>;
+-			};
+-		};
+-
+-	};
+-
+ 	sysclk: sysclk {
+ 		compatible = "fixed-clock";
+ 		#clock-cells = <0>;
+@@ -269,6 +169,7 @@
+ 		#address-cells = <2>;
+ 		#size-cells = <2>;
+ 		ranges;
++		dma-ranges = <0x0 0x0 0x0 0x0 0x10000 0x00000000>;
+ 
+ 		clockgen: clocking@1300000 {
+ 			compatible = "fsl,ls1088a-clockgen";
+@@ -499,6 +400,46 @@
+ 			#size-cells = <0>;
+ 		};
+ 
++		pcs_mdio1: mdio@0x8c07000 {
++			compatible = "fsl,fman-memac-mdio";
++			reg = <0x0 0x8c07000 0x0 0x1000>;
++			device_type = "mdio";
++			little-endian;
++
++			#address-cells = <1>;
++			#size-cells = <0>;
++		};
++
++		pcs_mdio2: mdio@0x8c0b000 {
++			compatible = "fsl,fman-memac-mdio";
++			reg = <0x0 0x8c0b000 0x0 0x1000>;
++			device_type = "mdio";
++			little-endian;
++
++			#address-cells = <1>;
++			#size-cells = <0>;
++		};
++
++		pcs_mdio3: mdio@0x8c0f000 {
++			compatible = "fsl,fman-memac-mdio";
++			reg = <0x0 0x8c0f000 0x0 0x1000>;
++			device_type = "mdio";
++			little-endian;
++
++			#address-cells = <1>;
++			#size-cells = <0>;
++		};
++
++		pcs_mdio4: mdio@0x8c13000 {
++			compatible = "fsl,fman-memac-mdio";
++			reg = <0x0 0x8c13000 0x0 0x1000>;
++			device_type = "mdio";
++			little-endian;
++
++			#address-cells = <1>;
++			#size-cells = <0>;
++		};
++
+ 		ifc: ifc@2240000 {
+ 			compatible = "fsl,ifc", "simple-bus";
+ 			reg = <0x0 0x2240000 0x0 0x20000>;
+@@ -518,12 +459,13 @@
+ 		};
+ 
+ 		i2c0: i2c@2000000 {
+-			compatible = "fsl,vf610-i2c";
++			compatible = "fsl,vf610-i2c", "fsl,ls1088a-vf610-i2c";
+ 			#address-cells = <1>;
+ 			#size-cells = <0>;
+ 			reg = <0x0 0x2000000 0x0 0x10000>;
+ 			interrupts = <0 34 IRQ_TYPE_LEVEL_HIGH>;
+ 			clocks = <&clockgen 4 7>;
++			fsl-scl-gpio = <&gpio3 30 0>;
+ 			status = "disabled";
+ 		};
+ 
+@@ -689,6 +631,79 @@
+ 					<0000 0 0 4 &gic 0 0 0 122 IRQ_TYPE_LEVEL_HIGH>;
+ 		};
+ 
++		fsl_mc: fsl-mc@80c000000 {
++			compatible = "fsl,qoriq-mc";
++			reg = <0x00000008 0x0c000000 0 0x40>,	 /* MC portal base */
++			      <0x00000000 0x08340000 0 0x40000>; /* MC control reg */
++			msi-parent = <&its>;
++			iommu-map = <0 &smmu 0 0>;	/* This is fixed-up by u-boot */
++			dma-coherent;
++			#address-cells = <3>;
++			#size-cells = <1>;
++
++			/*
++			 * Region type 0x0 - MC portals
++			 * Region type 0x1 - QBMAN portals
++			 */
++			ranges = <0x0 0x0 0x0 0x8 0x0c000000 0x4000000
++				  0x1 0x0 0x0 0x8 0x18000000 0x8000000>;
++
++			dpmacs {
++				#address-cells = <1>;
++				#size-cells = <0>;
++
++				dpmac1: dpmac@1 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <1>;
++				};
++
++				dpmac2: dpmac@2 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <2>;
++				};
++
++				dpmac3: dpmac@3 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <3>;
++				};
++
++				dpmac4: dpmac@4 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <4>;
++				};
++
++				dpmac5: dpmac@5 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <5>;
++				};
++
++				dpmac6: dpmac@6 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <6>;
++				};
++
++				dpmac7: dpmac@7 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <7>;
++				};
++
++				dpmac8: dpmac@8 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <8>;
++				};
++
++				dpmac9: dpmac@9 {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <9>;
++				};
++
++				dpmac10: dpmac@a {
++					compatible = "fsl,qoriq-mc-dpmac";
++					reg = <0xa>;
++				};
++			};
++		};
++
+ 		smmu: iommu@5000000 {
+ 			compatible = "arm,mmu-500";
+ 			reg = <0 0x5000000 0 0x800000>;
+@@ -775,6 +790,11 @@
+ 				     <0 209 4>;
+ 		};
+ 
++                serdes1: serdes@1ea0000 {
++                                reg = <0x0 0x1ea0000 0 0x00002000>;
++				compatible = "fsl,serdes-10g";
++                };
++
+ 		crypto: crypto@8000000 {
+ 			compatible = "fsl,sec-v5.0", "fsl,sec-v4.0";
+ 			fsl,sec-era = <8>;
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls2080a-rdb.dts b/arch/arm64/boot/dts/freescale/fsl-ls2080a-rdb.dts
+index 025f0f54..2ed99e98 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls2080a-rdb.dts
++++ b/arch/arm64/boot/dts/freescale/fsl-ls2080a-rdb.dts
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree file for Freescale LS2080a RDB Board.
+  *
+@@ -7,43 +8,6 @@
+  * Abhimanyu Saini <abhimanyu.saini@nxp.com>
+  * Bhupesh Sharma <bhupesh.sharma@freescale.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ /dts-v1/;
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls2080a.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls2080a.dtsi
+index 3a643977..a80e0707 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls2080a.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls2080a.dtsi
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for Freescale Layerscape-2080A family SoC.
+  *
+@@ -6,43 +7,6 @@
+  * Abhimanyu Saini <abhimanyu.saini@nxp.com>
+  * Bhupesh Sharma <bhupesh.sharma@freescale.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ #include "fsl-ls208xa.dtsi"
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls2088a-rdb.dts b/arch/arm64/boot/dts/freescale/fsl-ls2088a-rdb.dts
+index 9300119b..a32b0f6e 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls2088a-rdb.dts
++++ b/arch/arm64/boot/dts/freescale/fsl-ls2088a-rdb.dts
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree file for Freescale LS2088A RDB Board.
+  *
+@@ -6,43 +7,6 @@
+  *
+  * Abhimanyu Saini <abhimanyu.saini@nxp.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ /dts-v1/;
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls2088a.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls2088a.dtsi
+index 833699ea..c0e43aab 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls2088a.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls2088a.dtsi
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for Freescale Layerscape-2088A family SoC.
+  *
+@@ -6,43 +7,6 @@
+  *
+  * Abhimanyu Saini <abhimanyu.saini@nxp.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ #include "fsl-ls208xa.dtsi"
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls208xa-qds.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls208xa-qds.dtsi
+index b2374469..bceb724d 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls208xa-qds.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls208xa-qds.dtsi
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree file for Freescale LS2080A QDS Board.
+  *
+@@ -6,43 +7,6 @@
+  *
+  * Abhimanyu Saini <abhimanyu.saini@nxp.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ &esdhc {
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls208xa-rdb.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls208xa-rdb.dtsi
+index 5dedd4dc..20fa0ab6 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls208xa-rdb.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls208xa-rdb.dtsi
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree file for Freescale LS2080A RDB Board.
+  *
+@@ -6,43 +7,6 @@
+  *
+  * Abhimanyu Saini <abhimanyu.saini@nxp.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ &esdhc {
+diff --git a/arch/arm64/boot/dts/freescale/fsl-ls208xa.dtsi b/arch/arm64/boot/dts/freescale/fsl-ls208xa.dtsi
+index 2a0a1dfd..888a2b19 100644
+--- a/arch/arm64/boot/dts/freescale/fsl-ls208xa.dtsi
++++ b/arch/arm64/boot/dts/freescale/fsl-ls208xa.dtsi
+@@ -1,3 +1,4 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+ /*
+  * Device Tree Include file for Freescale Layerscape-2080A family SoC.
+  *
+@@ -6,43 +7,6 @@
+  *
+  * Abhimanyu Saini <abhimanyu.saini@nxp.com>
+  *
+- * This file is dual-licensed: you can use it either under the terms
+- * of the GPLv2 or the X11 license, at your option. Note that this dual
+- * licensing only applies to this file, and not this project as a
+- * whole.
+- *
+- *  a) This library is free software; you can redistribute it and/or
+- *     modify it under the terms of the GNU General Public License as
+- *     published by the Free Software Foundation; either version 2 of the
+- *     License, or (at your option) any later version.
+- *
+- *     This library is distributed in the hope that it will be useful,
+- *     but WITHOUT ANY WARRANTY; without even the implied warranty of
+- *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- *     GNU General Public License for more details.
+- *
+- * Or, alternatively,
+- *
+- *  b) Permission is hereby granted, free of charge, to any person
+- *     obtaining a copy of this software and associated documentation
+- *     files (the "Software"), to deal in the Software without
+- *     restriction, including without limitation the rights to use,
+- *     copy, modify, merge, publish, distribute, sublicense, and/or
+- *     sell copies of the Software, and to permit persons to whom the
+- *     Software is furnished to do so, subject to the following
+- *     conditions:
+- *
+- *     The above copyright notice and this permission notice shall be
+- *     included in all copies or substantial portions of the Software.
+- *
+- *     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+- *     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+- *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+- *     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+- *     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+- *     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+- *     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+- *     OTHER DEALINGS IN THE SOFTWARE.
+  */
+ 
+ #include <dt-bindings/thermal/thermal.h>
+@@ -129,6 +93,7 @@
+ 		#address-cells = <2>;
+ 		#size-cells = <2>;
+ 		ranges;
++		dma-ranges = <0x0 0x0 0x0 0x0 0x10000 0x00000000>;
+ 
+ 		clockgen: clocking@1300000 {
+ 			compatible = "fsl,ls2080a-clockgen";
+@@ -352,6 +317,7 @@
+ 			      <0x00000000 0x08340000 0 0x40000>; /* MC control reg */
+ 			msi-parent = <&its>;
+ 			iommu-map = <0 &smmu 0 0>;	/* This is fixed-up by u-boot */
++			dma-coherent;
+ 			#address-cells = <3>;
+ 			#size-cells = <1>;
+ 
+@@ -682,13 +648,14 @@
+ 
+ 		i2c0: i2c@2000000 {
+ 			status = "disabled";
+-			compatible = "fsl,vf610-i2c";
++			compatible = "fsl,vf610-i2c", "fsl,ls208xa-vf610-i2c";
+ 			#address-cells = <1>;
+ 			#size-cells = <0>;
+ 			reg = <0x0 0x2000000 0x0 0x10000>;
+ 			interrupts = <0 34 0x4>; /* Level high type */
+ 			clock-names = "i2c";
+ 			clocks = <&clockgen 4 1>;
++			fsl-scl-gpio = <&gpio3 10 0>;
+ 		};
+ 
+ 		i2c1: i2c@2010000 {
+@@ -878,6 +845,7 @@
+ 
+ 		serdes1: serdes@1ea0000 {
+ 				reg = <0x0 0x1ea0000 0 0x00002000>;
++				compatible = "fsl,serdes-10g";
+ 		};
+ 
+ 		ccn@4000000 {
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-bman-portals-sdk.dtsi b/arch/arm64/boot/dts/freescale/qoriq-bman-portals-sdk.dtsi
+new file mode 100644
+index 00000000..0a68ec4c
+--- /dev/null
++++ b/arch/arm64/boot/dts/freescale/qoriq-bman-portals-sdk.dtsi
+@@ -0,0 +1,55 @@
++/*
++ * QorIQ BMan SDK Portals device tree nodes
++ *
++ * Copyright 2011-2016 Freescale Semiconductor Inc.
++ * Copyright 2017 NXP
++ *
++ * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++ */
++
++&bportals {
++	bman-portal@0 {
++		cell-index = <0>;
++	};
++
++	bman-portal@10000 {
++		cell-index = <1>;
++	};
++
++	bman-portal@20000 {
++		cell-index = <2>;
++	};
++
++	bman-portal@30000 {
++		cell-index = <3>;
++	};
++
++	bman-portal@40000 {
++		cell-index = <4>;
++	};
++
++	bman-portal@50000 {
++		cell-index = <5>;
++	};
++
++	bman-portal@60000 {
++		cell-index = <6>;
++	};
++
++	bman-portal@70000 {
++		cell-index = <7>;
++	};
++
++	bman-portal@80000 {
++		cell-index = <8>;
++	};
++
++	bman-portal@90000 {
++		cell-index = <9>;
++	};
++
++	 bman-bpids@0 {
++		compatible = "fsl,bpid-range";
++		fsl,bpid-range = <32 32>;
++	};
++};
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-bman-portals.dtsi b/arch/arm64/boot/dts/freescale/qoriq-bman-portals.dtsi
+new file mode 100644
+index 00000000..ff1aba5f
+--- /dev/null
++++ b/arch/arm64/boot/dts/freescale/qoriq-bman-portals.dtsi
+@@ -0,0 +1,77 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * QorIQ BMan Portals device tree
++ *
++ * Copyright 2011-2016 Freescale Semiconductor Inc.
++ *
++ */
++
++&bportals {
++	#address-cells = <1>;
++	#size-cells = <1>;
++	compatible = "simple-bus";
++
++	bman-portal@0 {
++		/*
++		 * bootloader fix-ups are expected to provide the
++		 * "fsl,bman-portal-<hardware revision>" compatible
++		 */
++		compatible = "fsl,bman-portal";
++		reg = <0x0 0x4000>, <0x4000000 0x4000>;
++		interrupts = <GIC_SPI 173 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@10000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x10000 0x4000>, <0x4010000 0x4000>;
++		interrupts = <GIC_SPI 175 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@20000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x20000 0x4000>, <0x4020000 0x4000>;
++		interrupts = <GIC_SPI 177 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@30000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x30000 0x4000>, <0x4030000 0x4000>;
++		interrupts = <GIC_SPI 179 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@40000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x40000 0x4000>, <0x4040000 0x4000>;
++		interrupts = <GIC_SPI 181 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@50000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x50000 0x4000>, <0x4050000 0x4000>;
++		interrupts = <GIC_SPI 183 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@60000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x60000 0x4000>, <0x4060000 0x4000>;
++		interrupts = <GIC_SPI 185 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@70000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x70000 0x4000>, <0x4070000 0x4000>;
++		interrupts = <GIC_SPI 187 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@80000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x80000 0x4000>, <0x4080000 0x4000>;
++		interrupts = <GIC_SPI 189 IRQ_TYPE_LEVEL_HIGH>;
++	};
++
++	bman-portal@90000 {
++		compatible = "fsl,bman-portal";
++		reg = <0x90000 0x4000>, <0x4090000 0x4000>;
++		interrupts = <GIC_SPI 191 IRQ_TYPE_LEVEL_HIGH>;
++	};
++};
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-dpaa-eth.dtsi b/arch/arm64/boot/dts/freescale/qoriq-dpaa-eth.dtsi
+index 5c6be929..8f970bd8 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-dpaa-eth.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-dpaa-eth.dtsi
+@@ -69,5 +69,29 @@ fsldpaa: fsl,dpaa {
+ 		fsl,fman-mac = <&enet6>;
+ 		dma-coherent;
+ 	};
++	ethernet@6 {
++		compatible = "fsl,im-ethernet";
++		fsl,fman-mac = <&enet2>;
++		dma-coherent;
++		fpmevt-sel = <0>;
++	};
++	ethernet@7 {
++		compatible = "fsl,im-ethernet";
++		fsl,fman-mac = <&enet3>;
++		dma-coherent;
++		fpmevt-sel = <1>;
++	};
++	ethernet@10 {
++		compatible = "fsl,im-ethernet";
++		fsl,fman-mac = <&enet4>;
++		dma-coherent;
++		fpmevt-sel = <2>;
++	};
++	ethernet@11 {
++		compatible = "fsl,im-ethernet";
++		fsl,fman-mac = <&enet5>;
++		dma-coherent;
++		fpmevt-sel = <3>;
++	};
+ };
+ 
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-10g-0.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-10g-0.dtsi
+index 474bff5e..7886b94c 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-10g-0.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-10g-0.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 10g port #0 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman@1a00000 {
+@@ -22,7 +22,7 @@ fman@1a00000 {
+ 		fsl,qman-channel-id = <0x800>;
+ 	};
+ 
+-	ethernet@f0000 {
++	mac9: ethernet@f0000 {
+ 		cell-index = <0x8>;
+ 		compatible = "fsl,fman-memac";
+ 		reg = <0xf0000 0x1000>;
+@@ -30,7 +30,7 @@ fman@1a00000 {
+ 		pcsphy-handle = <&pcsphy6>;
+ 	};
+ 
+-	mdio@f1000 {
++	mdio9: mdio@f1000 {
+ 		#address-cells = <1>;
+ 		#size-cells = <0>;
+ 		compatible = "fsl,fman-memac-mdio", "fsl,fman-xmdio";
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-10g-1.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-10g-1.dtsi
+index d4326f85..89b9d14b 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-10g-1.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-10g-1.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 10g port #1 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman@1a00000 {
+@@ -22,7 +22,7 @@ fman@1a00000 {
+ 		fsl,qman-channel-id = <0x801>;
+ 	};
+ 
+-	ethernet@f2000 {
++	mac10: ethernet@f2000 {
+ 		cell-index = <0x9>;
+ 		compatible = "fsl,fman-memac";
+ 		reg = <0xf2000 0x1000>;
+@@ -30,7 +30,7 @@ fman@1a00000 {
+ 		pcsphy-handle = <&pcsphy7>;
+ 	};
+ 
+-	mdio@f3000 {
++	mdio10: mdio@f3000 {
+ 		#address-cells = <1>;
+ 		#size-cells = <0>;
+ 		compatible = "fsl,fman-memac-mdio", "fsl,fman-xmdio";
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-0.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-0.dtsi
+index 7170cab9..60ed4473 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-0.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-0.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 1g port #0 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman@1a00000 {
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-1.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-1.dtsi
+index c7eb8b6e..69de2c1f 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-1.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-1.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 1g port #1 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman@1a00000 {
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-2.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-2.dtsi
+index 56f9f0dd..981bbfca 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-2.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-2.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 1g port #2 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman@1a00000 {
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-3.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-3.dtsi
+index bbe7dbaf..956b1b9d 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-3.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-3.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 1g port #3 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman@1a00000 {
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-4.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-4.dtsi
+index ead4f062..52899ce6 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-4.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-4.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 1g port #4 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman@1a00000 {
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-5.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-5.dtsi
+index 389eadaf..fe8c4c29 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-5.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0-1g-5.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 1g port #5 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman@1a00000 {
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-fman3-0.dtsi b/arch/arm64/boot/dts/freescale/qoriq-fman3-0.dtsi
+index 8e089f0c..67a118ee 100644
+--- a/arch/arm64/boot/dts/freescale/qoriq-fman3-0.dtsi
++++ b/arch/arm64/boot/dts/freescale/qoriq-fman3-0.dtsi
+@@ -1,9 +1,9 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * QorIQ FMan v3 device tree
+  *
+  * Copyright 2012-2015 Freescale Semiconductor Inc.
+  *
+- * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+  */
+ 
+ fman0: fman@1a00000 {
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-qman-portals-sdk.dtsi b/arch/arm64/boot/dts/freescale/qoriq-qman-portals-sdk.dtsi
+new file mode 100644
+index 00000000..d3abfaf6
+--- /dev/null
++++ b/arch/arm64/boot/dts/freescale/qoriq-qman-portals-sdk.dtsi
+@@ -0,0 +1,38 @@
++/*
++ * QorIQ QMan SDK Portals device tree nodes
++ *
++ * Copyright 2011-2016 Freescale Semiconductor Inc.
++ * Copyright 2017 NXP
++ *
++ * SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++ */
++
++&qportals {
++	qman-fqids@0 {
++		compatible = "fsl,fqid-range";
++		fsl,fqid-range = <256 256>;
++	};
++
++	qman-fqids@1 {
++		compatible = "fsl,fqid-range";
++		fsl,fqid-range = <32768 32768>;
++	};
++
++	qman-pools@0 {
++		compatible = "fsl,pool-channel-range";
++		fsl,pool-channel-range = <0x401 0xf>;
++	};
++
++	qman-cgrids@0 {
++		compatible = "fsl,cgrid-range";
++		fsl,cgrid-range = <0 256>;
++	};
++
++	qman-ceetm@0 {
++		compatible = "fsl,qman-ceetm";
++		fsl,ceetm-lfqid-range = <0xf00000 0x1000>;
++		fsl,ceetm-sp-range = <0 16>;
++		fsl,ceetm-lni-range = <0 8>;
++		fsl,ceetm-channel-range = <0 32>;
++	};
++};
+diff --git a/arch/arm64/boot/dts/freescale/qoriq-qman-portals.dtsi b/arch/arm64/boot/dts/freescale/qoriq-qman-portals.dtsi
+new file mode 100644
+index 00000000..e3bec08b
+--- /dev/null
++++ b/arch/arm64/boot/dts/freescale/qoriq-qman-portals.dtsi
+@@ -0,0 +1,87 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * QorIQ QMan Portals device tree
++ *
++ * Copyright 2011-2016 Freescale Semiconductor Inc.
++ *
++ */
++
++&qportals {
++	#address-cells = <1>;
++	#size-cells = <1>;
++	compatible = "simple-bus";
++
++	qportal0: qman-portal@0 {
++		/*
++		 * bootloader fix-ups are expected to provide the
++		 * "fsl,bman-portal-<hardware revision>" compatible
++		 */
++		compatible = "fsl,qman-portal";
++		reg = <0x0 0x4000>, <0x4000000 0x4000>;
++		interrupts = <GIC_SPI 172 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <0>;
++	};
++
++	qportal1: qman-portal@10000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x10000 0x4000>, <0x4010000 0x4000>;
++		interrupts = <GIC_SPI 174 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <1>;
++	};
++
++	qportal2: qman-portal@20000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x20000 0x4000>, <0x4020000 0x4000>;
++		interrupts = <GIC_SPI 176 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <2>;
++	};
++
++	qportal3: qman-portal@30000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x30000 0x4000>, <0x4030000 0x4000>;
++		interrupts = <GIC_SPI 178 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <3>;
++	};
++
++	qportal4: qman-portal@40000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x40000 0x4000>, <0x4040000 0x4000>;
++		interrupts = <GIC_SPI 180 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <4>;
++	};
++
++	qportal5: qman-portal@50000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x50000 0x4000>, <0x4050000 0x4000>;
++		interrupts = <GIC_SPI 182 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <5>;
++	};
++
++	qportal6: qman-portal@60000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x60000 0x4000>, <0x4060000 0x4000>;
++		interrupts = <GIC_SPI 184 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <6>;
++	};
++
++	qportal7: qman-portal@70000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x70000 0x4000>, <0x4070000 0x4000>;
++		interrupts = <GIC_SPI 186 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <7>;
++	};
++
++	qportal8: qman-portal@80000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x80000 0x4000>, <0x4080000 0x4000>;
++		interrupts = <GIC_SPI 188 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <8>;
++	};
++
++	qportal9: qman-portal@90000 {
++		compatible = "fsl,qman-portal";
++		reg = <0x90000 0x4000>, <0x4090000 0x4000>;
++		interrupts = <GIC_SPI 190 IRQ_TYPE_LEVEL_HIGH>;
++		cell-index = <9>;
++	};
++};
+diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c
+index 5a010bcc..1cf8c905 100644
+--- a/arch/arm64/mm/dma-mapping.c
++++ b/arch/arm64/mm/dma-mapping.c
+@@ -30,7 +30,7 @@
+ #include <linux/swiotlb.h>
+ 
+ #include <asm/cacheflush.h>
+-#include <../../../drivers/staging/fsl-mc/include/mc-bus.h>
++#include <linux/fsl/mc.h>
+ 
+ static int swiotlb __ro_after_init;
+ 
+diff --git a/crypto/tcrypt.c b/crypto/tcrypt.c
+index 2a6f11a6..d4c43216 100644
+--- a/crypto/tcrypt.c
++++ b/crypto/tcrypt.c
+@@ -357,11 +357,13 @@ static void test_aead_speed(const char *algo, int enc, unsigned int secs,
+ 					       iv);
+ 			aead_request_set_ad(req, aad_size);
+ 
+-			if (secs)
++			if (secs) {
+ 				ret = test_aead_jiffies(req, enc, *b_size,
+ 							secs);
+-			else
++				cond_resched();
++			} else {
+ 				ret = test_aead_cycles(req, enc, *b_size);
++			}
+ 
+ 			if (ret) {
+ 				pr_err("%s() failed return code=%d\n", e, ret);
+@@ -735,12 +737,14 @@ static void test_ahash_speed_common(const char *algo, unsigned int secs,
+ 
+ 		ahash_request_set_crypt(req, sg, output, speed[i].plen);
+ 
+-		if (secs)
++		if (secs) {
+ 			ret = test_ahash_jiffies(req, speed[i].blen,
+ 						 speed[i].plen, output, secs);
+-		else
++			cond_resched();
++		} else {
+ 			ret = test_ahash_cycles(req, speed[i].blen,
+ 						speed[i].plen, output);
++		}
+ 
+ 		if (ret) {
+ 			pr_err("hashing failed ret=%d\n", ret);
+@@ -958,12 +962,14 @@ static void test_skcipher_speed(const char *algo, int enc, unsigned int secs,
+ 
+ 			skcipher_request_set_crypt(req, sg, sg, *b_size, iv);
+ 
+-			if (secs)
++			if (secs) {
+ 				ret = test_acipher_jiffies(req, enc,
+ 							   *b_size, secs);
+-			else
++				cond_resched();
++			} else {
+ 				ret = test_acipher_cycles(req, enc,
+ 							  *b_size);
++			}
+ 
+ 			if (ret) {
+ 				pr_err("%s() failed flags=%x\n", e,
+diff --git a/drivers/bus/Kconfig b/drivers/bus/Kconfig
+index 78751057..2b4c3344 100644
+--- a/drivers/bus/Kconfig
++++ b/drivers/bus/Kconfig
+@@ -167,4 +167,7 @@ config VEXPRESS_CONFIG
+ 	help
+ 	  Platform configuration infrastructure for the ARM Ltd.
+ 	  Versatile Express.
++
++source "drivers/bus/fsl-mc/Kconfig"
++
+ endmenu
+diff --git a/drivers/bus/Makefile b/drivers/bus/Makefile
+index c6cfa6b2..fd3b8d1d 100644
+--- a/drivers/bus/Makefile
++++ b/drivers/bus/Makefile
+@@ -7,6 +7,10 @@ obj-$(CONFIG_ARM_CCI)		+= arm-cci.o
+ obj-$(CONFIG_ARM_CCN)		+= arm-ccn.o
+ 
+ obj-$(CONFIG_BRCMSTB_GISB_ARB)	+= brcmstb_gisb.o
++
++# DPAA2 fsl-mc bus
++obj-$(CONFIG_FSL_MC_BUS)	+= fsl-mc/
++
+ obj-$(CONFIG_IMX_WEIM)		+= imx-weim.o
+ obj-$(CONFIG_MIPS_CDMM)		+= mips_cdmm.o
+ obj-$(CONFIG_MVEBU_MBUS) 	+= mvebu-mbus.o
+diff --git a/drivers/bus/fsl-mc/Kconfig b/drivers/bus/fsl-mc/Kconfig
+new file mode 100644
+index 00000000..74a8a83c
+--- /dev/null
++++ b/drivers/bus/fsl-mc/Kconfig
+@@ -0,0 +1,23 @@
++# SPDX-License-Identifier: GPL-2.0
++#
++# DPAA2 fsl-mc bus
++#
++# Copyright (C) 2014-2016 Freescale Semiconductor, Inc.
++#
++
++config FSL_MC_BUS
++	bool "QorIQ DPAA2 fsl-mc bus driver"
++	depends on OF && (ARCH_LAYERSCAPE || (COMPILE_TEST && (ARM || ARM64 || X86 || PPC)))
++	select GENERIC_MSI_IRQ_DOMAIN
++	help
++	  Driver to enable the bus infrastructure for the QorIQ DPAA2
++	  architecture.  The fsl-mc bus driver handles discovery of
++	  DPAA2 objects (which are represented as Linux devices) and
++	  binding objects to drivers.
++
++config FSL_MC_RESTOOL
++	bool "Management Complex (MC) restool support"
++	depends on FSL_MC_BUS
++	help
++	  Provides kernel support for the Management Complex resource
++	  manager user-space tool - restool.
+diff --git a/drivers/bus/fsl-mc/Makefile b/drivers/bus/fsl-mc/Makefile
+new file mode 100644
+index 00000000..5093bc4a
+--- /dev/null
++++ b/drivers/bus/fsl-mc/Makefile
+@@ -0,0 +1,22 @@
++# SPDX-License-Identifier: GPL-2.0
++#
++# Freescale Management Complex (MC) bus drivers
++#
++# Copyright (C) 2014 Freescale Semiconductor, Inc.
++#
++obj-$(CONFIG_FSL_MC_BUS) += mc-bus-driver.o
++
++mc-bus-driver-objs := fsl-mc-bus.o \
++		      mc-sys.o \
++		      mc-io.o \
++		      dpbp.o \
++		      dpcon.o \
++		      dprc.o \
++		      dprc-driver.o \
++		      fsl-mc-allocator.o \
++		      fsl-mc-msi.o \
++		      dpmcp.o \
++		      fsl-mc-iommu.o
++
++# MC restool kernel support
++obj-$(CONFIG_FSL_MC_RESTOOL) += fsl-mc-restool.o
+diff --git a/drivers/bus/fsl-mc/dpbp.c b/drivers/bus/fsl-mc/dpbp.c
+new file mode 100644
+index 00000000..17e3c5d2
+--- /dev/null
++++ b/drivers/bus/fsl-mc/dpbp.c
+@@ -0,0 +1,186 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * Copyright 2013-2016 Freescale Semiconductor Inc.
++ *
++ */
++#include <linux/kernel.h>
++#include <linux/fsl/mc.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++/**
++ * dpbp_open() - Open a control session for the specified object.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @dpbp_id:	DPBP unique ID
++ * @token:	Returned token; use in subsequent API calls
++ *
++ * This function can be used to open a control session for an
++ * already created object; an object may have been declared in
++ * the DPL or by calling the dpbp_create function.
++ * This function returns a unique authentication token,
++ * associated with the specific object ID and the specific MC
++ * portal; this token must be used in all subsequent commands for
++ * this specific object
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpbp_open(struct fsl_mc_io *mc_io,
++	      u32 cmd_flags,
++	      int dpbp_id,
++	      u16 *token)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dpbp_cmd_open *cmd_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPBP_CMDID_OPEN,
++					  cmd_flags, 0);
++	cmd_params = (struct dpbp_cmd_open *)cmd.params;
++	cmd_params->dpbp_id = cpu_to_le32(dpbp_id);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	*token = mc_cmd_hdr_read_token(&cmd);
++
++	return err;
++}
++EXPORT_SYMBOL_GPL(dpbp_open);
++
++/**
++ * dpbp_close() - Close the control session of the object
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPBP object
++ *
++ * After this function is called, no further operations are
++ * allowed on the object without opening a new control session.
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpbp_close(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPBP_CMDID_CLOSE, cmd_flags,
++					  token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpbp_close);
++
++/**
++ * dpbp_enable() - Enable the DPBP.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPBP object
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpbp_enable(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPBP_CMDID_ENABLE, cmd_flags,
++					  token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpbp_enable);
++
++/**
++ * dpbp_disable() - Disable the DPBP.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPBP object
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpbp_disable(struct fsl_mc_io *mc_io,
++		 u32 cmd_flags,
++		 u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPBP_CMDID_DISABLE,
++					  cmd_flags, token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpbp_disable);
++
++/**
++ * dpbp_reset() - Reset the DPBP, returns the object to initial state.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPBP object
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpbp_reset(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPBP_CMDID_RESET,
++					  cmd_flags, token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpbp_reset);
++
++/**
++ * dpbp_get_attributes - Retrieve DPBP attributes.
++ *
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPBP object
++ * @attr:	Returned object's attributes
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpbp_get_attributes(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			struct dpbp_attr *attr)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dpbp_rsp_get_attributes *rsp_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPBP_CMDID_GET_ATTR,
++					  cmd_flags, token);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	rsp_params = (struct dpbp_rsp_get_attributes *)cmd.params;
++	attr->bpid = le16_to_cpu(rsp_params->bpid);
++	attr->id = le32_to_cpu(rsp_params->id);
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(dpbp_get_attributes);
+diff --git a/drivers/bus/fsl-mc/dpcon.c b/drivers/bus/fsl-mc/dpcon.c
+new file mode 100644
+index 00000000..760555d7
+--- /dev/null
++++ b/drivers/bus/fsl-mc/dpcon.c
+@@ -0,0 +1,222 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * Copyright 2013-2016 Freescale Semiconductor Inc.
++ *
++ */
++#include <linux/kernel.h>
++#include <linux/fsl/mc.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++/**
++ * dpcon_open() - Open a control session for the specified object
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @dpcon_id:	DPCON unique ID
++ * @token:	Returned token; use in subsequent API calls
++ *
++ * This function can be used to open a control session for an
++ * already created object; an object may have been declared in
++ * the DPL or by calling the dpcon_create() function.
++ * This function returns a unique authentication token,
++ * associated with the specific object ID and the specific MC
++ * portal; this token must be used in all subsequent commands for
++ * this specific object.
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpcon_open(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       int dpcon_id,
++	       u16 *token)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dpcon_cmd_open *dpcon_cmd;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPCON_CMDID_OPEN,
++					  cmd_flags,
++					  0);
++	dpcon_cmd = (struct dpcon_cmd_open *)cmd.params;
++	dpcon_cmd->dpcon_id = cpu_to_le32(dpcon_id);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	*token = mc_cmd_hdr_read_token(&cmd);
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(dpcon_open);
++
++/**
++ * dpcon_close() - Close the control session of the object
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPCON object
++ *
++ * After this function is called, no further operations are
++ * allowed on the object without opening a new control session.
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpcon_close(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPCON_CMDID_CLOSE,
++					  cmd_flags,
++					  token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpcon_close);
++
++/**
++ * dpcon_enable() - Enable the DPCON
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPCON object
++ *
++ * Return:	'0' on Success; Error code otherwise
++ */
++int dpcon_enable(struct fsl_mc_io *mc_io,
++		 u32 cmd_flags,
++		 u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPCON_CMDID_ENABLE,
++					  cmd_flags,
++					  token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpcon_enable);
++
++/**
++ * dpcon_disable() - Disable the DPCON
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPCON object
++ *
++ * Return:	'0' on Success; Error code otherwise
++ */
++int dpcon_disable(struct fsl_mc_io *mc_io,
++		  u32 cmd_flags,
++		  u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPCON_CMDID_DISABLE,
++					  cmd_flags,
++					  token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpcon_disable);
++
++/**
++ * dpcon_reset() - Reset the DPCON, returns the object to initial state.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPCON object
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpcon_reset(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPCON_CMDID_RESET,
++					  cmd_flags, token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpcon_reset);
++
++/**
++ * dpcon_get_attributes() - Retrieve DPCON attributes.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPCON object
++ * @attr:	Object's attributes
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpcon_get_attributes(struct fsl_mc_io *mc_io,
++			 u32 cmd_flags,
++			 u16 token,
++			 struct dpcon_attr *attr)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dpcon_rsp_get_attr *dpcon_rsp;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPCON_CMDID_GET_ATTR,
++					  cmd_flags,
++					  token);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	dpcon_rsp = (struct dpcon_rsp_get_attr *)cmd.params;
++	attr->id = le32_to_cpu(dpcon_rsp->id);
++	attr->qbman_ch_id = le16_to_cpu(dpcon_rsp->qbman_ch_id);
++	attr->num_priorities = dpcon_rsp->num_priorities;
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(dpcon_get_attributes);
++
++/**
++ * dpcon_set_notification() - Set DPCON notification destination
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPCON object
++ * @cfg:	Notification parameters
++ *
++ * Return:	'0' on Success; Error code otherwise
++ */
++int dpcon_set_notification(struct fsl_mc_io *mc_io,
++			   u32 cmd_flags,
++			   u16 token,
++			   struct dpcon_notification_cfg *cfg)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dpcon_cmd_set_notification *dpcon_cmd;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPCON_CMDID_SET_NOTIFICATION,
++					  cmd_flags,
++					  token);
++	dpcon_cmd = (struct dpcon_cmd_set_notification *)cmd.params;
++	dpcon_cmd->dpio_id = cpu_to_le32(cfg->dpio_id);
++	dpcon_cmd->priority = cfg->priority;
++	dpcon_cmd->user_ctx = cpu_to_le64(cfg->user_ctx);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dpcon_set_notification);
+diff --git a/drivers/bus/fsl-mc/dpmcp.c b/drivers/bus/fsl-mc/dpmcp.c
+new file mode 100644
+index 00000000..5fbd0dbd
+--- /dev/null
++++ b/drivers/bus/fsl-mc/dpmcp.c
+@@ -0,0 +1,99 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * Copyright 2013-2016 Freescale Semiconductor Inc.
++ *
++ */
++#include <linux/kernel.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++/**
++ * dpmcp_open() - Open a control session for the specified object.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @dpmcp_id:	DPMCP unique ID
++ * @token:	Returned token; use in subsequent API calls
++ *
++ * This function can be used to open a control session for an
++ * already created object; an object may have been declared in
++ * the DPL or by calling the dpmcp_create function.
++ * This function returns a unique authentication token,
++ * associated with the specific object ID and the specific MC
++ * portal; this token must be used in all subsequent commands for
++ * this specific object
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpmcp_open(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       int dpmcp_id,
++	       u16 *token)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dpmcp_cmd_open *cmd_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPMCP_CMDID_OPEN,
++					  cmd_flags, 0);
++	cmd_params = (struct dpmcp_cmd_open *)cmd.params;
++	cmd_params->dpmcp_id = cpu_to_le32(dpmcp_id);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	*token = mc_cmd_hdr_read_token(&cmd);
++
++	return err;
++}
++
++/**
++ * dpmcp_close() - Close the control session of the object
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPMCP object
++ *
++ * After this function is called, no further operations are
++ * allowed on the object without opening a new control session.
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpmcp_close(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPMCP_CMDID_CLOSE,
++					  cmd_flags, token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++
++/**
++ * dpmcp_reset() - Reset the DPMCP, returns the object to initial state.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPMCP object
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpmcp_reset(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPMCP_CMDID_RESET,
++					  cmd_flags, token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
+diff --git a/drivers/bus/fsl-mc/dprc-driver.c b/drivers/bus/fsl-mc/dprc-driver.c
+new file mode 100644
+index 00000000..d9655edb
+--- /dev/null
++++ b/drivers/bus/fsl-mc/dprc-driver.c
+@@ -0,0 +1,815 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Freescale data path resource container (DPRC) driver
++ *
++ * Copyright (C) 2014-2016 Freescale Semiconductor, Inc.
++ * Author: German Rivera <German.Rivera@freescale.com>
++ *
++ */
++
++#include <linux/module.h>
++#include <linux/slab.h>
++#include <linux/interrupt.h>
++#include <linux/msi.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++#define FSL_MC_DPRC_DRIVER_NAME    "fsl_mc_dprc"
++
++struct fsl_mc_child_objs {
++	int child_count;
++	struct fsl_mc_obj_desc *child_array;
++};
++
++static bool fsl_mc_device_match(struct fsl_mc_device *mc_dev,
++				struct fsl_mc_obj_desc *obj_desc)
++{
++	return mc_dev->obj_desc.id == obj_desc->id &&
++	       strcmp(mc_dev->obj_desc.type, obj_desc->type) == 0;
++
++}
++
++static int __fsl_mc_device_remove_if_not_in_mc(struct device *dev, void *data)
++{
++	int i;
++	struct fsl_mc_child_objs *objs;
++	struct fsl_mc_device *mc_dev;
++
++	mc_dev = to_fsl_mc_device(dev);
++	objs = data;
++
++	for (i = 0; i < objs->child_count; i++) {
++		struct fsl_mc_obj_desc *obj_desc = &objs->child_array[i];
++
++		if (strlen(obj_desc->type) != 0 &&
++		    fsl_mc_device_match(mc_dev, obj_desc))
++			break;
++	}
++
++	if (i == objs->child_count)
++		fsl_mc_device_remove(mc_dev);
++
++	return 0;
++}
++
++static int __fsl_mc_device_remove(struct device *dev, void *data)
++{
++	fsl_mc_device_remove(to_fsl_mc_device(dev));
++	return 0;
++}
++
++/**
++ * dprc_remove_devices - Removes devices for objects removed from a DPRC
++ *
++ * @mc_bus_dev: pointer to the fsl-mc device that represents a DPRC object
++ * @obj_desc_array: array of object descriptors for child objects currently
++ * present in the DPRC in the MC.
++ * @num_child_objects_in_mc: number of entries in obj_desc_array
++ *
++ * Synchronizes the state of the Linux bus driver with the actual state of
++ * the MC by removing devices that represent MC objects that have
++ * been dynamically removed in the physical DPRC.
++ */
++static void dprc_remove_devices(struct fsl_mc_device *mc_bus_dev,
++				struct fsl_mc_obj_desc *obj_desc_array,
++				int num_child_objects_in_mc)
++{
++	if (num_child_objects_in_mc != 0) {
++		/*
++		 * Remove child objects that are in the DPRC in Linux,
++		 * but not in the MC:
++		 */
++		struct fsl_mc_child_objs objs;
++
++		objs.child_count = num_child_objects_in_mc;
++		objs.child_array = obj_desc_array;
++		device_for_each_child(&mc_bus_dev->dev, &objs,
++				      __fsl_mc_device_remove_if_not_in_mc);
++	} else {
++		/*
++		 * There are no child objects for this DPRC in the MC.
++		 * So, remove all the child devices from Linux:
++		 */
++		device_for_each_child(&mc_bus_dev->dev, NULL,
++				      __fsl_mc_device_remove);
++	}
++}
++
++static int __fsl_mc_device_match(struct device *dev, void *data)
++{
++	struct fsl_mc_obj_desc *obj_desc = data;
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++
++	return fsl_mc_device_match(mc_dev, obj_desc);
++}
++
++static struct fsl_mc_device *fsl_mc_device_lookup(struct fsl_mc_obj_desc
++								*obj_desc,
++						  struct fsl_mc_device
++								*mc_bus_dev)
++{
++	struct device *dev;
++
++	dev = device_find_child(&mc_bus_dev->dev, obj_desc,
++				__fsl_mc_device_match);
++
++	return dev ? to_fsl_mc_device(dev) : NULL;
++}
++
++/**
++ * check_plugged_state_change - Check change in an MC object's plugged state
++ *
++ * @mc_dev: pointer to the fsl-mc device for a given MC object
++ * @obj_desc: pointer to the MC object's descriptor in the MC
++ *
++ * If the plugged state has changed from unplugged to plugged, the fsl-mc
++ * device is bound to the corresponding device driver.
++ * If the plugged state has changed from plugged to unplugged, the fsl-mc
++ * device is unbound from the corresponding device driver.
++ */
++static void check_plugged_state_change(struct fsl_mc_device *mc_dev,
++				       struct fsl_mc_obj_desc *obj_desc)
++{
++	int error;
++	u32 plugged_flag_at_mc =
++			obj_desc->state & FSL_MC_OBJ_STATE_PLUGGED;
++
++	if (plugged_flag_at_mc !=
++	    (mc_dev->obj_desc.state & FSL_MC_OBJ_STATE_PLUGGED)) {
++		if (plugged_flag_at_mc) {
++			mc_dev->obj_desc.state |= FSL_MC_OBJ_STATE_PLUGGED;
++			error = device_attach(&mc_dev->dev);
++			if (error < 0) {
++				dev_err(&mc_dev->dev,
++					"device_attach() failed: %d\n",
++					error);
++			}
++		} else {
++			mc_dev->obj_desc.state &= ~FSL_MC_OBJ_STATE_PLUGGED;
++			device_release_driver(&mc_dev->dev);
++		}
++	}
++}
++
++/**
++ * dprc_add_new_devices - Adds devices to the logical bus for a DPRC
++ *
++ * @mc_bus_dev: pointer to the fsl-mc device that represents a DPRC object
++ * @driver_override: driver override to apply to new objects found in the
++ * DPRC, or NULL, if none.
++ * @obj_desc_array: array of device descriptors for child devices currently
++ * present in the physical DPRC.
++ * @num_child_objects_in_mc: number of entries in obj_desc_array
++ *
++ * Synchronizes the state of the Linux bus driver with the actual
++ * state of the MC by adding objects that have been newly discovered
++ * in the physical DPRC.
++ */
++static void dprc_add_new_devices(struct fsl_mc_device *mc_bus_dev,
++				 const char *driver_override,
++				 struct fsl_mc_obj_desc *obj_desc_array,
++				 int num_child_objects_in_mc)
++{
++	int error;
++	int i;
++
++	for (i = 0; i < num_child_objects_in_mc; i++) {
++		struct fsl_mc_device *child_dev;
++		struct fsl_mc_obj_desc *obj_desc = &obj_desc_array[i];
++
++		if (strlen(obj_desc->type) == 0)
++			continue;
++
++		/*
++		 * Check if device is already known to Linux:
++		 */
++		child_dev = fsl_mc_device_lookup(obj_desc, mc_bus_dev);
++		if (child_dev) {
++			check_plugged_state_change(child_dev, obj_desc);
++			put_device(&child_dev->dev);
++			continue;
++		}
++
++		error = fsl_mc_device_add(obj_desc, NULL, &mc_bus_dev->dev,
++					  driver_override, &child_dev);
++		if (error < 0)
++			continue;
++	}
++}
++
++/**
++ * dprc_scan_objects - Discover objects in a DPRC
++ *
++ * @mc_bus_dev: pointer to the fsl-mc device that represents a DPRC object
++ * @driver_override: driver override to apply to new objects found in the
++ * DPRC, or NULL, if none.
++ * @total_irq_count: If argument is provided the function populates the
++ * total number of IRQs created by objects in the DPRC.
++ *
++ * Detects objects added and removed from a DPRC and synchronizes the
++ * state of the Linux bus driver, MC by adding and removing
++ * devices accordingly.
++ * Two types of devices can be found in a DPRC: allocatable objects (e.g.,
++ * dpbp, dpmcp) and non-allocatable devices (e.g., dprc, dpni).
++ * All allocatable devices needed to be probed before all non-allocatable
++ * devices, to ensure that device drivers for non-allocatable
++ * devices can allocate any type of allocatable devices.
++ * That is, we need to ensure that the corresponding resource pools are
++ * populated before they can get allocation requests from probe callbacks
++ * of the device drivers for the non-allocatable devices.
++ */
++int dprc_scan_objects(struct fsl_mc_device *mc_bus_dev,
++		      const char *driver_override,
++		      unsigned int *total_irq_count)
++{
++	int num_child_objects;
++	int dprc_get_obj_failures;
++	int error;
++	unsigned int irq_count = mc_bus_dev->obj_desc.irq_count;
++	struct fsl_mc_obj_desc *child_obj_desc_array = NULL;
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc_bus_dev);
++
++	error = dprc_get_obj_count(mc_bus_dev->mc_io,
++				   0,
++				   mc_bus_dev->mc_handle,
++				   &num_child_objects);
++	if (error < 0) {
++		dev_err(&mc_bus_dev->dev, "dprc_get_obj_count() failed: %d\n",
++			error);
++		return error;
++	}
++
++	if (num_child_objects != 0) {
++		int i;
++
++		child_obj_desc_array =
++		    devm_kmalloc_array(&mc_bus_dev->dev, num_child_objects,
++				       sizeof(*child_obj_desc_array),
++				       GFP_KERNEL);
++		if (!child_obj_desc_array)
++			return -ENOMEM;
++
++		/*
++		 * Discover objects currently present in the physical DPRC:
++		 */
++		dprc_get_obj_failures = 0;
++		for (i = 0; i < num_child_objects; i++) {
++			struct fsl_mc_obj_desc *obj_desc =
++			    &child_obj_desc_array[i];
++
++			error = dprc_get_obj(mc_bus_dev->mc_io,
++					     0,
++					     mc_bus_dev->mc_handle,
++					     i, obj_desc);
++			if (error < 0) {
++				dev_err(&mc_bus_dev->dev,
++					"dprc_get_obj(i=%d) failed: %d\n",
++					i, error);
++				/*
++				 * Mark the obj entry as "invalid", by using the
++				 * empty string as obj type:
++				 */
++				obj_desc->type[0] = '\0';
++				obj_desc->id = error;
++				dprc_get_obj_failures++;
++				continue;
++			}
++
++			/*
++			 * add a quirk for all versions of dpsec < 4.0...none
++			 * are coherent regardless of what the MC reports.
++			 */
++			if ((strcmp(obj_desc->type, "dpseci") == 0) &&
++			    (obj_desc->ver_major < 4))
++				obj_desc->flags |=
++					FSL_MC_OBJ_FLAG_NO_MEM_SHAREABILITY;
++
++			irq_count += obj_desc->irq_count;
++			dev_dbg(&mc_bus_dev->dev,
++				"Discovered object: type %s, id %d\n",
++				obj_desc->type, obj_desc->id);
++		}
++
++		if (dprc_get_obj_failures != 0) {
++			dev_err(&mc_bus_dev->dev,
++				"%d out of %d devices could not be retrieved\n",
++				dprc_get_obj_failures, num_child_objects);
++		}
++	}
++
++	/*
++	 * Allocate IRQ's before binding the scanned devices with their
++	 * respective drivers.
++	 */
++	if (dev_get_msi_domain(&mc_bus_dev->dev) && !mc_bus->irq_resources) {
++		if (irq_count > FSL_MC_IRQ_POOL_MAX_TOTAL_IRQS) {
++			dev_warn(&mc_bus_dev->dev,
++				 "IRQs needed (%u) exceed IRQs preallocated (%u)\n",
++				 irq_count, FSL_MC_IRQ_POOL_MAX_TOTAL_IRQS);
++		}
++
++		error = fsl_mc_populate_irq_pool(mc_bus,
++				FSL_MC_IRQ_POOL_MAX_TOTAL_IRQS);
++		if (error < 0)
++			return error;
++	}
++
++	if (total_irq_count)
++		*total_irq_count = irq_count;
++
++	dprc_remove_devices(mc_bus_dev, child_obj_desc_array,
++			    num_child_objects);
++
++	dprc_add_new_devices(mc_bus_dev, driver_override, child_obj_desc_array,
++			     num_child_objects);
++
++	if (child_obj_desc_array)
++		devm_kfree(&mc_bus_dev->dev, child_obj_desc_array);
++
++	return 0;
++}
++
++/**
++ * dprc_scan_container - Scans a physical DPRC and synchronizes Linux bus state
++ *
++ * @mc_bus_dev: pointer to the fsl-mc device that represents a DPRC object
++ *
++ * Scans the physical DPRC and synchronizes the state of the Linux
++ * bus driver with the actual state of the MC by adding and removing
++ * devices as appropriate.
++ */
++static int dprc_scan_container(struct fsl_mc_device *mc_bus_dev)
++{
++	int error;
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc_bus_dev);
++
++	fsl_mc_init_all_resource_pools(mc_bus_dev);
++
++	/*
++	 * Discover objects in the DPRC:
++	 */
++	mutex_lock(&mc_bus->scan_mutex);
++	error = dprc_scan_objects(mc_bus_dev, NULL, NULL);
++	mutex_unlock(&mc_bus->scan_mutex);
++	if (error < 0) {
++		fsl_mc_cleanup_all_resource_pools(mc_bus_dev);
++		return error;
++	}
++
++	return 0;
++}
++
++/**
++ * dprc_irq0_handler - Regular ISR for DPRC interrupt 0
++ *
++ * @irq: IRQ number of the interrupt being handled
++ * @arg: Pointer to device structure
++ */
++static irqreturn_t dprc_irq0_handler(int irq_num, void *arg)
++{
++	return IRQ_WAKE_THREAD;
++}
++
++/**
++ * dprc_irq0_handler_thread - Handler thread function for DPRC interrupt 0
++ *
++ * @irq: IRQ number of the interrupt being handled
++ * @arg: Pointer to device structure
++ */
++static irqreturn_t dprc_irq0_handler_thread(int irq_num, void *arg)
++{
++	int error;
++	u32 status;
++	struct device *dev = arg;
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc_dev);
++	struct fsl_mc_io *mc_io = mc_dev->mc_io;
++	struct msi_desc *msi_desc = mc_dev->irqs[0]->msi_desc;
++
++	dev_dbg(dev, "DPRC IRQ %d triggered on CPU %u\n",
++		irq_num, smp_processor_id());
++
++	if (!(mc_dev->flags & FSL_MC_IS_DPRC))
++		return IRQ_HANDLED;
++
++	mutex_lock(&mc_bus->scan_mutex);
++	if (!msi_desc || msi_desc->irq != (u32)irq_num)
++		goto out;
++
++	status = 0;
++	error = dprc_get_irq_status(mc_io, 0, mc_dev->mc_handle, 0,
++				    &status);
++	if (error < 0) {
++		dev_err(dev,
++			"dprc_get_irq_status() failed: %d\n", error);
++		goto out;
++	}
++
++	error = dprc_clear_irq_status(mc_io, 0, mc_dev->mc_handle, 0,
++				      status);
++	if (error < 0) {
++		dev_err(dev,
++			"dprc_clear_irq_status() failed: %d\n", error);
++		goto out;
++	}
++
++	if (status & (DPRC_IRQ_EVENT_OBJ_ADDED |
++		      DPRC_IRQ_EVENT_OBJ_REMOVED |
++		      DPRC_IRQ_EVENT_CONTAINER_DESTROYED |
++		      DPRC_IRQ_EVENT_OBJ_DESTROYED |
++		      DPRC_IRQ_EVENT_OBJ_CREATED)) {
++		unsigned int irq_count;
++
++		error = dprc_scan_objects(mc_dev, NULL, &irq_count);
++		if (error < 0) {
++			/*
++			 * If the error is -ENXIO, we ignore it, as it indicates
++			 * that the object scan was aborted, as we detected that
++			 * an object was removed from the DPRC in the MC, while
++			 * we were scanning the DPRC.
++			 */
++			if (error != -ENXIO) {
++				dev_err(dev, "dprc_scan_objects() failed: %d\n",
++					error);
++			}
++
++			goto out;
++		}
++
++		if (irq_count > FSL_MC_IRQ_POOL_MAX_TOTAL_IRQS) {
++			dev_warn(dev,
++				 "IRQs needed (%u) exceed IRQs preallocated (%u)\n",
++				 irq_count, FSL_MC_IRQ_POOL_MAX_TOTAL_IRQS);
++		}
++	}
++
++out:
++	mutex_unlock(&mc_bus->scan_mutex);
++	return IRQ_HANDLED;
++}
++
++/*
++ * Disable and clear interrupt for a given DPRC object
++ */
++static int disable_dprc_irq(struct fsl_mc_device *mc_dev)
++{
++	int error;
++	struct fsl_mc_io *mc_io = mc_dev->mc_io;
++
++	/*
++	 * Disable generation of interrupt, while we configure it:
++	 */
++	error = dprc_set_irq_enable(mc_io, 0, mc_dev->mc_handle, 0, 0);
++	if (error < 0) {
++		dev_err(&mc_dev->dev,
++			"Disabling DPRC IRQ failed: dprc_set_irq_enable() failed: %d\n",
++			error);
++		return error;
++	}
++
++	/*
++	 * Disable all interrupt causes for the interrupt:
++	 */
++	error = dprc_set_irq_mask(mc_io, 0, mc_dev->mc_handle, 0, 0x0);
++	if (error < 0) {
++		dev_err(&mc_dev->dev,
++			"Disabling DPRC IRQ failed: dprc_set_irq_mask() failed: %d\n",
++			error);
++		return error;
++	}
++
++	/*
++	 * Clear any leftover interrupts:
++	 */
++	error = dprc_clear_irq_status(mc_io, 0, mc_dev->mc_handle, 0, ~0x0U);
++	if (error < 0) {
++		dev_err(&mc_dev->dev,
++			"Disabling DPRC IRQ failed: dprc_clear_irq_status() failed: %d\n",
++			error);
++		return error;
++	}
++
++	return 0;
++}
++
++static int register_dprc_irq_handler(struct fsl_mc_device *mc_dev)
++{
++	int error;
++	struct fsl_mc_device_irq *irq = mc_dev->irqs[0];
++
++	/*
++	 * NOTE: devm_request_threaded_irq() invokes the device-specific
++	 * function that programs the MSI physically in the device
++	 */
++	error = devm_request_threaded_irq(&mc_dev->dev,
++					  irq->msi_desc->irq,
++					  dprc_irq0_handler,
++					  dprc_irq0_handler_thread,
++					  IRQF_NO_SUSPEND | IRQF_ONESHOT,
++					  dev_name(&mc_dev->dev),
++					  &mc_dev->dev);
++	if (error < 0) {
++		dev_err(&mc_dev->dev,
++			"devm_request_threaded_irq() failed: %d\n",
++			error);
++		return error;
++	}
++
++	return 0;
++}
++
++static int enable_dprc_irq(struct fsl_mc_device *mc_dev)
++{
++	int error;
++
++	/*
++	 * Enable all interrupt causes for the interrupt:
++	 */
++	error = dprc_set_irq_mask(mc_dev->mc_io, 0, mc_dev->mc_handle, 0,
++				  ~0x0u);
++	if (error < 0) {
++		dev_err(&mc_dev->dev,
++			"Enabling DPRC IRQ failed: dprc_set_irq_mask() failed: %d\n",
++			error);
++
++		return error;
++	}
++
++	/*
++	 * Enable generation of the interrupt:
++	 */
++	error = dprc_set_irq_enable(mc_dev->mc_io, 0, mc_dev->mc_handle, 0, 1);
++	if (error < 0) {
++		dev_err(&mc_dev->dev,
++			"Enabling DPRC IRQ failed: dprc_set_irq_enable() failed: %d\n",
++			error);
++
++		return error;
++	}
++
++	return 0;
++}
++
++/*
++ * Setup interrupt for a given DPRC device
++ */
++static int dprc_setup_irq(struct fsl_mc_device *mc_dev)
++{
++	int error;
++
++	error = fsl_mc_allocate_irqs(mc_dev);
++	if (error < 0)
++		return error;
++
++	error = disable_dprc_irq(mc_dev);
++	if (error < 0)
++		goto error_free_irqs;
++
++	error = register_dprc_irq_handler(mc_dev);
++	if (error < 0)
++		goto error_free_irqs;
++
++	error = enable_dprc_irq(mc_dev);
++	if (error < 0)
++		goto error_free_irqs;
++
++	return 0;
++
++error_free_irqs:
++	fsl_mc_free_irqs(mc_dev);
++	return error;
++}
++
++/**
++ * dprc_probe - callback invoked when a DPRC is being bound to this driver
++ *
++ * @mc_dev: Pointer to fsl-mc device representing a DPRC
++ *
++ * It opens the physical DPRC in the MC.
++ * It scans the DPRC to discover the MC objects contained in it.
++ * It creates the interrupt pool for the MC bus associated with the DPRC.
++ * It configures the interrupts for the DPRC device itself.
++ */
++static int dprc_probe(struct fsl_mc_device *mc_dev)
++{
++	int error;
++	size_t region_size;
++	struct device *parent_dev = mc_dev->dev.parent;
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc_dev);
++	bool mc_io_created = false;
++	bool msi_domain_set = false;
++	u16 major_ver, minor_ver;
++
++	if (!is_fsl_mc_bus_dprc(mc_dev))
++		return -EINVAL;
++
++	if (dev_get_msi_domain(&mc_dev->dev))
++		return -EINVAL;
++
++	if (!mc_dev->mc_io) {
++		/*
++		 * This is a child DPRC:
++		 */
++		if (!dev_is_fsl_mc(parent_dev))
++			return -EINVAL;
++
++		if (mc_dev->obj_desc.region_count == 0)
++			return -EINVAL;
++
++		region_size = resource_size(mc_dev->regions);
++
++		error = fsl_create_mc_io(&mc_dev->dev,
++					 mc_dev->regions[0].start,
++					 region_size,
++					 NULL,
++					 FSL_MC_IO_ATOMIC_CONTEXT_PORTAL,
++					 &mc_dev->mc_io);
++		if (error < 0)
++			return error;
++
++		mc_io_created = true;
++
++		/*
++		 * Inherit parent MSI domain:
++		 */
++		dev_set_msi_domain(&mc_dev->dev,
++				   dev_get_msi_domain(parent_dev));
++		msi_domain_set = true;
++	} else {
++		/*
++		 * This is a root DPRC
++		 */
++		struct irq_domain *mc_msi_domain;
++
++		if (dev_is_fsl_mc(parent_dev))
++			return -EINVAL;
++
++		error = fsl_mc_find_msi_domain(parent_dev,
++					       &mc_msi_domain);
++		if (error < 0) {
++			dev_warn(&mc_dev->dev,
++				 "WARNING: MC bus without interrupt support\n");
++		} else {
++			dev_set_msi_domain(&mc_dev->dev, mc_msi_domain);
++			msi_domain_set = true;
++		}
++	}
++
++	error = dprc_open(mc_dev->mc_io, 0, mc_dev->obj_desc.id,
++			  &mc_dev->mc_handle);
++	if (error < 0) {
++		dev_err(&mc_dev->dev, "dprc_open() failed: %d\n", error);
++		goto error_cleanup_msi_domain;
++	}
++
++	error = dprc_get_attributes(mc_dev->mc_io, 0, mc_dev->mc_handle,
++				    &mc_bus->dprc_attr);
++	if (error < 0) {
++		dev_err(&mc_dev->dev, "dprc_get_attributes() failed: %d\n",
++			error);
++		goto error_cleanup_open;
++	}
++
++	error = dprc_get_api_version(mc_dev->mc_io, 0,
++				     &major_ver,
++				     &minor_ver);
++	if (error < 0) {
++		dev_err(&mc_dev->dev, "dprc_get_api_version() failed: %d\n",
++			error);
++		goto error_cleanup_open;
++	}
++
++	if (major_ver < DPRC_MIN_VER_MAJOR ||
++	    (major_ver == DPRC_MIN_VER_MAJOR &&
++	     minor_ver < DPRC_MIN_VER_MINOR)) {
++		dev_err(&mc_dev->dev,
++			"ERROR: DPRC version %d.%d not supported\n",
++			major_ver, minor_ver);
++		error = -ENOTSUPP;
++		goto error_cleanup_open;
++	}
++
++	mutex_init(&mc_bus->scan_mutex);
++
++	/*
++	 * Discover MC objects in DPRC object:
++	 */
++	error = dprc_scan_container(mc_dev);
++	if (error < 0)
++		goto error_cleanup_open;
++
++	/*
++	 * Configure interrupt for the DPRC object associated with this MC bus:
++	 */
++	error = dprc_setup_irq(mc_dev);
++	if (error < 0)
++		goto error_cleanup_open;
++
++	dev_info(&mc_dev->dev, "DPRC device bound to driver");
++	return 0;
++
++error_cleanup_open:
++	(void)dprc_close(mc_dev->mc_io, 0, mc_dev->mc_handle);
++
++error_cleanup_msi_domain:
++	if (msi_domain_set)
++		dev_set_msi_domain(&mc_dev->dev, NULL);
++
++	if (mc_io_created) {
++		fsl_destroy_mc_io(mc_dev->mc_io);
++		mc_dev->mc_io = NULL;
++	}
++
++	return error;
++}
++
++/*
++ * Tear down interrupt for a given DPRC object
++ */
++static void dprc_teardown_irq(struct fsl_mc_device *mc_dev)
++{
++	struct fsl_mc_device_irq *irq = mc_dev->irqs[0];
++
++	(void)disable_dprc_irq(mc_dev);
++
++	devm_free_irq(&mc_dev->dev, irq->msi_desc->irq, &mc_dev->dev);
++
++	fsl_mc_free_irqs(mc_dev);
++}
++
++/**
++ * dprc_remove - callback invoked when a DPRC is being unbound from this driver
++ *
++ * @mc_dev: Pointer to fsl-mc device representing the DPRC
++ *
++ * It removes the DPRC's child objects from Linux (not from the MC) and
++ * closes the DPRC device in the MC.
++ * It tears down the interrupts that were configured for the DPRC device.
++ * It destroys the interrupt pool associated with this MC bus.
++ */
++static int dprc_remove(struct fsl_mc_device *mc_dev)
++{
++	int error;
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc_dev);
++
++	if (!is_fsl_mc_bus_dprc(mc_dev))
++		return -EINVAL;
++	if (!mc_dev->mc_io)
++		return -EINVAL;
++
++	if (!mc_bus->irq_resources)
++		return -EINVAL;
++
++	if (dev_get_msi_domain(&mc_dev->dev))
++		dprc_teardown_irq(mc_dev);
++
++	device_for_each_child(&mc_dev->dev, NULL, __fsl_mc_device_remove);
++
++	if (dev_get_msi_domain(&mc_dev->dev)) {
++		fsl_mc_cleanup_irq_pool(mc_bus);
++		dev_set_msi_domain(&mc_dev->dev, NULL);
++	}
++
++	fsl_mc_cleanup_all_resource_pools(mc_dev);
++
++	error = dprc_close(mc_dev->mc_io, 0, mc_dev->mc_handle);
++	if (error < 0)
++		dev_err(&mc_dev->dev, "dprc_close() failed: %d\n", error);
++
++	if (!fsl_mc_is_root_dprc(&mc_dev->dev)) {
++		fsl_destroy_mc_io(mc_dev->mc_io);
++		mc_dev->mc_io = NULL;
++	}
++
++	dev_info(&mc_dev->dev, "DPRC device unbound from driver");
++	return 0;
++}
++
++static const struct fsl_mc_device_id match_id_table[] = {
++	{
++	 .vendor = FSL_MC_VENDOR_FREESCALE,
++	 .obj_type = "dprc"},
++	{.vendor = 0x0},
++};
++
++static struct fsl_mc_driver dprc_driver = {
++	.driver = {
++		   .name = FSL_MC_DPRC_DRIVER_NAME,
++		   .owner = THIS_MODULE,
++		   .pm = NULL,
++		   },
++	.match_id_table = match_id_table,
++	.probe = dprc_probe,
++	.remove = dprc_remove,
++};
++
++int __init dprc_driver_init(void)
++{
++	return fsl_mc_driver_register(&dprc_driver);
++}
++
++void dprc_driver_exit(void)
++{
++	fsl_mc_driver_unregister(&dprc_driver);
++}
+diff --git a/drivers/bus/fsl-mc/dprc.c b/drivers/bus/fsl-mc/dprc.c
+new file mode 100644
+index 00000000..9167d6ca
+--- /dev/null
++++ b/drivers/bus/fsl-mc/dprc.c
+@@ -0,0 +1,575 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * Copyright 2013-2016 Freescale Semiconductor Inc.
++ *
++ */
++#include <linux/kernel.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++/**
++ * dprc_open() - Open DPRC object for use
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @container_id: Container ID to open
++ * @token:	Returned token of DPRC object
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ *
++ * @warning	Required before any operation on the object.
++ */
++int dprc_open(struct fsl_mc_io *mc_io,
++	      u32 cmd_flags,
++	      int container_id,
++	      u16 *token)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_open *cmd_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_OPEN, cmd_flags,
++					  0);
++	cmd_params = (struct dprc_cmd_open *)cmd.params;
++	cmd_params->container_id = cpu_to_le32(container_id);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	*token = mc_cmd_hdr_read_token(&cmd);
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(dprc_open);
++
++/**
++ * dprc_close() - Close the control session of the object
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ *
++ * After this function is called, no further operations are
++ * allowed on the object without opening a new control session.
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_close(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       u16 token)
++{
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_CLOSE, cmd_flags,
++					  token);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dprc_close);
++
++/**
++ * dprc_reset_container - Reset child container.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @child_container_id:	ID of the container to reset
++ *
++ * In case a software context crashes or becomes non-responsive, the parent
++ * may wish to reset its resources container before the software context is
++ * restarted.
++ *
++ * This routine informs all objects assigned to the child container that the
++ * container is being reset, so they may perform any cleanup operations that are
++ * needed. All objects handles that were owned by the child container shall be
++ * closed.
++ *
++ * Note that such request may be submitted even if the child software context
++ * has not crashed, but the resulting object cleanup operations will not be
++ * aware of that.
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_reset_container(struct fsl_mc_io *mc_io,
++			 u32 cmd_flags,
++			 u16 token,
++			 int child_container_id)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_reset_container *cmd_params;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_RESET_CONT,
++					  cmd_flags, token);
++	cmd_params = (struct dprc_cmd_reset_container *)cmd.params;
++	cmd_params->child_container_id = cpu_to_le32(child_container_id);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dprc_reset_container);
++
++/**
++ * dprc_set_irq() - Set IRQ information for the DPRC to trigger an interrupt.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @irq_index:	Identifies the interrupt index to configure
++ * @irq_cfg:	IRQ configuration
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_set_irq(struct fsl_mc_io *mc_io,
++		 u32 cmd_flags,
++		 u16 token,
++		 u8 irq_index,
++		 struct dprc_irq_cfg *irq_cfg)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_set_irq *cmd_params;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_SET_IRQ,
++					  cmd_flags,
++					  token);
++	cmd_params = (struct dprc_cmd_set_irq *)cmd.params;
++	cmd_params->irq_val = cpu_to_le32(irq_cfg->val);
++	cmd_params->irq_index = irq_index;
++	cmd_params->irq_addr = cpu_to_le64(irq_cfg->paddr);
++	cmd_params->irq_num = cpu_to_le32(irq_cfg->irq_num);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++
++/**
++ * dprc_set_irq_enable() - Set overall interrupt state.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @irq_index:	The interrupt index to configure
++ * @en:		Interrupt state - enable = 1, disable = 0
++ *
++ * Allows GPP software to control when interrupts are generated.
++ * Each interrupt can have up to 32 causes.  The enable/disable control's the
++ * overall interrupt state. if the interrupt is disabled no causes will cause
++ * an interrupt.
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_set_irq_enable(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			u8 irq_index,
++			u8 en)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_set_irq_enable *cmd_params;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_SET_IRQ_ENABLE,
++					  cmd_flags, token);
++	cmd_params = (struct dprc_cmd_set_irq_enable *)cmd.params;
++	cmd_params->enable = en & DPRC_ENABLE;
++	cmd_params->irq_index = irq_index;
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++
++/**
++ * dprc_set_irq_mask() - Set interrupt mask.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @irq_index:	The interrupt index to configure
++ * @mask:	event mask to trigger interrupt;
++ *			each bit:
++ *				0 = ignore event
++ *				1 = consider event for asserting irq
++ *
++ * Every interrupt can have up to 32 causes and the interrupt model supports
++ * masking/unmasking each cause independently
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_set_irq_mask(struct fsl_mc_io *mc_io,
++		      u32 cmd_flags,
++		      u16 token,
++		      u8 irq_index,
++		      u32 mask)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_set_irq_mask *cmd_params;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_SET_IRQ_MASK,
++					  cmd_flags, token);
++	cmd_params = (struct dprc_cmd_set_irq_mask *)cmd.params;
++	cmd_params->mask = cpu_to_le32(mask);
++	cmd_params->irq_index = irq_index;
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++
++/**
++ * dprc_get_irq_status() - Get the current status of any pending interrupts.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @irq_index:	The interrupt index to configure
++ * @status:	Returned interrupts status - one bit per cause:
++ *			0 = no interrupt pending
++ *			1 = interrupt pending
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_get_irq_status(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			u8 irq_index,
++			u32 *status)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_get_irq_status *cmd_params;
++	struct dprc_rsp_get_irq_status *rsp_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_GET_IRQ_STATUS,
++					  cmd_flags, token);
++	cmd_params = (struct dprc_cmd_get_irq_status *)cmd.params;
++	cmd_params->status = cpu_to_le32(*status);
++	cmd_params->irq_index = irq_index;
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	rsp_params = (struct dprc_rsp_get_irq_status *)cmd.params;
++	*status = le32_to_cpu(rsp_params->status);
++
++	return 0;
++}
++
++/**
++ * dprc_clear_irq_status() - Clear a pending interrupt's status
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @irq_index:	The interrupt index to configure
++ * @status:	bits to clear (W1C) - one bit per cause:
++ *					0 = don't change
++ *					1 = clear status bit
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_clear_irq_status(struct fsl_mc_io *mc_io,
++			  u32 cmd_flags,
++			  u16 token,
++			  u8 irq_index,
++			  u32 status)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_clear_irq_status *cmd_params;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_CLEAR_IRQ_STATUS,
++					  cmd_flags, token);
++	cmd_params = (struct dprc_cmd_clear_irq_status *)cmd.params;
++	cmd_params->status = cpu_to_le32(status);
++	cmd_params->irq_index = irq_index;
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++
++/**
++ * dprc_get_attributes() - Obtains container attributes
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @attributes	Returned container attributes
++ *
++ * Return:     '0' on Success; Error code otherwise.
++ */
++int dprc_get_attributes(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			struct dprc_attributes *attr)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_rsp_get_attributes *rsp_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_GET_ATTR,
++					  cmd_flags,
++					  token);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	rsp_params = (struct dprc_rsp_get_attributes *)cmd.params;
++	attr->container_id = le32_to_cpu(rsp_params->container_id);
++	attr->icid = le32_to_cpu(rsp_params->icid);
++	attr->options = le32_to_cpu(rsp_params->options);
++	attr->portal_id = le32_to_cpu(rsp_params->portal_id);
++
++	return 0;
++}
++
++/**
++ * dprc_get_obj_count() - Obtains the number of objects in the DPRC
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @obj_count:	Number of objects assigned to the DPRC
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_get_obj_count(struct fsl_mc_io *mc_io,
++		       u32 cmd_flags,
++		       u16 token,
++		       int *obj_count)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_rsp_get_obj_count *rsp_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_GET_OBJ_COUNT,
++					  cmd_flags, token);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	rsp_params = (struct dprc_rsp_get_obj_count *)cmd.params;
++	*obj_count = le32_to_cpu(rsp_params->obj_count);
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(dprc_get_obj_count);
++
++/**
++ * dprc_get_obj() - Get general information on an object
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @obj_index:	Index of the object to be queried (< obj_count)
++ * @obj_desc:	Returns the requested object descriptor
++ *
++ * The object descriptors are retrieved one by one by incrementing
++ * obj_index up to (not including) the value of obj_count returned
++ * from dprc_get_obj_count(). dprc_get_obj_count() must
++ * be called prior to dprc_get_obj().
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_get_obj(struct fsl_mc_io *mc_io,
++		 u32 cmd_flags,
++		 u16 token,
++		 int obj_index,
++		 struct fsl_mc_obj_desc *obj_desc)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_get_obj *cmd_params;
++	struct dprc_rsp_get_obj *rsp_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_GET_OBJ,
++					  cmd_flags,
++					  token);
++	cmd_params = (struct dprc_cmd_get_obj *)cmd.params;
++	cmd_params->obj_index = cpu_to_le32(obj_index);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	rsp_params = (struct dprc_rsp_get_obj *)cmd.params;
++	obj_desc->id = le32_to_cpu(rsp_params->id);
++	obj_desc->vendor = le16_to_cpu(rsp_params->vendor);
++	obj_desc->irq_count = rsp_params->irq_count;
++	obj_desc->region_count = rsp_params->region_count;
++	obj_desc->state = le32_to_cpu(rsp_params->state);
++	obj_desc->ver_major = le16_to_cpu(rsp_params->version_major);
++	obj_desc->ver_minor = le16_to_cpu(rsp_params->version_minor);
++	obj_desc->flags = le16_to_cpu(rsp_params->flags);
++	strncpy(obj_desc->type, rsp_params->type, 16);
++	obj_desc->type[15] = '\0';
++	strncpy(obj_desc->label, rsp_params->label, 16);
++	obj_desc->label[15] = '\0';
++	return 0;
++}
++EXPORT_SYMBOL_GPL(dprc_get_obj);
++
++/**
++ * dprc_set_obj_irq() - Set IRQ information for object to trigger an interrupt.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @obj_type:	Type of the object to set its IRQ
++ * @obj_id:	ID of the object to set its IRQ
++ * @irq_index:	The interrupt index to configure
++ * @irq_cfg:	IRQ configuration
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_set_obj_irq(struct fsl_mc_io *mc_io,
++		     u32 cmd_flags,
++		     u16 token,
++		     char *obj_type,
++		     int obj_id,
++		     u8 irq_index,
++		     struct dprc_irq_cfg *irq_cfg)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_set_obj_irq *cmd_params;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_SET_OBJ_IRQ,
++					  cmd_flags,
++					  token);
++	cmd_params = (struct dprc_cmd_set_obj_irq *)cmd.params;
++	cmd_params->irq_val = cpu_to_le32(irq_cfg->val);
++	cmd_params->irq_index = irq_index;
++	cmd_params->irq_addr = cpu_to_le64(irq_cfg->paddr);
++	cmd_params->irq_num = cpu_to_le32(irq_cfg->irq_num);
++	cmd_params->obj_id = cpu_to_le32(obj_id);
++	strncpy(cmd_params->obj_type, obj_type, 16);
++	cmd_params->obj_type[15] = '\0';
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++EXPORT_SYMBOL_GPL(dprc_set_obj_irq);
++
++/**
++ * dprc_get_obj_region() - Get region information for a specified object.
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPRC object
++ * @obj_type;	Object type as returned in dprc_get_obj()
++ * @obj_id:	Unique object instance as returned in dprc_get_obj()
++ * @region_index: The specific region to query
++ * @region_desc:  Returns the requested region descriptor
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_get_obj_region(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			char *obj_type,
++			int obj_id,
++			u8 region_index,
++			struct dprc_region_desc *region_desc)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dprc_cmd_get_obj_region *cmd_params;
++	struct dprc_rsp_get_obj_region *rsp_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_GET_OBJ_REG,
++					  cmd_flags, token);
++	cmd_params = (struct dprc_cmd_get_obj_region *)cmd.params;
++	cmd_params->obj_id = cpu_to_le32(obj_id);
++	cmd_params->region_index = region_index;
++	strncpy(cmd_params->obj_type, obj_type, 16);
++	cmd_params->obj_type[15] = '\0';
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	rsp_params = (struct dprc_rsp_get_obj_region *)cmd.params;
++	region_desc->base_offset = le32_to_cpu(rsp_params->base_addr);
++	region_desc->size = le32_to_cpu(rsp_params->size);
++	region_desc->type = rsp_params->type;
++	region_desc->flags = le32_to_cpu(rsp_params->flags);
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(dprc_get_obj_region);
++
++/**
++ * dprc_get_api_version - Get Data Path Resource Container API version
++ * @mc_io:	Pointer to Mc portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @major_ver:	Major version of Data Path Resource Container API
++ * @minor_ver:	Minor version of Data Path Resource Container API
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_get_api_version(struct fsl_mc_io *mc_io,
++			 u32 cmd_flags,
++			 u16 *major_ver,
++			 u16 *minor_ver)
++{
++	struct fsl_mc_command cmd = { 0 };
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_GET_API_VERSION,
++					  cmd_flags, 0);
++
++	/* send command to mc */
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	mc_cmd_read_api_version(&cmd, major_ver, minor_ver);
++
++	return 0;
++}
++
++/**
++ * dprc_get_container_id - Get container ID associated with a given portal.
++ * @mc_io:		Pointer to Mc portal's I/O object
++ * @cmd_flags:		Command flags; one or more of 'MC_CMD_FLAG_'
++ * @container_id:	Requested container id
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dprc_get_container_id(struct fsl_mc_io *mc_io,
++			  u32 cmd_flags,
++			  int *container_id)
++{
++	struct fsl_mc_command cmd = { 0 };
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPRC_CMDID_GET_CONT_ID,
++					  cmd_flags,
++					  0);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	*container_id = (int)mc_cmd_read_object_id(&cmd);
++
++	return 0;
++}
+diff --git a/drivers/bus/fsl-mc/fsl-mc-allocator.c b/drivers/bus/fsl-mc/fsl-mc-allocator.c
+new file mode 100644
+index 00000000..fc1efa19
+--- /dev/null
++++ b/drivers/bus/fsl-mc/fsl-mc-allocator.c
+@@ -0,0 +1,655 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * fsl-mc object allocator driver
++ *
++ * Copyright (C) 2013-2016 Freescale Semiconductor, Inc.
++ *
++ */
++
++#include <linux/module.h>
++#include <linux/msi.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++static bool __must_check fsl_mc_is_allocatable(struct fsl_mc_device *mc_dev)
++{
++	return is_fsl_mc_bus_dpbp(mc_dev) ||
++	       is_fsl_mc_bus_dpmcp(mc_dev) ||
++	       is_fsl_mc_bus_dpcon(mc_dev);
++}
++
++/**
++ * fsl_mc_resource_pool_add_device - add allocatable object to a resource
++ * pool of a given fsl-mc bus
++ *
++ * @mc_bus: pointer to the fsl-mc bus
++ * @pool_type: pool type
++ * @mc_dev: pointer to allocatable fsl-mc device
++ */
++static int __must_check fsl_mc_resource_pool_add_device(struct fsl_mc_bus
++								*mc_bus,
++							enum fsl_mc_pool_type
++								pool_type,
++							struct fsl_mc_device
++								*mc_dev)
++{
++	struct fsl_mc_resource_pool *res_pool;
++	struct fsl_mc_resource *resource;
++	struct fsl_mc_device *mc_bus_dev = &mc_bus->mc_dev;
++	int error = -EINVAL;
++
++	if (pool_type < 0 || pool_type >= FSL_MC_NUM_POOL_TYPES)
++		goto out;
++	if (!fsl_mc_is_allocatable(mc_dev))
++		goto out;
++	if (mc_dev->resource)
++		goto out;
++
++	res_pool = &mc_bus->resource_pools[pool_type];
++	if (res_pool->type != pool_type)
++		goto out;
++	if (res_pool->mc_bus != mc_bus)
++		goto out;
++
++	mutex_lock(&res_pool->mutex);
++
++	if (res_pool->max_count < 0)
++		goto out_unlock;
++	if (res_pool->free_count < 0 ||
++	    res_pool->free_count > res_pool->max_count)
++		goto out_unlock;
++
++	resource = devm_kzalloc(&mc_bus_dev->dev, sizeof(*resource),
++				GFP_KERNEL);
++	if (!resource) {
++		error = -ENOMEM;
++		dev_err(&mc_bus_dev->dev,
++			"Failed to allocate memory for fsl_mc_resource\n");
++		goto out_unlock;
++	}
++
++	resource->type = pool_type;
++	resource->id = mc_dev->obj_desc.id;
++	resource->data = mc_dev;
++	resource->parent_pool = res_pool;
++	INIT_LIST_HEAD(&resource->node);
++	list_add_tail(&resource->node, &res_pool->free_list);
++	mc_dev->resource = resource;
++	res_pool->free_count++;
++	res_pool->max_count++;
++	error = 0;
++out_unlock:
++	mutex_unlock(&res_pool->mutex);
++out:
++	return error;
++}
++
++/**
++ * fsl_mc_resource_pool_remove_device - remove an allocatable device from a
++ * resource pool
++ *
++ * @mc_dev: pointer to allocatable fsl-mc device
++ *
++ * It permanently removes an allocatable fsl-mc device from the resource
++ * pool. It's an error if the device is in use.
++ */
++static int __must_check fsl_mc_resource_pool_remove_device(struct fsl_mc_device
++								   *mc_dev)
++{
++	struct fsl_mc_device *mc_bus_dev;
++	struct fsl_mc_bus *mc_bus;
++	struct fsl_mc_resource_pool *res_pool;
++	struct fsl_mc_resource *resource;
++	int error = -EINVAL;
++
++	if (!fsl_mc_is_allocatable(mc_dev))
++		goto out;
++
++	resource = mc_dev->resource;
++	if (!resource || resource->data != mc_dev)
++		goto out;
++
++	mc_bus_dev = to_fsl_mc_device(mc_dev->dev.parent);
++	mc_bus = to_fsl_mc_bus(mc_bus_dev);
++	res_pool = resource->parent_pool;
++	if (res_pool != &mc_bus->resource_pools[resource->type])
++		goto out;
++
++	mutex_lock(&res_pool->mutex);
++
++	if (res_pool->max_count <= 0)
++		goto out_unlock;
++	if (res_pool->free_count <= 0 ||
++	    res_pool->free_count > res_pool->max_count)
++		goto out_unlock;
++
++	/*
++	 * If the device is currently allocated, its resource is not
++	 * in the free list and thus, the device cannot be removed.
++	 */
++	if (list_empty(&resource->node)) {
++		error = -EBUSY;
++		dev_err(&mc_bus_dev->dev,
++			"Device %s cannot be removed from resource pool\n",
++			dev_name(&mc_dev->dev));
++		goto out_unlock;
++	}
++
++	list_del_init(&resource->node);
++	res_pool->free_count--;
++	res_pool->max_count--;
++
++	devm_kfree(&mc_bus_dev->dev, resource);
++	mc_dev->resource = NULL;
++	error = 0;
++out_unlock:
++	mutex_unlock(&res_pool->mutex);
++out:
++	return error;
++}
++
++static const char *const fsl_mc_pool_type_strings[] = {
++	[FSL_MC_POOL_DPMCP] = "dpmcp",
++	[FSL_MC_POOL_DPBP] = "dpbp",
++	[FSL_MC_POOL_DPCON] = "dpcon",
++	[FSL_MC_POOL_IRQ] = "irq",
++};
++
++static int __must_check object_type_to_pool_type(const char *object_type,
++						 enum fsl_mc_pool_type
++								*pool_type)
++{
++	unsigned int i;
++
++	for (i = 0; i < ARRAY_SIZE(fsl_mc_pool_type_strings); i++) {
++		if (strcmp(object_type, fsl_mc_pool_type_strings[i]) == 0) {
++			*pool_type = i;
++			return 0;
++		}
++	}
++
++	return -EINVAL;
++}
++
++int __must_check fsl_mc_resource_allocate(struct fsl_mc_bus *mc_bus,
++					  enum fsl_mc_pool_type pool_type,
++					  struct fsl_mc_resource **new_resource)
++{
++	struct fsl_mc_resource_pool *res_pool;
++	struct fsl_mc_resource *resource;
++	struct fsl_mc_device *mc_bus_dev = &mc_bus->mc_dev;
++	int error = -EINVAL;
++
++	BUILD_BUG_ON(ARRAY_SIZE(fsl_mc_pool_type_strings) !=
++		     FSL_MC_NUM_POOL_TYPES);
++
++	*new_resource = NULL;
++	if (pool_type < 0 || pool_type >= FSL_MC_NUM_POOL_TYPES)
++		goto out;
++
++	res_pool = &mc_bus->resource_pools[pool_type];
++	if (res_pool->mc_bus != mc_bus)
++		goto out;
++
++	mutex_lock(&res_pool->mutex);
++	resource = list_first_entry_or_null(&res_pool->free_list,
++					    struct fsl_mc_resource, node);
++
++	if (!resource) {
++		error = -ENXIO;
++		dev_err(&mc_bus_dev->dev,
++			"No more resources of type %s left\n",
++			fsl_mc_pool_type_strings[pool_type]);
++		goto out_unlock;
++	}
++
++	if (resource->type != pool_type)
++		goto out_unlock;
++	if (resource->parent_pool != res_pool)
++		goto out_unlock;
++	if (res_pool->free_count <= 0 ||
++	    res_pool->free_count > res_pool->max_count)
++		goto out_unlock;
++
++	list_del_init(&resource->node);
++
++	res_pool->free_count--;
++	error = 0;
++out_unlock:
++	mutex_unlock(&res_pool->mutex);
++	*new_resource = resource;
++out:
++	return error;
++}
++EXPORT_SYMBOL_GPL(fsl_mc_resource_allocate);
++
++void fsl_mc_resource_free(struct fsl_mc_resource *resource)
++{
++	struct fsl_mc_resource_pool *res_pool;
++
++	res_pool = resource->parent_pool;
++	if (resource->type != res_pool->type)
++		return;
++
++	mutex_lock(&res_pool->mutex);
++	if (res_pool->free_count < 0 ||
++	    res_pool->free_count >= res_pool->max_count)
++		goto out_unlock;
++
++	if (!list_empty(&resource->node))
++		goto out_unlock;
++
++	list_add_tail(&resource->node, &res_pool->free_list);
++	res_pool->free_count++;
++out_unlock:
++	mutex_unlock(&res_pool->mutex);
++}
++EXPORT_SYMBOL_GPL(fsl_mc_resource_free);
++
++/**
++ * fsl_mc_object_allocate - Allocates an fsl-mc object of the given
++ * pool type from a given fsl-mc bus instance
++ *
++ * @mc_dev: fsl-mc device which is used in conjunction with the
++ * allocated object
++ * @pool_type: pool type
++ * @new_mc_dev: pointer to area where the pointer to the allocated device
++ * is to be returned
++ *
++ * Allocatable objects are always used in conjunction with some functional
++ * device.  This function allocates an object of the specified type from
++ * the DPRC containing the functional device.
++ *
++ * NOTE: pool_type must be different from FSL_MC_POOL_MCP, since MC
++ * portals are allocated using fsl_mc_portal_allocate(), instead of
++ * this function.
++ */
++int __must_check fsl_mc_object_allocate(struct fsl_mc_device *mc_dev,
++					enum fsl_mc_pool_type pool_type,
++					struct fsl_mc_device **new_mc_adev)
++{
++	struct fsl_mc_device *mc_bus_dev;
++	struct fsl_mc_bus *mc_bus;
++	struct fsl_mc_device *mc_adev;
++	int error = -EINVAL;
++	struct fsl_mc_resource *resource = NULL;
++
++	*new_mc_adev = NULL;
++	if (mc_dev->flags & FSL_MC_IS_DPRC)
++		goto error;
++
++	if (!dev_is_fsl_mc(mc_dev->dev.parent))
++		goto error;
++
++	if (pool_type == FSL_MC_POOL_DPMCP)
++		goto error;
++
++	mc_bus_dev = to_fsl_mc_device(mc_dev->dev.parent);
++	mc_bus = to_fsl_mc_bus(mc_bus_dev);
++	error = fsl_mc_resource_allocate(mc_bus, pool_type, &resource);
++	if (error < 0)
++		goto error;
++
++	mc_adev = resource->data;
++	if (!mc_adev)
++		goto error;
++
++	*new_mc_adev = mc_adev;
++	return 0;
++error:
++	if (resource)
++		fsl_mc_resource_free(resource);
++
++	return error;
++}
++EXPORT_SYMBOL_GPL(fsl_mc_object_allocate);
++
++/**
++ * fsl_mc_object_free - Returns an fsl-mc object to the resource
++ * pool where it came from.
++ * @mc_adev: Pointer to the fsl-mc device
++ */
++void fsl_mc_object_free(struct fsl_mc_device *mc_adev)
++{
++	struct fsl_mc_resource *resource;
++
++	resource = mc_adev->resource;
++	if (resource->type == FSL_MC_POOL_DPMCP)
++		return;
++	if (resource->data != mc_adev)
++		return;
++
++	fsl_mc_resource_free(resource);
++}
++EXPORT_SYMBOL_GPL(fsl_mc_object_free);
++
++/*
++ * A DPRC and the devices in the DPRC all share the same GIC-ITS device
++ * ID.  A block of IRQs is pre-allocated and maintained in a pool
++ * from which devices can allocate them when needed.
++ */
++
++/*
++ * Initialize the interrupt pool associated with an fsl-mc bus.
++ * It allocates a block of IRQs from the GIC-ITS.
++ */
++int fsl_mc_populate_irq_pool(struct fsl_mc_bus *mc_bus,
++			     unsigned int irq_count)
++{
++	unsigned int i;
++	struct msi_desc *msi_desc;
++	struct fsl_mc_device_irq *irq_resources;
++	struct fsl_mc_device_irq *mc_dev_irq;
++	int error;
++	struct fsl_mc_device *mc_bus_dev = &mc_bus->mc_dev;
++	struct fsl_mc_resource_pool *res_pool =
++			&mc_bus->resource_pools[FSL_MC_POOL_IRQ];
++
++	if (irq_count == 0 ||
++	    irq_count > FSL_MC_IRQ_POOL_MAX_TOTAL_IRQS)
++		return -EINVAL;
++
++	error = fsl_mc_msi_domain_alloc_irqs(&mc_bus_dev->dev, irq_count);
++	if (error < 0)
++		return error;
++
++	irq_resources = devm_kzalloc(&mc_bus_dev->dev,
++				     sizeof(*irq_resources) * irq_count,
++				     GFP_KERNEL);
++	if (!irq_resources) {
++		error = -ENOMEM;
++		goto cleanup_msi_irqs;
++	}
++
++	for (i = 0; i < irq_count; i++) {
++		mc_dev_irq = &irq_resources[i];
++
++		/*
++		 * NOTE: This mc_dev_irq's MSI addr/value pair will be set
++		 * by the fsl_mc_msi_write_msg() callback
++		 */
++		mc_dev_irq->resource.type = res_pool->type;
++		mc_dev_irq->resource.data = mc_dev_irq;
++		mc_dev_irq->resource.parent_pool = res_pool;
++		INIT_LIST_HEAD(&mc_dev_irq->resource.node);
++		list_add_tail(&mc_dev_irq->resource.node, &res_pool->free_list);
++	}
++
++	for_each_msi_entry(msi_desc, &mc_bus_dev->dev) {
++		mc_dev_irq = &irq_resources[msi_desc->fsl_mc.msi_index];
++		mc_dev_irq->msi_desc = msi_desc;
++		mc_dev_irq->resource.id = msi_desc->irq;
++	}
++
++	res_pool->max_count = irq_count;
++	res_pool->free_count = irq_count;
++	mc_bus->irq_resources = irq_resources;
++	return 0;
++
++cleanup_msi_irqs:
++	fsl_mc_msi_domain_free_irqs(&mc_bus_dev->dev);
++	return error;
++}
++EXPORT_SYMBOL_GPL(fsl_mc_populate_irq_pool);
++
++/**
++ * Teardown the interrupt pool associated with an fsl-mc bus.
++ * It frees the IRQs that were allocated to the pool, back to the GIC-ITS.
++ */
++void fsl_mc_cleanup_irq_pool(struct fsl_mc_bus *mc_bus)
++{
++	struct fsl_mc_device *mc_bus_dev = &mc_bus->mc_dev;
++	struct fsl_mc_resource_pool *res_pool =
++			&mc_bus->resource_pools[FSL_MC_POOL_IRQ];
++
++	if (!mc_bus->irq_resources)
++		return;
++
++	if (res_pool->max_count == 0)
++		return;
++
++	if (res_pool->free_count != res_pool->max_count)
++		return;
++
++	INIT_LIST_HEAD(&res_pool->free_list);
++	res_pool->max_count = 0;
++	res_pool->free_count = 0;
++	mc_bus->irq_resources = NULL;
++	fsl_mc_msi_domain_free_irqs(&mc_bus_dev->dev);
++}
++EXPORT_SYMBOL_GPL(fsl_mc_cleanup_irq_pool);
++
++/**
++ * Allocate the IRQs required by a given fsl-mc device.
++ */
++int __must_check fsl_mc_allocate_irqs(struct fsl_mc_device *mc_dev)
++{
++	int i;
++	int irq_count;
++	int res_allocated_count = 0;
++	int error = -EINVAL;
++	struct fsl_mc_device_irq **irqs = NULL;
++	struct fsl_mc_bus *mc_bus;
++	struct fsl_mc_resource_pool *res_pool;
++
++	if (mc_dev->irqs)
++		return -EINVAL;
++
++	irq_count = mc_dev->obj_desc.irq_count;
++	if (irq_count == 0)
++		return -EINVAL;
++
++	if (is_fsl_mc_bus_dprc(mc_dev))
++		mc_bus = to_fsl_mc_bus(mc_dev);
++	else
++		mc_bus = to_fsl_mc_bus(to_fsl_mc_device(mc_dev->dev.parent));
++
++	if (!mc_bus->irq_resources)
++		return -EINVAL;
++
++	res_pool = &mc_bus->resource_pools[FSL_MC_POOL_IRQ];
++	if (res_pool->free_count < irq_count) {
++		dev_err(&mc_dev->dev,
++			"Not able to allocate %u irqs for device\n", irq_count);
++		return -ENOSPC;
++	}
++
++	irqs = devm_kzalloc(&mc_dev->dev, irq_count * sizeof(irqs[0]),
++			    GFP_KERNEL);
++	if (!irqs)
++		return -ENOMEM;
++
++	for (i = 0; i < irq_count; i++) {
++		struct fsl_mc_resource *resource;
++
++		error = fsl_mc_resource_allocate(mc_bus, FSL_MC_POOL_IRQ,
++						 &resource);
++		if (error < 0)
++			goto error_resource_alloc;
++
++		irqs[i] = to_fsl_mc_irq(resource);
++		res_allocated_count++;
++
++		irqs[i]->mc_dev = mc_dev;
++		irqs[i]->dev_irq_index = i;
++	}
++
++	mc_dev->irqs = irqs;
++	return 0;
++
++error_resource_alloc:
++	for (i = 0; i < res_allocated_count; i++) {
++		irqs[i]->mc_dev = NULL;
++		fsl_mc_resource_free(&irqs[i]->resource);
++	}
++
++	return error;
++}
++EXPORT_SYMBOL_GPL(fsl_mc_allocate_irqs);
++
++/*
++ * Frees the IRQs that were allocated for an fsl-mc device.
++ */
++void fsl_mc_free_irqs(struct fsl_mc_device *mc_dev)
++{
++	int i;
++	int irq_count;
++	struct fsl_mc_bus *mc_bus;
++	struct fsl_mc_device_irq **irqs = mc_dev->irqs;
++
++	if (!irqs)
++		return;
++
++	irq_count = mc_dev->obj_desc.irq_count;
++
++	if (is_fsl_mc_bus_dprc(mc_dev))
++		mc_bus = to_fsl_mc_bus(mc_dev);
++	else
++		mc_bus = to_fsl_mc_bus(to_fsl_mc_device(mc_dev->dev.parent));
++
++	if (!mc_bus->irq_resources)
++		return;
++
++	for (i = 0; i < irq_count; i++) {
++		irqs[i]->mc_dev = NULL;
++		fsl_mc_resource_free(&irqs[i]->resource);
++	}
++
++	mc_dev->irqs = NULL;
++}
++EXPORT_SYMBOL_GPL(fsl_mc_free_irqs);
++
++void fsl_mc_init_all_resource_pools(struct fsl_mc_device *mc_bus_dev)
++{
++	int pool_type;
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc_bus_dev);
++
++	for (pool_type = 0; pool_type < FSL_MC_NUM_POOL_TYPES; pool_type++) {
++		struct fsl_mc_resource_pool *res_pool =
++		    &mc_bus->resource_pools[pool_type];
++
++		res_pool->type = pool_type;
++		res_pool->max_count = 0;
++		res_pool->free_count = 0;
++		res_pool->mc_bus = mc_bus;
++		INIT_LIST_HEAD(&res_pool->free_list);
++		mutex_init(&res_pool->mutex);
++	}
++}
++EXPORT_SYMBOL_GPL(fsl_mc_init_all_resource_pools);
++
++static void fsl_mc_cleanup_resource_pool(struct fsl_mc_device *mc_bus_dev,
++					 enum fsl_mc_pool_type pool_type)
++{
++	struct fsl_mc_resource *resource;
++	struct fsl_mc_resource *next;
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc_bus_dev);
++	struct fsl_mc_resource_pool *res_pool =
++					&mc_bus->resource_pools[pool_type];
++	int free_count = 0;
++
++	list_for_each_entry_safe(resource, next, &res_pool->free_list, node) {
++		free_count++;
++		devm_kfree(&mc_bus_dev->dev, resource);
++	}
++}
++
++void fsl_mc_cleanup_all_resource_pools(struct fsl_mc_device *mc_bus_dev)
++{
++	int pool_type;
++
++	for (pool_type = 0; pool_type < FSL_MC_NUM_POOL_TYPES; pool_type++)
++		fsl_mc_cleanup_resource_pool(mc_bus_dev, pool_type);
++}
++EXPORT_SYMBOL_GPL(fsl_mc_cleanup_all_resource_pools);
++
++/**
++ * fsl_mc_allocator_probe - callback invoked when an allocatable device is
++ * being added to the system
++ */
++static int fsl_mc_allocator_probe(struct fsl_mc_device *mc_dev)
++{
++	enum fsl_mc_pool_type pool_type;
++	struct fsl_mc_device *mc_bus_dev;
++	struct fsl_mc_bus *mc_bus;
++	int error;
++
++	if (!fsl_mc_is_allocatable(mc_dev))
++		return -EINVAL;
++
++	mc_bus_dev = to_fsl_mc_device(mc_dev->dev.parent);
++	if (!dev_is_fsl_mc(&mc_bus_dev->dev))
++		return -EINVAL;
++
++	mc_bus = to_fsl_mc_bus(mc_bus_dev);
++	error = object_type_to_pool_type(mc_dev->obj_desc.type, &pool_type);
++	if (error < 0)
++		return error;
++
++	error = fsl_mc_resource_pool_add_device(mc_bus, pool_type, mc_dev);
++	if (error < 0)
++		return error;
++
++	dev_dbg(&mc_dev->dev,
++		"Allocatable fsl-mc device bound to fsl_mc_allocator driver");
++	return 0;
++}
++
++/**
++ * fsl_mc_allocator_remove - callback invoked when an allocatable device is
++ * being removed from the system
++ */
++static int fsl_mc_allocator_remove(struct fsl_mc_device *mc_dev)
++{
++	int error;
++
++	if (!fsl_mc_is_allocatable(mc_dev))
++		return -EINVAL;
++
++	if (mc_dev->resource) {
++		error = fsl_mc_resource_pool_remove_device(mc_dev);
++		if (error < 0)
++			return error;
++	}
++
++	dev_dbg(&mc_dev->dev,
++		"Allocatable fsl-mc device unbound from fsl_mc_allocator driver");
++	return 0;
++}
++
++static const struct fsl_mc_device_id match_id_table[] = {
++	{
++	 .vendor = FSL_MC_VENDOR_FREESCALE,
++	 .obj_type = "dpbp",
++	},
++	{
++	 .vendor = FSL_MC_VENDOR_FREESCALE,
++	 .obj_type = "dpmcp",
++	},
++	{
++	 .vendor = FSL_MC_VENDOR_FREESCALE,
++	 .obj_type = "dpcon",
++	},
++	{.vendor = 0x0},
++};
++
++static struct fsl_mc_driver fsl_mc_allocator_driver = {
++	.driver = {
++		   .name = "fsl_mc_allocator",
++		   .pm = NULL,
++		   },
++	.match_id_table = match_id_table,
++	.probe = fsl_mc_allocator_probe,
++	.remove = fsl_mc_allocator_remove,
++};
++
++int __init fsl_mc_allocator_driver_init(void)
++{
++	return fsl_mc_driver_register(&fsl_mc_allocator_driver);
++}
++
++void fsl_mc_allocator_driver_exit(void)
++{
++	fsl_mc_driver_unregister(&fsl_mc_allocator_driver);
++}
+diff --git a/drivers/bus/fsl-mc/fsl-mc-bus.c b/drivers/bus/fsl-mc/fsl-mc-bus.c
+new file mode 100644
+index 00000000..7564e580
+--- /dev/null
++++ b/drivers/bus/fsl-mc/fsl-mc-bus.c
+@@ -0,0 +1,1151 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Freescale Management Complex (MC) bus driver
++ *
++ * Copyright (C) 2014-2016 Freescale Semiconductor, Inc.
++ * Author: German Rivera <German.Rivera@freescale.com>
++ *
++ */
++
++#define pr_fmt(fmt) "fsl-mc: " fmt
++
++#include <linux/module.h>
++#include <linux/of_device.h>
++#include <linux/of_address.h>
++#include <linux/ioport.h>
++#include <linux/slab.h>
++#include <linux/limits.h>
++#include <linux/bitops.h>
++#include <linux/msi.h>
++#include <linux/dma-mapping.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++/**
++ * Default DMA mask for devices on a fsl-mc bus
++ */
++#define FSL_MC_DEFAULT_DMA_MASK	(~0ULL)
++
++/**
++ * struct fsl_mc - Private data of a "fsl,qoriq-mc" platform device
++ * @root_mc_bus_dev: fsl-mc device representing the root DPRC
++ * @num_translation_ranges: number of entries in addr_translation_ranges
++ * @translation_ranges: array of bus to system address translation ranges
++ */
++struct fsl_mc {
++	struct fsl_mc_device *root_mc_bus_dev;
++	u8 num_translation_ranges;
++	struct fsl_mc_addr_translation_range *translation_ranges;
++};
++
++/**
++ * struct fsl_mc_addr_translation_range - bus to system address translation
++ * range
++ * @mc_region_type: Type of MC region for the range being translated
++ * @start_mc_offset: Start MC offset of the range being translated
++ * @end_mc_offset: MC offset of the first byte after the range (last MC
++ * offset of the range is end_mc_offset - 1)
++ * @start_phys_addr: system physical address corresponding to start_mc_addr
++ */
++struct fsl_mc_addr_translation_range {
++	enum dprc_region_type mc_region_type;
++	u64 start_mc_offset;
++	u64 end_mc_offset;
++	phys_addr_t start_phys_addr;
++};
++
++/**
++ * struct mc_version
++ * @major: Major version number: incremented on API compatibility changes
++ * @minor: Minor version number: incremented on API additions (that are
++ *		backward compatible); reset when major version is incremented
++ * @revision: Internal revision number: incremented on implementation changes
++ *		and/or bug fixes that have no impact on API
++ */
++struct mc_version {
++	u32 major;
++	u32 minor;
++	u32 revision;
++};
++
++/**
++ * fsl_mc_bus_match - device to driver matching callback
++ * @dev: the fsl-mc device to match against
++ * @drv: the device driver to search for matching fsl-mc object type
++ * structures
++ *
++ * Returns 1 on success, 0 otherwise.
++ */
++static int fsl_mc_bus_match(struct device *dev, struct device_driver *drv)
++{
++	const struct fsl_mc_device_id *id;
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++	struct fsl_mc_driver *mc_drv = to_fsl_mc_driver(drv);
++	bool found = false;
++
++	/* When driver_override is set, only bind to the matching driver */
++	if (mc_dev->driver_override) {
++		found = !strcmp(mc_dev->driver_override, mc_drv->driver.name);
++		goto out;
++	}
++
++	if (!mc_drv->match_id_table)
++		goto out;
++
++	/*
++	 * If the object is not 'plugged' don't match.
++	 * Only exception is the root DPRC, which is a special case.
++	 */
++	if ((mc_dev->obj_desc.state & FSL_MC_OBJ_STATE_PLUGGED) == 0 &&
++	    !fsl_mc_is_root_dprc(&mc_dev->dev))
++		goto out;
++
++	/*
++	 * Traverse the match_id table of the given driver, trying to find
++	 * a matching for the given device.
++	 */
++	for (id = mc_drv->match_id_table; id->vendor != 0x0; id++) {
++		if (id->vendor == mc_dev->obj_desc.vendor &&
++		    strcmp(id->obj_type, mc_dev->obj_desc.type) == 0) {
++			found = true;
++
++			break;
++		}
++	}
++
++out:
++	dev_dbg(dev, "%smatched\n", found ? "" : "not ");
++	return found;
++}
++
++/**
++ * fsl_mc_bus_uevent - callback invoked when a device is added
++ */
++static int fsl_mc_bus_uevent(struct device *dev, struct kobj_uevent_env *env)
++{
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++
++	if (add_uevent_var(env, "MODALIAS=fsl-mc:v%08Xd%s",
++			   mc_dev->obj_desc.vendor,
++			   mc_dev->obj_desc.type))
++		return -ENOMEM;
++
++	return 0;
++}
++
++static ssize_t modalias_show(struct device *dev, struct device_attribute *attr,
++			     char *buf)
++{
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++
++	return sprintf(buf, "fsl-mc:v%08Xd%s\n", mc_dev->obj_desc.vendor,
++		       mc_dev->obj_desc.type);
++}
++static DEVICE_ATTR_RO(modalias);
++
++static ssize_t rescan_store(struct device *dev,
++			    struct device_attribute *attr,
++			    const char *buf, size_t count)
++{
++	struct fsl_mc_device *root_mc_dev;
++	struct fsl_mc_bus *root_mc_bus;
++	unsigned long val;
++
++	if (!fsl_mc_is_root_dprc(dev))
++		return -EINVAL;
++
++	root_mc_dev = to_fsl_mc_device(dev);
++	root_mc_bus = to_fsl_mc_bus(root_mc_dev);
++
++	if (kstrtoul(buf, 0, &val) < 0)
++		return -EINVAL;
++
++	if (val) {
++		mutex_lock(&root_mc_bus->scan_mutex);
++		dprc_scan_objects(root_mc_dev, NULL, NULL);
++		mutex_unlock(&root_mc_bus->scan_mutex);
++	}
++
++	return count;
++}
++static DEVICE_ATTR_WO(rescan);
++
++static ssize_t driver_override_store(struct device *dev,
++				     struct device_attribute *attr,
++				     const char *buf, size_t count)
++{
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++	const char *driver_override, *old = mc_dev->driver_override;
++	char *cp;
++
++	if (WARN_ON(dev->bus != &fsl_mc_bus_type))
++		return -EINVAL;
++
++	if (count >= (PAGE_SIZE - 1))
++		return -EINVAL;
++
++	driver_override = kstrndup(buf, count, GFP_KERNEL);
++	if (!driver_override)
++		return -ENOMEM;
++
++	cp = strchr(driver_override, '\n');
++	if (cp)
++		*cp = '\0';
++
++	if (strlen(driver_override)) {
++		mc_dev->driver_override = driver_override;
++	} else {
++		kfree(driver_override);
++		mc_dev->driver_override = NULL;
++	}
++
++	kfree(old);
++
++	return count;
++}
++
++static ssize_t driver_override_show(struct device *dev,
++				    struct device_attribute *attr, char *buf)
++{
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++
++	return snprintf(buf, PAGE_SIZE, "%s\n", mc_dev->driver_override);
++}
++static DEVICE_ATTR_RW(driver_override);
++
++static struct attribute *fsl_mc_dev_attrs[] = {
++	&dev_attr_modalias.attr,
++	&dev_attr_rescan.attr,
++	&dev_attr_driver_override.attr,
++	NULL,
++};
++
++ATTRIBUTE_GROUPS(fsl_mc_dev);
++
++static int scan_fsl_mc_bus(struct device *dev, void *data)
++{
++	struct fsl_mc_device *root_mc_dev;
++	struct fsl_mc_bus *root_mc_bus;
++
++	if (!fsl_mc_is_root_dprc(dev))
++		goto exit;
++
++	root_mc_dev = to_fsl_mc_device(dev);
++	root_mc_bus = to_fsl_mc_bus(root_mc_dev);
++	mutex_lock(&root_mc_bus->scan_mutex);
++	dprc_scan_objects(root_mc_dev, NULL, NULL);
++	mutex_unlock(&root_mc_bus->scan_mutex);
++
++exit:
++	return 0;
++}
++
++static ssize_t bus_rescan_store(struct bus_type *bus,
++				const char *buf, size_t count)
++{
++	unsigned long val;
++
++	if (kstrtoul(buf, 0, &val) < 0)
++		return -EINVAL;
++
++	if (val)
++		bus_for_each_dev(bus, NULL, NULL, scan_fsl_mc_bus);
++
++	return count;
++}
++static BUS_ATTR(rescan, 0220, NULL, bus_rescan_store);
++
++static struct attribute *fsl_mc_bus_attrs[] = {
++	&bus_attr_rescan.attr,
++	NULL,
++};
++
++static const struct attribute_group fsl_mc_bus_group = {
++	.attrs = fsl_mc_bus_attrs,
++};
++
++static const struct attribute_group *fsl_mc_bus_groups[] = {
++	&fsl_mc_bus_group,
++	NULL,
++};
++
++struct bus_type fsl_mc_bus_type = {
++	.name = "fsl-mc",
++	.match = fsl_mc_bus_match,
++	.uevent = fsl_mc_bus_uevent,
++	.dev_groups = fsl_mc_dev_groups,
++	.bus_groups = fsl_mc_bus_groups,
++};
++EXPORT_SYMBOL_GPL(fsl_mc_bus_type);
++
++struct device_type fsl_mc_bus_dprc_type = {
++	.name = "fsl_mc_bus_dprc"
++};
++
++struct device_type fsl_mc_bus_dpni_type = {
++	.name = "fsl_mc_bus_dpni"
++};
++
++struct device_type fsl_mc_bus_dpio_type = {
++	.name = "fsl_mc_bus_dpio"
++};
++
++struct device_type fsl_mc_bus_dpsw_type = {
++	.name = "fsl_mc_bus_dpsw"
++};
++
++struct device_type fsl_mc_bus_dpdmux_type = {
++	.name = "fsl_mc_bus_dpdmux"
++};
++
++struct device_type fsl_mc_bus_dpbp_type = {
++	.name = "fsl_mc_bus_dpbp"
++};
++
++struct device_type fsl_mc_bus_dpcon_type = {
++	.name = "fsl_mc_bus_dpcon"
++};
++
++struct device_type fsl_mc_bus_dpmcp_type = {
++	.name = "fsl_mc_bus_dpmcp"
++};
++
++struct device_type fsl_mc_bus_dpmac_type = {
++	.name = "fsl_mc_bus_dpmac"
++};
++
++struct device_type fsl_mc_bus_dprtc_type = {
++	.name = "fsl_mc_bus_dprtc"
++};
++
++struct device_type fsl_mc_bus_dpseci_type = {
++	.name = "fsl_mc_bus_dpseci"
++};
++
++struct device_type fsl_mc_bus_dpdcei_type = {
++	.name = "fsl_mc_bus_dpdcei"
++};
++
++struct device_type fsl_mc_bus_dpaiop_type = {
++	.name = "fsl_mc_bus_dpaiop"
++};
++
++struct device_type fsl_mc_bus_dpci_type = {
++	.name = "fsl_mc_bus_dpci"
++};
++
++struct device_type fsl_mc_bus_dpdmai_type = {
++	.name = "fsl_mc_bus_dpdmai"
++};
++
++static struct device_type *fsl_mc_get_device_type(const char *type)
++{
++	static const struct {
++		struct device_type *dev_type;
++		const char *type;
++	} dev_types[] = {
++		{ &fsl_mc_bus_dprc_type, "dprc" },
++		{ &fsl_mc_bus_dpni_type, "dpni" },
++		{ &fsl_mc_bus_dpio_type, "dpio" },
++		{ &fsl_mc_bus_dpsw_type, "dpsw" },
++		{ &fsl_mc_bus_dpdmux_type, "dpdmux" },
++		{ &fsl_mc_bus_dpbp_type, "dpbp" },
++		{ &fsl_mc_bus_dpcon_type, "dpcon" },
++		{ &fsl_mc_bus_dpmcp_type, "dpmcp" },
++		{ &fsl_mc_bus_dpmac_type, "dpmac" },
++		{ &fsl_mc_bus_dprtc_type, "dprtc" },
++		{ &fsl_mc_bus_dpseci_type, "dpseci" },
++		{ &fsl_mc_bus_dpdcei_type, "dpdcei" },
++		{ &fsl_mc_bus_dpaiop_type, "dpaiop" },
++		{ &fsl_mc_bus_dpci_type, "dpci" },
++		{ &fsl_mc_bus_dpdmai_type, "dpdmai" },
++		{ NULL, NULL }
++	};
++	int i;
++
++	for (i = 0; dev_types[i].dev_type; i++)
++		if (!strcmp(dev_types[i].type, type))
++			return dev_types[i].dev_type;
++
++	return NULL;
++}
++
++static int fsl_mc_driver_probe(struct device *dev)
++{
++	struct fsl_mc_driver *mc_drv;
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++	int error;
++
++	mc_drv = to_fsl_mc_driver(dev->driver);
++
++	error = mc_drv->probe(mc_dev);
++	if (error < 0) {
++		if (error != -EPROBE_DEFER)
++			dev_err(dev, "%s failed: %d\n", __func__, error);
++		return error;
++	}
++
++	return 0;
++}
++
++static int fsl_mc_driver_remove(struct device *dev)
++{
++	struct fsl_mc_driver *mc_drv = to_fsl_mc_driver(dev->driver);
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++	int error;
++
++	error = mc_drv->remove(mc_dev);
++	if (error < 0) {
++		dev_err(dev, "%s failed: %d\n", __func__, error);
++		return error;
++	}
++
++	return 0;
++}
++
++static void fsl_mc_driver_shutdown(struct device *dev)
++{
++	struct fsl_mc_driver *mc_drv = to_fsl_mc_driver(dev->driver);
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++
++	mc_drv->shutdown(mc_dev);
++}
++
++/**
++ * __fsl_mc_driver_register - registers a child device driver with the
++ * MC bus
++ *
++ * This function is implicitly invoked from the registration function of
++ * fsl_mc device drivers, which is generated by the
++ * module_fsl_mc_driver() macro.
++ */
++int __fsl_mc_driver_register(struct fsl_mc_driver *mc_driver,
++			     struct module *owner)
++{
++	int error;
++
++	mc_driver->driver.owner = owner;
++	mc_driver->driver.bus = &fsl_mc_bus_type;
++
++	if (mc_driver->probe)
++		mc_driver->driver.probe = fsl_mc_driver_probe;
++
++	if (mc_driver->remove)
++		mc_driver->driver.remove = fsl_mc_driver_remove;
++
++	if (mc_driver->shutdown)
++		mc_driver->driver.shutdown = fsl_mc_driver_shutdown;
++
++	error = driver_register(&mc_driver->driver);
++	if (error < 0) {
++		pr_err("driver_register() failed for %s: %d\n",
++		       mc_driver->driver.name, error);
++		return error;
++	}
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(__fsl_mc_driver_register);
++
++/**
++ * fsl_mc_driver_unregister - unregisters a device driver from the
++ * MC bus
++ */
++void fsl_mc_driver_unregister(struct fsl_mc_driver *mc_driver)
++{
++	driver_unregister(&mc_driver->driver);
++}
++EXPORT_SYMBOL_GPL(fsl_mc_driver_unregister);
++
++/**
++ * mc_get_version() - Retrieves the Management Complex firmware
++ *			version information
++ * @mc_io:		Pointer to opaque I/O object
++ * @cmd_flags:		Command flags; one or more of 'MC_CMD_FLAG_'
++ * @mc_ver_info:	Returned version information structure
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++static int mc_get_version(struct fsl_mc_io *mc_io,
++			  u32 cmd_flags,
++			  struct mc_version *mc_ver_info)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dpmng_rsp_get_version *rsp_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPMNG_CMDID_GET_VERSION,
++					  cmd_flags,
++					  0);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	rsp_params = (struct dpmng_rsp_get_version *)cmd.params;
++	mc_ver_info->revision = le32_to_cpu(rsp_params->revision);
++	mc_ver_info->major = le32_to_cpu(rsp_params->version_major);
++	mc_ver_info->minor = le32_to_cpu(rsp_params->version_minor);
++
++	return 0;
++}
++
++/**
++ * fsl_mc_get_root_dprc - function to traverse to the root dprc
++ */
++void fsl_mc_get_root_dprc(struct device *dev,
++			  struct device **root_dprc_dev)
++{
++	if (!dev) {
++		*root_dprc_dev = NULL;
++	} else if (!dev_is_fsl_mc(dev)) {
++		*root_dprc_dev = NULL;
++	} else {
++		*root_dprc_dev = dev;
++		while (dev_is_fsl_mc((*root_dprc_dev)->parent))
++			*root_dprc_dev = (*root_dprc_dev)->parent;
++	}
++}
++EXPORT_SYMBOL_GPL(fsl_mc_get_root_dprc);
++
++static int get_dprc_attr(struct fsl_mc_io *mc_io,
++			 int container_id, struct dprc_attributes *attr)
++{
++	u16 dprc_handle;
++	int error;
++
++	error = dprc_open(mc_io, 0, container_id, &dprc_handle);
++	if (error < 0) {
++		dev_err(mc_io->dev, "dprc_open() failed: %d\n", error);
++		return error;
++	}
++
++	memset(attr, 0, sizeof(struct dprc_attributes));
++	error = dprc_get_attributes(mc_io, 0, dprc_handle, attr);
++	if (error < 0) {
++		dev_err(mc_io->dev, "dprc_get_attributes() failed: %d\n",
++			error);
++		goto common_cleanup;
++	}
++
++	error = 0;
++
++common_cleanup:
++	(void)dprc_close(mc_io, 0, dprc_handle);
++	return error;
++}
++
++static int get_dprc_icid(struct fsl_mc_io *mc_io,
++			 int container_id, u32 *icid)
++{
++	struct dprc_attributes attr;
++	int error;
++
++	error = get_dprc_attr(mc_io, container_id, &attr);
++	if (error == 0)
++		*icid = attr.icid;
++
++	return error;
++}
++
++static int translate_mc_addr(struct fsl_mc_device *mc_dev,
++			     enum dprc_region_type mc_region_type,
++			     u64 mc_offset, phys_addr_t *phys_addr)
++{
++	int i;
++	struct device *root_dprc_dev;
++	struct fsl_mc *mc;
++
++	fsl_mc_get_root_dprc(&mc_dev->dev, &root_dprc_dev);
++	mc = dev_get_drvdata(root_dprc_dev->parent);
++
++	if (mc->num_translation_ranges == 0) {
++		/*
++		 * Do identity mapping:
++		 */
++		*phys_addr = mc_offset;
++		return 0;
++	}
++
++	for (i = 0; i < mc->num_translation_ranges; i++) {
++		struct fsl_mc_addr_translation_range *range =
++			&mc->translation_ranges[i];
++
++		if (mc_region_type == range->mc_region_type &&
++		    mc_offset >= range->start_mc_offset &&
++		    mc_offset < range->end_mc_offset) {
++			*phys_addr = range->start_phys_addr +
++				     (mc_offset - range->start_mc_offset);
++			return 0;
++		}
++	}
++
++	return -EFAULT;
++}
++
++static int fsl_mc_device_get_mmio_regions(struct fsl_mc_device *mc_dev,
++					  struct fsl_mc_device *mc_bus_dev)
++{
++	int i;
++	int error;
++	struct resource *regions;
++	struct fsl_mc_obj_desc *obj_desc = &mc_dev->obj_desc;
++	struct device *parent_dev = mc_dev->dev.parent;
++	enum dprc_region_type mc_region_type;
++
++	if (is_fsl_mc_bus_dprc(mc_dev) ||
++	    is_fsl_mc_bus_dpmcp(mc_dev)) {
++		mc_region_type = DPRC_REGION_TYPE_MC_PORTAL;
++	} else if (is_fsl_mc_bus_dpio(mc_dev)) {
++		mc_region_type = DPRC_REGION_TYPE_QBMAN_PORTAL;
++	} else {
++		/*
++		 * This function should not have been called for this MC object
++		 * type, as this object type is not supposed to have MMIO
++		 * regions
++		 */
++		return -EINVAL;
++	}
++
++	regions = kmalloc_array(obj_desc->region_count,
++				sizeof(regions[0]), GFP_KERNEL);
++	if (!regions)
++		return -ENOMEM;
++
++	for (i = 0; i < obj_desc->region_count; i++) {
++		struct dprc_region_desc region_desc;
++
++		error = dprc_get_obj_region(mc_bus_dev->mc_io,
++					    0,
++					    mc_bus_dev->mc_handle,
++					    obj_desc->type,
++					    obj_desc->id, i, &region_desc);
++		if (error < 0) {
++			dev_err(parent_dev,
++				"dprc_get_obj_region() failed: %d\n", error);
++			goto error_cleanup_regions;
++		}
++
++		error = translate_mc_addr(mc_dev, mc_region_type,
++					  region_desc.base_offset,
++					  &regions[i].start);
++		if (error < 0) {
++			dev_err(parent_dev,
++				"Invalid MC offset: %#x (for %s.%d\'s region %d)\n",
++				region_desc.base_offset,
++				obj_desc->type, obj_desc->id, i);
++			goto error_cleanup_regions;
++		}
++
++		regions[i].end = regions[i].start + region_desc.size - 1;
++		regions[i].name = "fsl-mc object MMIO region";
++		regions[i].flags = IORESOURCE_IO;
++		if (region_desc.flags & DPRC_REGION_CACHEABLE)
++			regions[i].flags |= IORESOURCE_CACHEABLE;
++	}
++
++	mc_dev->regions = regions;
++	return 0;
++
++error_cleanup_regions:
++	kfree(regions);
++	return error;
++}
++
++/**
++ * fsl_mc_is_root_dprc - function to check if a given device is a root dprc
++ */
++bool fsl_mc_is_root_dprc(struct device *dev)
++{
++	struct device *root_dprc_dev;
++
++	fsl_mc_get_root_dprc(dev, &root_dprc_dev);
++	if (!root_dprc_dev)
++		return false;
++	return dev == root_dprc_dev;
++}
++
++static void fsl_mc_device_release(struct device *dev)
++{
++	struct fsl_mc_device *mc_dev = to_fsl_mc_device(dev);
++
++	kfree(mc_dev->regions);
++
++	if (is_fsl_mc_bus_dprc(mc_dev))
++		kfree(to_fsl_mc_bus(mc_dev));
++	else
++		kfree(mc_dev);
++}
++
++/**
++ * Add a newly discovered fsl-mc device to be visible in Linux
++ */
++int fsl_mc_device_add(struct fsl_mc_obj_desc *obj_desc,
++		      struct fsl_mc_io *mc_io,
++		      struct device *parent_dev,
++		      const char *driver_override,
++		      struct fsl_mc_device **new_mc_dev)
++{
++	int error;
++	struct fsl_mc_device *mc_dev = NULL;
++	struct fsl_mc_bus *mc_bus = NULL;
++	struct fsl_mc_device *parent_mc_dev;
++	struct device *fsl_mc_platform_dev;
++	struct device_node *fsl_mc_platform_node;
++
++	if (dev_is_fsl_mc(parent_dev))
++		parent_mc_dev = to_fsl_mc_device(parent_dev);
++	else
++		parent_mc_dev = NULL;
++
++	if (strcmp(obj_desc->type, "dprc") == 0) {
++		/*
++		 * Allocate an MC bus device object:
++		 */
++		mc_bus = kzalloc(sizeof(*mc_bus), GFP_KERNEL);
++		if (!mc_bus)
++			return -ENOMEM;
++
++		mc_dev = &mc_bus->mc_dev;
++	} else {
++		/*
++		 * Allocate a regular fsl_mc_device object:
++		 */
++		mc_dev = kzalloc(sizeof(*mc_dev), GFP_KERNEL);
++		if (!mc_dev)
++			return -ENOMEM;
++	}
++
++	mc_dev->obj_desc = *obj_desc;
++	mc_dev->mc_io = mc_io;
++
++	if (driver_override) {
++		/*
++		 * We trust driver_override, so we don't need to use
++		 * kstrndup() here
++		 */
++		mc_dev->driver_override = kstrdup(driver_override, GFP_KERNEL);
++		if (!mc_dev->driver_override) {
++			error = -ENOMEM;
++			goto error_cleanup_dev;
++		}
++	}
++
++	device_initialize(&mc_dev->dev);
++	mc_dev->dev.parent = parent_dev;
++	mc_dev->dev.bus = &fsl_mc_bus_type;
++	mc_dev->dev.release = fsl_mc_device_release;
++	mc_dev->dev.type = fsl_mc_get_device_type(obj_desc->type);
++	if (!mc_dev->dev.type) {
++		error = -ENODEV;
++		dev_err(parent_dev, "unknown device type %s\n", obj_desc->type);
++		goto error_cleanup_dev;
++	}
++	dev_set_name(&mc_dev->dev, "%s.%d", obj_desc->type, obj_desc->id);
++
++	if (strcmp(obj_desc->type, "dprc") == 0) {
++		struct fsl_mc_io *mc_io2;
++
++		mc_dev->flags |= FSL_MC_IS_DPRC;
++
++		/*
++		 * To get the DPRC's ICID, we need to open the DPRC
++		 * in get_dprc_icid(). For child DPRCs, we do so using the
++		 * parent DPRC's MC portal instead of the child DPRC's MC
++		 * portal, in case the child DPRC is already opened with
++		 * its own portal (e.g., the DPRC used by AIOP).
++		 *
++		 * NOTE: There cannot be more than one active open for a
++		 * given MC object, using the same MC portal.
++		 */
++		if (parent_mc_dev) {
++			/*
++			 * device being added is a child DPRC device
++			 */
++			mc_io2 = parent_mc_dev->mc_io;
++		} else {
++			/*
++			 * device being added is the root DPRC device
++			 */
++			if (!mc_io) {
++				error = -EINVAL;
++				goto error_cleanup_dev;
++			}
++
++			mc_io2 = mc_io;
++		}
++
++		error = get_dprc_icid(mc_io2, obj_desc->id, &mc_dev->icid);
++		if (error < 0)
++			goto error_cleanup_dev;
++	} else {
++		/*
++		 * A non-DPRC object has to be a child of a DPRC, use the
++		 * parent's ICID and interrupt domain.
++		 */
++		mc_dev->icid = parent_mc_dev->icid;
++		mc_dev->dma_mask = FSL_MC_DEFAULT_DMA_MASK;
++		mc_dev->dev.dma_mask = &mc_dev->dma_mask;
++		mc_dev->dev.coherent_dma_mask = mc_dev->dma_mask;
++		dev_set_msi_domain(&mc_dev->dev,
++				   dev_get_msi_domain(&parent_mc_dev->dev));
++	}
++
++	/*
++	 * Get MMIO regions for the device from the MC:
++	 *
++	 * NOTE: the root DPRC is a special case as its MMIO region is
++	 * obtained from the device tree
++	 */
++	if (parent_mc_dev && obj_desc->region_count != 0) {
++		error = fsl_mc_device_get_mmio_regions(mc_dev,
++						       parent_mc_dev);
++		if (error < 0)
++			goto error_cleanup_dev;
++	}
++
++	fsl_mc_platform_dev = &mc_dev->dev;
++	while (dev_is_fsl_mc(fsl_mc_platform_dev))
++		fsl_mc_platform_dev = fsl_mc_platform_dev->parent;
++	fsl_mc_platform_node = fsl_mc_platform_dev->of_node;
++
++	/* Set up the iommu configuration for the devices. */
++	fsl_mc_dma_configure(mc_dev, fsl_mc_platform_node,
++		!(obj_desc->flags & DPRC_OBJ_FLAG_NO_MEM_SHAREABILITY));
++
++	/*
++	 * The device-specific probe callback will get invoked by device_add()
++	 */
++	error = device_add(&mc_dev->dev);
++	if (error < 0) {
++		dev_err(parent_dev,
++			"device_add() failed for device %s: %d\n",
++			dev_name(&mc_dev->dev), error);
++		goto error_cleanup_dev;
++	}
++
++	dev_dbg(parent_dev, "added %s\n", dev_name(&mc_dev->dev));
++
++	*new_mc_dev = mc_dev;
++	return 0;
++
++error_cleanup_dev:
++	kfree(mc_dev->regions);
++	kfree(mc_bus);
++	kfree(mc_dev);
++
++	return error;
++}
++EXPORT_SYMBOL_GPL(fsl_mc_device_add);
++
++/**
++ * fsl_mc_device_remove - Remove an fsl-mc device from being visible to
++ * Linux
++ *
++ * @mc_dev: Pointer to an fsl-mc device
++ */
++void fsl_mc_device_remove(struct fsl_mc_device *mc_dev)
++{
++	kfree(mc_dev->driver_override);
++	mc_dev->driver_override = NULL;
++
++	/*
++	 * The device-specific remove callback will get invoked by device_del()
++	 */
++	device_del(&mc_dev->dev);
++	put_device(&mc_dev->dev);
++}
++EXPORT_SYMBOL_GPL(fsl_mc_device_remove);
++
++static int parse_mc_ranges(struct device *dev,
++			   int *paddr_cells,
++			   int *mc_addr_cells,
++			   int *mc_size_cells,
++			   const __be32 **ranges_start)
++{
++	const __be32 *prop;
++	int range_tuple_cell_count;
++	int ranges_len;
++	int tuple_len;
++	struct device_node *mc_node = dev->of_node;
++
++	*ranges_start = of_get_property(mc_node, "ranges", &ranges_len);
++	if (!(*ranges_start) || !ranges_len) {
++		dev_warn(dev,
++			 "missing or empty ranges property for device tree node '%s'\n",
++			 mc_node->name);
++		return 0;
++	}
++
++	*paddr_cells = of_n_addr_cells(mc_node);
++
++	prop = of_get_property(mc_node, "#address-cells", NULL);
++	if (prop)
++		*mc_addr_cells = be32_to_cpup(prop);
++	else
++		*mc_addr_cells = *paddr_cells;
++
++	prop = of_get_property(mc_node, "#size-cells", NULL);
++	if (prop)
++		*mc_size_cells = be32_to_cpup(prop);
++	else
++		*mc_size_cells = of_n_size_cells(mc_node);
++
++	range_tuple_cell_count = *paddr_cells + *mc_addr_cells +
++				 *mc_size_cells;
++
++	tuple_len = range_tuple_cell_count * sizeof(__be32);
++	if (ranges_len % tuple_len != 0) {
++		dev_err(dev, "malformed ranges property '%s'\n", mc_node->name);
++		return -EINVAL;
++	}
++
++	return ranges_len / tuple_len;
++}
++
++static int get_mc_addr_translation_ranges(struct device *dev,
++					  struct fsl_mc_addr_translation_range
++						**ranges,
++					  u8 *num_ranges)
++{
++	int ret;
++	int paddr_cells;
++	int mc_addr_cells;
++	int mc_size_cells;
++	int i;
++	const __be32 *ranges_start;
++	const __be32 *cell;
++
++	ret = parse_mc_ranges(dev,
++			      &paddr_cells,
++			      &mc_addr_cells,
++			      &mc_size_cells,
++			      &ranges_start);
++	if (ret < 0)
++		return ret;
++
++	*num_ranges = ret;
++	if (!ret) {
++		/*
++		 * Missing or empty ranges property ("ranges;") for the
++		 * 'fsl,qoriq-mc' node. In this case, identity mapping
++		 * will be used.
++		 */
++		*ranges = NULL;
++		return 0;
++	}
++
++	*ranges = devm_kcalloc(dev, *num_ranges,
++			       sizeof(struct fsl_mc_addr_translation_range),
++			       GFP_KERNEL);
++	if (!(*ranges))
++		return -ENOMEM;
++
++	cell = ranges_start;
++	for (i = 0; i < *num_ranges; ++i) {
++		struct fsl_mc_addr_translation_range *range = &(*ranges)[i];
++
++		range->mc_region_type = of_read_number(cell, 1);
++		range->start_mc_offset = of_read_number(cell + 1,
++							mc_addr_cells - 1);
++		cell += mc_addr_cells;
++		range->start_phys_addr = of_read_number(cell, paddr_cells);
++		cell += paddr_cells;
++		range->end_mc_offset = range->start_mc_offset +
++				     of_read_number(cell, mc_size_cells);
++
++		cell += mc_size_cells;
++	}
++
++	return 0;
++}
++
++/**
++ * fsl_mc_bus_probe - callback invoked when the root MC bus is being
++ * added
++ */
++static int fsl_mc_bus_probe(struct platform_device *pdev)
++{
++	struct fsl_mc_obj_desc obj_desc;
++	int error;
++	struct fsl_mc *mc;
++	struct fsl_mc_device *mc_bus_dev = NULL;
++	struct fsl_mc_io *mc_io = NULL;
++	struct fsl_mc_bus *mc_bus = NULL;
++	int container_id;
++	phys_addr_t mc_portal_phys_addr;
++	u32 mc_portal_size;
++	struct mc_version mc_version;
++	struct resource res;
++
++	mc = devm_kzalloc(&pdev->dev, sizeof(*mc), GFP_KERNEL);
++	if (!mc)
++		return -ENOMEM;
++
++	platform_set_drvdata(pdev, mc);
++
++	/*
++	 * Get physical address of MC portal for the root DPRC:
++	 */
++	error = of_address_to_resource(pdev->dev.of_node, 0, &res);
++	if (error < 0) {
++		dev_err(&pdev->dev,
++			"of_address_to_resource() failed for %pOF\n",
++			pdev->dev.of_node);
++		return error;
++	}
++
++	mc_portal_phys_addr = res.start;
++	mc_portal_size = resource_size(&res);
++	error = fsl_create_mc_io(&pdev->dev, mc_portal_phys_addr,
++				 mc_portal_size, NULL,
++				 FSL_MC_IO_ATOMIC_CONTEXT_PORTAL, &mc_io);
++	if (error < 0)
++		return error;
++
++	error = mc_get_version(mc_io, 0, &mc_version);
++	if (error != 0) {
++		dev_err(&pdev->dev,
++			"mc_get_version() failed with error %d\n", error);
++		goto error_cleanup_mc_io;
++	}
++
++	dev_info(&pdev->dev, "MC firmware version: %u.%u.%u\n",
++		 mc_version.major, mc_version.minor, mc_version.revision);
++
++	error = get_mc_addr_translation_ranges(&pdev->dev,
++					       &mc->translation_ranges,
++					       &mc->num_translation_ranges);
++	if (error < 0)
++		goto error_cleanup_mc_io;
++
++	error = dprc_get_container_id(mc_io, 0, &container_id);
++	if (error < 0) {
++		dev_err(&pdev->dev,
++			"dprc_get_container_id() failed: %d\n", error);
++		goto error_cleanup_mc_io;
++	}
++
++	memset(&obj_desc, 0, sizeof(struct fsl_mc_obj_desc));
++	error = dprc_get_api_version(mc_io, 0,
++				     &obj_desc.ver_major,
++				     &obj_desc.ver_minor);
++	if (error < 0)
++		goto error_cleanup_mc_io;
++
++	obj_desc.vendor = FSL_MC_VENDOR_FREESCALE;
++	strcpy(obj_desc.type, "dprc");
++	obj_desc.id = container_id;
++	obj_desc.irq_count = 1;
++	obj_desc.region_count = 0;
++
++	error = fsl_mc_device_add(&obj_desc, mc_io, &pdev->dev, NULL,
++				 &mc_bus_dev);
++	if (error < 0)
++		goto error_cleanup_mc_io;
++
++	mc_bus = to_fsl_mc_bus(mc_bus_dev);
++	error = fsl_mc_restool_create_device_file(mc_bus);
++	if (error < 0)
++		goto error_cleanup_device;
++
++	mc->root_mc_bus_dev = mc_bus_dev;
++
++	return 0;
++
++error_cleanup_device:
++	fsl_mc_device_remove(mc_bus_dev);
++
++error_cleanup_mc_io:
++	fsl_destroy_mc_io(mc_io);
++	return error;
++}
++
++/**
++ * fsl_mc_bus_remove - callback invoked when the root MC bus is being
++ * removed
++ */
++static int fsl_mc_bus_remove(struct platform_device *pdev)
++{
++	struct fsl_mc *mc = platform_get_drvdata(pdev);
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc->root_mc_bus_dev);
++
++	if (!fsl_mc_is_root_dprc(&mc->root_mc_bus_dev->dev))
++		return -EINVAL;
++
++	fsl_mc_restool_remove_device_file(mc_bus);
++	fsl_mc_device_remove(mc->root_mc_bus_dev);
++
++	fsl_destroy_mc_io(mc->root_mc_bus_dev->mc_io);
++	mc->root_mc_bus_dev->mc_io = NULL;
++
++	return 0;
++}
++
++static const struct of_device_id fsl_mc_bus_match_table[] = {
++	{.compatible = "fsl,qoriq-mc",},
++	{},
++};
++
++MODULE_DEVICE_TABLE(of, fsl_mc_bus_match_table);
++
++static struct platform_driver fsl_mc_bus_driver = {
++	.driver = {
++		   .name = "fsl_mc_bus",
++		   .pm = NULL,
++		   .of_match_table = fsl_mc_bus_match_table,
++		   },
++	.probe = fsl_mc_bus_probe,
++	.remove = fsl_mc_bus_remove,
++};
++
++static int __init fsl_mc_bus_driver_init(void)
++{
++	int error;
++
++	error = bus_register(&fsl_mc_bus_type);
++	if (error < 0) {
++		pr_err("bus type registration failed: %d\n", error);
++		goto error_cleanup_cache;
++	}
++
++	error = platform_driver_register(&fsl_mc_bus_driver);
++	if (error < 0) {
++		pr_err("platform_driver_register() failed: %d\n", error);
++		goto error_cleanup_bus;
++	}
++
++	error = dprc_driver_init();
++	if (error < 0)
++		goto error_cleanup_driver;
++
++	error = fsl_mc_allocator_driver_init();
++	if (error < 0)
++		goto error_cleanup_dprc_driver;
++
++	error = fsl_mc_restool_init();
++	if (error < 0)
++		goto error_cleanup_mc_allocator;
++
++	return 0;
++
++error_cleanup_mc_allocator:
++	fsl_mc_allocator_driver_exit();
++
++error_cleanup_dprc_driver:
++	dprc_driver_exit();
++
++error_cleanup_driver:
++	platform_driver_unregister(&fsl_mc_bus_driver);
++
++error_cleanup_bus:
++	bus_unregister(&fsl_mc_bus_type);
++
++error_cleanup_cache:
++	return error;
++}
++postcore_initcall(fsl_mc_bus_driver_init);
+diff --git a/drivers/bus/fsl-mc/fsl-mc-iommu.c b/drivers/bus/fsl-mc/fsl-mc-iommu.c
+new file mode 100644
+index 00000000..f90650b2
+--- /dev/null
++++ b/drivers/bus/fsl-mc/fsl-mc-iommu.c
+@@ -0,0 +1,78 @@
++/*
++ * Copyright 2016 Freescale Semiconductor, Inc.
++ * Copyright 2017 NXP
++ * Author: Nipun Gupta <nipun.gupta@nxp.com>
++ *
++ * This file is licensed under the terms of the GNU General Public
++ * License version 2. This program is licensed "as is" without any
++ * warranty of any kind, whether express or implied.
++ */
++
++#include <linux/iommu.h>
++#include <linux/of.h>
++#include <linux/of_iommu.h>
++#include <linux/fsl/mc.h>
++
++/* Setup the IOMMU for the DPRC container */
++static const struct iommu_ops
++*fsl_mc_iommu_configure(struct fsl_mc_device *mc_dev,
++	struct device_node *fsl_mc_platform_node)
++{
++	struct of_phandle_args iommu_spec;
++	const struct iommu_ops *ops;
++	u32 iommu_phandle;
++	struct device_node *iommu_node;
++	const __be32 *map = NULL;
++	int iommu_cells, map_len, ret;
++
++	map = of_get_property(fsl_mc_platform_node, "iommu-map", &map_len);
++	if (!map)
++		return NULL;
++
++	ops = mc_dev->dev.bus->iommu_ops;
++	if (!ops || !ops->of_xlate)
++		return NULL;
++
++	iommu_phandle = be32_to_cpup(map + 1);
++	iommu_node = of_find_node_by_phandle(iommu_phandle);
++
++	if (of_property_read_u32(iommu_node, "#iommu-cells", &iommu_cells)) {
++		pr_err("%s: missing #iommu-cells property\n", iommu_node->name);
++		return NULL;
++	}
++
++	/* Initialize the fwspec */
++	ret = iommu_fwspec_init(&mc_dev->dev, &iommu_node->fwnode, ops);
++	if (ret)
++		return NULL;
++
++	/*
++	 * Fill in the required stream-id before calling the iommu's
++	 * ops->xlate callback.
++	 */
++	iommu_spec.np = iommu_node;
++	iommu_spec.args[0] = mc_dev->icid;
++	iommu_spec.args_count = 1;
++
++	ret = ops->of_xlate(&mc_dev->dev, &iommu_spec);
++	if (ret)
++		return NULL;
++
++	of_node_put(iommu_spec.np);
++
++	return ops;
++}
++
++/* Set up DMA configuration for fsl-mc devices */
++void fsl_mc_dma_configure(struct fsl_mc_device *mc_dev,
++	struct device_node *fsl_mc_platform_node, int coherent)
++{
++	const struct iommu_ops *ops;
++
++	ops = fsl_mc_iommu_configure(mc_dev, fsl_mc_platform_node);
++
++	mc_dev->dev.coherent_dma_mask = DMA_BIT_MASK(48);
++	mc_dev->dev.dma_mask = &mc_dev->dev.coherent_dma_mask;
++	arch_setup_dma_ops(&mc_dev->dev, 0,
++		mc_dev->dev.coherent_dma_mask + 1, ops, coherent);
++}
+diff --git a/drivers/bus/fsl-mc/fsl-mc-msi.c b/drivers/bus/fsl-mc/fsl-mc-msi.c
+new file mode 100644
+index 00000000..ec35e255
+--- /dev/null
++++ b/drivers/bus/fsl-mc/fsl-mc-msi.c
+@@ -0,0 +1,285 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Freescale Management Complex (MC) bus driver MSI support
++ *
++ * Copyright (C) 2015-2016 Freescale Semiconductor, Inc.
++ * Author: German Rivera <German.Rivera@freescale.com>
++ *
++ */
++
++#include <linux/of_device.h>
++#include <linux/of_address.h>
++#include <linux/of_irq.h>
++#include <linux/irq.h>
++#include <linux/irqdomain.h>
++#include <linux/msi.h>
++
++#include "fsl-mc-private.h"
++
++#ifdef GENERIC_MSI_DOMAIN_OPS
++/*
++ * Generate a unique ID identifying the interrupt (only used within the MSI
++ * irqdomain.  Combine the icid with the interrupt index.
++ */
++static irq_hw_number_t fsl_mc_domain_calc_hwirq(struct fsl_mc_device *dev,
++						struct msi_desc *desc)
++{
++	/*
++	 * Make the base hwirq value for ICID*10000 so it is readable
++	 * as a decimal value in /proc/interrupts.
++	 */
++	return (irq_hw_number_t)(desc->fsl_mc.msi_index + (dev->icid * 10000));
++}
++
++static void fsl_mc_msi_set_desc(msi_alloc_info_t *arg,
++				struct msi_desc *desc)
++{
++	arg->desc = desc;
++	arg->hwirq = fsl_mc_domain_calc_hwirq(to_fsl_mc_device(desc->dev),
++					      desc);
++}
++#else
++#define fsl_mc_msi_set_desc NULL
++#endif
++
++static void fsl_mc_msi_update_dom_ops(struct msi_domain_info *info)
++{
++	struct msi_domain_ops *ops = info->ops;
++
++	if (!ops)
++		return;
++
++	/*
++	 * set_desc should not be set by the caller
++	 */
++	if (!ops->set_desc)
++		ops->set_desc = fsl_mc_msi_set_desc;
++}
++
++static void __fsl_mc_msi_write_msg(struct fsl_mc_device *mc_bus_dev,
++				   struct fsl_mc_device_irq *mc_dev_irq)
++{
++	int error;
++	struct fsl_mc_device *owner_mc_dev = mc_dev_irq->mc_dev;
++	struct msi_desc *msi_desc = mc_dev_irq->msi_desc;
++	struct dprc_irq_cfg irq_cfg;
++
++	/*
++	 * msi_desc->msg.address is 0x0 when this function is invoked in
++	 * the free_irq() code path. In this case, for the MC, we don't
++	 * really need to "unprogram" the MSI, so we just return.
++	 */
++	if (msi_desc->msg.address_lo == 0x0 && msi_desc->msg.address_hi == 0x0)
++		return;
++
++	if (!owner_mc_dev)
++		return;
++
++	irq_cfg.paddr = ((u64)msi_desc->msg.address_hi << 32) |
++			msi_desc->msg.address_lo;
++	irq_cfg.val = msi_desc->msg.data;
++	irq_cfg.irq_num = msi_desc->irq;
++
++	if (owner_mc_dev == mc_bus_dev) {
++		/*
++		 * IRQ is for the mc_bus_dev's DPRC itself
++		 */
++		error = dprc_set_irq(mc_bus_dev->mc_io,
++				     MC_CMD_FLAG_INTR_DIS | MC_CMD_FLAG_PRI,
++				     mc_bus_dev->mc_handle,
++				     mc_dev_irq->dev_irq_index,
++				     &irq_cfg);
++		if (error < 0) {
++			dev_err(&owner_mc_dev->dev,
++				"dprc_set_irq() failed: %d\n", error);
++		}
++	} else {
++		/*
++		 * IRQ is for for a child device of mc_bus_dev
++		 */
++		error = dprc_set_obj_irq(mc_bus_dev->mc_io,
++					 MC_CMD_FLAG_INTR_DIS | MC_CMD_FLAG_PRI,
++					 mc_bus_dev->mc_handle,
++					 owner_mc_dev->obj_desc.type,
++					 owner_mc_dev->obj_desc.id,
++					 mc_dev_irq->dev_irq_index,
++					 &irq_cfg);
++		if (error < 0) {
++			dev_err(&owner_mc_dev->dev,
++				"dprc_obj_set_irq() failed: %d\n", error);
++		}
++	}
++}
++
++/*
++ * NOTE: This function is invoked with interrupts disabled
++ */
++static void fsl_mc_msi_write_msg(struct irq_data *irq_data,
++				 struct msi_msg *msg)
++{
++	struct msi_desc *msi_desc = irq_data_get_msi_desc(irq_data);
++	struct fsl_mc_device *mc_bus_dev = to_fsl_mc_device(msi_desc->dev);
++	struct fsl_mc_bus *mc_bus = to_fsl_mc_bus(mc_bus_dev);
++	struct fsl_mc_device_irq *mc_dev_irq =
++		&mc_bus->irq_resources[msi_desc->fsl_mc.msi_index];
++
++	msi_desc->msg = *msg;
++
++	/*
++	 * Program the MSI (paddr, value) pair in the device:
++	 */
++	__fsl_mc_msi_write_msg(mc_bus_dev, mc_dev_irq);
++}
++
++static void fsl_mc_msi_update_chip_ops(struct msi_domain_info *info)
++{
++	struct irq_chip *chip = info->chip;
++
++	if (!chip)
++		return;
++
++	/*
++	 * irq_write_msi_msg should not be set by the caller
++	 */
++	if (!chip->irq_write_msi_msg)
++		chip->irq_write_msi_msg = fsl_mc_msi_write_msg;
++}
++
++/**
++ * fsl_mc_msi_create_irq_domain - Create a fsl-mc MSI interrupt domain
++ * @np:		Optional device-tree node of the interrupt controller
++ * @info:	MSI domain info
++ * @parent:	Parent irq domain
++ *
++ * Updates the domain and chip ops and creates a fsl-mc MSI
++ * interrupt domain.
++ *
++ * Returns:
++ * A domain pointer or NULL in case of failure.
++ */
++struct irq_domain *fsl_mc_msi_create_irq_domain(struct fwnode_handle *fwnode,
++						struct msi_domain_info *info,
++						struct irq_domain *parent)
++{
++	struct irq_domain *domain;
++
++	if (info->flags & MSI_FLAG_USE_DEF_DOM_OPS)
++		fsl_mc_msi_update_dom_ops(info);
++	if (info->flags & MSI_FLAG_USE_DEF_CHIP_OPS)
++		fsl_mc_msi_update_chip_ops(info);
++
++	domain = msi_create_irq_domain(fwnode, info, parent);
++	if (domain)
++		irq_domain_update_bus_token(domain, DOMAIN_BUS_FSL_MC_MSI);
++
++	return domain;
++}
++
++int fsl_mc_find_msi_domain(struct device *mc_platform_dev,
++			   struct irq_domain **mc_msi_domain)
++{
++	struct irq_domain *msi_domain;
++	struct device_node *mc_of_node = mc_platform_dev->of_node;
++
++	msi_domain = of_msi_get_domain(mc_platform_dev, mc_of_node,
++				       DOMAIN_BUS_FSL_MC_MSI);
++	if (!msi_domain) {
++		pr_err("Unable to find fsl-mc MSI domain for %pOF\n",
++		       mc_of_node);
++
++		return -ENOENT;
++	}
++
++	*mc_msi_domain = msi_domain;
++	return 0;
++}
++
++static void fsl_mc_msi_free_descs(struct device *dev)
++{
++	struct msi_desc *desc, *tmp;
++
++	list_for_each_entry_safe(desc, tmp, dev_to_msi_list(dev), list) {
++		list_del(&desc->list);
++		free_msi_entry(desc);
++	}
++}
++
++static int fsl_mc_msi_alloc_descs(struct device *dev, unsigned int irq_count)
++
++{
++	unsigned int i;
++	int error;
++	struct msi_desc *msi_desc;
++
++	for (i = 0; i < irq_count; i++) {
++		msi_desc = alloc_msi_entry(dev, 1, NULL);
++		if (!msi_desc) {
++			dev_err(dev, "Failed to allocate msi entry\n");
++			error = -ENOMEM;
++			goto cleanup_msi_descs;
++		}
++
++		msi_desc->fsl_mc.msi_index = i;
++		INIT_LIST_HEAD(&msi_desc->list);
++		list_add_tail(&msi_desc->list, dev_to_msi_list(dev));
++	}
++
++	return 0;
++
++cleanup_msi_descs:
++	fsl_mc_msi_free_descs(dev);
++	return error;
++}
++
++int fsl_mc_msi_domain_alloc_irqs(struct device *dev,
++				 unsigned int irq_count)
++{
++	struct irq_domain *msi_domain;
++	int error;
++
++	if (!list_empty(dev_to_msi_list(dev)))
++		return -EINVAL;
++
++	error = fsl_mc_msi_alloc_descs(dev, irq_count);
++	if (error < 0)
++		return error;
++
++	msi_domain = dev_get_msi_domain(dev);
++	if (!msi_domain) {
++		error = -EINVAL;
++		goto cleanup_msi_descs;
++	}
++
++	/*
++	 * NOTE: Calling this function will trigger the invocation of the
++	 * its_fsl_mc_msi_prepare() callback
++	 */
++	error = msi_domain_alloc_irqs(msi_domain, dev, irq_count);
++
++	if (error) {
++		dev_err(dev, "Failed to allocate IRQs\n");
++		goto cleanup_msi_descs;
++	}
++
++	return 0;
++
++cleanup_msi_descs:
++	fsl_mc_msi_free_descs(dev);
++	return error;
++}
++
++void fsl_mc_msi_domain_free_irqs(struct device *dev)
++{
++	struct irq_domain *msi_domain;
++
++	msi_domain = dev_get_msi_domain(dev);
++	if (!msi_domain)
++		return;
++
++	msi_domain_free_irqs(msi_domain, dev);
++
++	if (list_empty(dev_to_msi_list(dev)))
++		return;
++
++	fsl_mc_msi_free_descs(dev);
++}
+diff --git a/drivers/bus/fsl-mc/fsl-mc-private.h b/drivers/bus/fsl-mc/fsl-mc-private.h
+new file mode 100644
+index 00000000..bf823037
+--- /dev/null
++++ b/drivers/bus/fsl-mc/fsl-mc-private.h
+@@ -0,0 +1,223 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++/*
++ * Freescale Management Complex (MC) bus private declarations
++ *
++ * Copyright (C) 2016 Freescale Semiconductor, Inc.
++ *
++ */
++#ifndef _FSL_MC_PRIVATE_H_
++#define _FSL_MC_PRIVATE_H_
++
++#include <linux/fsl/mc.h>
++#include <linux/mutex.h>
++#include <linux/cdev.h>
++#include <linux/ioctl.h>
++
++/*
++ * Data Path Management Complex (DPMNG) General API
++ */
++
++/* DPMNG command versioning */
++#define DPMNG_CMD_BASE_VERSION		1
++#define DPMNG_CMD_ID_OFFSET		4
++
++#define DPMNG_CMD(id)	(((id) << DPMNG_CMD_ID_OFFSET) | DPMNG_CMD_BASE_VERSION)
++
++/* DPMNG command IDs */
++#define DPMNG_CMDID_GET_VERSION		DPMNG_CMD(0x831)
++
++struct dpmng_rsp_get_version {
++	__le32 revision;
++	__le32 version_major;
++	__le32 version_minor;
++};
++
++/*
++ * Data Path Management Command Portal (DPMCP) API
++ */
++
++/* Minimal supported DPMCP Version */
++#define DPMCP_MIN_VER_MAJOR		3
++#define DPMCP_MIN_VER_MINOR		0
++
++/* DPMCP command versioning */
++#define DPMCP_CMD_BASE_VERSION		1
++#define DPMCP_CMD_ID_OFFSET		4
++
++#define DPMCP_CMD(id)	(((id) << DPMCP_CMD_ID_OFFSET) | DPMCP_CMD_BASE_VERSION)
++
++/* DPMCP command IDs */
++#define DPMCP_CMDID_CLOSE		DPMCP_CMD(0x800)
++#define DPMCP_CMDID_OPEN		DPMCP_CMD(0x80b)
++#define DPMCP_CMDID_RESET		DPMCP_CMD(0x005)
++
++struct dpmcp_cmd_open {
++	__le32 dpmcp_id;
++};
++
++/*
++ * Initialization and runtime control APIs for DPMCP
++ */
++int dpmcp_open(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       int dpmcp_id,
++	       u16 *token);
++
++int dpmcp_close(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token);
++
++int dpmcp_reset(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token);
++
++/*
++ * Data Path Buffer Pool (DPBP) API
++ */
++
++/* DPBP Version */
++#define DPBP_VER_MAJOR				3
++#define DPBP_VER_MINOR				2
++
++/* Command versioning */
++#define DPBP_CMD_BASE_VERSION			1
++#define DPBP_CMD_ID_OFFSET			4
++
++#define DPBP_CMD(id)	(((id) << DPBP_CMD_ID_OFFSET) | DPBP_CMD_BASE_VERSION)
++
++/* Command IDs */
++#define DPBP_CMDID_CLOSE		DPBP_CMD(0x800)
++#define DPBP_CMDID_OPEN			DPBP_CMD(0x804)
++
++#define DPBP_CMDID_ENABLE		DPBP_CMD(0x002)
++#define DPBP_CMDID_DISABLE		DPBP_CMD(0x003)
++#define DPBP_CMDID_GET_ATTR		DPBP_CMD(0x004)
++#define DPBP_CMDID_RESET		DPBP_CMD(0x005)
++
++struct dpbp_cmd_open {
++	__le32 dpbp_id;
++};
++
++#define DPBP_ENABLE			0x1
++
++struct dpbp_rsp_get_attributes {
++	/* response word 0 */
++	__le16 pad;
++	__le16 bpid;
++	__le32 id;
++	/* response word 1 */
++	__le16 version_major;
++	__le16 version_minor;
++};
++
++/*
++ * Data Path Concentrator (DPCON) API
++ */
++
++/* DPCON Version */
++#define DPCON_VER_MAJOR				3
++#define DPCON_VER_MINOR				2
++
++/* Command versioning */
++#define DPCON_CMD_BASE_VERSION			1
++#define DPCON_CMD_ID_OFFSET			4
++
++#define DPCON_CMD(id)	(((id) << DPCON_CMD_ID_OFFSET) | DPCON_CMD_BASE_VERSION)
++
++/* Command IDs */
++#define DPCON_CMDID_CLOSE			DPCON_CMD(0x800)
++#define DPCON_CMDID_OPEN			DPCON_CMD(0x808)
++
++#define DPCON_CMDID_ENABLE			DPCON_CMD(0x002)
++#define DPCON_CMDID_DISABLE			DPCON_CMD(0x003)
++#define DPCON_CMDID_GET_ATTR			DPCON_CMD(0x004)
++#define DPCON_CMDID_RESET			DPCON_CMD(0x005)
++
++#define DPCON_CMDID_SET_NOTIFICATION		DPCON_CMD(0x100)
++
++struct dpcon_cmd_open {
++	__le32 dpcon_id;
++};
++
++#define DPCON_ENABLE			1
++
++struct dpcon_rsp_get_attr {
++	/* response word 0 */
++	__le32 id;
++	__le16 qbman_ch_id;
++	u8 num_priorities;
++	u8 pad;
++};
++
++struct dpcon_cmd_set_notification {
++	/* cmd word 0 */
++	__le32 dpio_id;
++	u8 priority;
++	u8 pad[3];
++	/* cmd word 1 */
++	__le64 user_ctx;
++};
++
++int __must_check fsl_mc_device_add(struct fsl_mc_obj_desc *obj_desc,
++				   struct fsl_mc_io *mc_io,
++				   struct device *parent_dev,
++				   const char *driver_override,
++				   struct fsl_mc_device **new_mc_dev);
++
++int __init dprc_driver_init(void);
++
++void dprc_driver_exit(void);
++
++int __init fsl_mc_allocator_driver_init(void);
++
++void fsl_mc_allocator_driver_exit(void);
++
++int __must_check fsl_mc_resource_allocate(struct fsl_mc_bus *mc_bus,
++					  enum fsl_mc_pool_type pool_type,
++					  struct fsl_mc_resource
++							  **new_resource);
++
++void fsl_mc_resource_free(struct fsl_mc_resource *resource);
++
++int fsl_mc_msi_domain_alloc_irqs(struct device *dev,
++				 unsigned int irq_count);
++
++void fsl_mc_msi_domain_free_irqs(struct device *dev);
++
++int __must_check fsl_create_mc_io(struct device *dev,
++				  phys_addr_t mc_portal_phys_addr,
++				  u32 mc_portal_size,
++				  struct fsl_mc_device *dpmcp_dev,
++				  u32 flags, struct fsl_mc_io **new_mc_io);
++
++void fsl_destroy_mc_io(struct fsl_mc_io *mc_io);
++
++bool fsl_mc_is_root_dprc(struct device *dev);
++
++#ifdef CONFIG_FSL_MC_RESTOOL
++
++int fsl_mc_restool_create_device_file(struct fsl_mc_bus *mc_bus);
++
++void fsl_mc_restool_remove_device_file(struct fsl_mc_bus *mc_bus);
++
++int fsl_mc_restool_init(void);
++
++#else
++
++static inline int fsl_mc_restool_create_device_file(struct fsl_mc_bus *mc_bus)
++{
++	return 0;
++}
++
++static inline void fsl_mc_restool_remove_device_file(struct fsl_mc_bus *mc_bus)
++{
++}
++
++static inline int fsl_mc_restool_init(void)
++{
++	return 0;
++}
++
++#endif
++
++#endif /* _FSL_MC_PRIVATE_H_ */
+diff --git a/drivers/bus/fsl-mc/fsl-mc-restool.c b/drivers/bus/fsl-mc/fsl-mc-restool.c
+new file mode 100644
+index 00000000..c39b8e8b
+--- /dev/null
++++ b/drivers/bus/fsl-mc/fsl-mc-restool.c
+@@ -0,0 +1,219 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Management Complex (MC) restool support
++ *
++ * Copyright 2018 NXP
++ *
++ */
++
++#include <linux/slab.h>
++#include <linux/cdev.h>
++#include <linux/fs.h>
++#include <linux/uaccess.h>
++
++#include "fsl-mc-private.h"
++
++#define FSL_MC_BUS_MAX_MINORS	1
++
++static struct class *fsl_mc_bus_class;
++static int fsl_mc_bus_major;
++
++static int fsl_mc_restool_send_command(unsigned long arg,
++				       struct fsl_mc_io *mc_io)
++{
++	struct fsl_mc_command mc_cmd;
++	int error;
++
++	error = copy_from_user(&mc_cmd, (void __user *)arg, sizeof(mc_cmd));
++	if (error)
++		return -EFAULT;
++
++	error = mc_send_command(mc_io, &mc_cmd);
++	if (error)
++		return error;
++
++	error = copy_to_user((void __user *)arg, &mc_cmd, sizeof(mc_cmd));
++	if (error)
++		return -EFAULT;
++
++	return 0;
++}
++
++int fsl_mc_restool_init(void)
++{
++	dev_t dev;
++	int error;
++
++	fsl_mc_bus_class = class_create(THIS_MODULE, "fsl_mc_bus");
++	if (IS_ERR(fsl_mc_bus_class)) {
++		error = PTR_ERR(fsl_mc_bus_class);
++		return error;
++	}
++
++	error = alloc_chrdev_region(&dev, 0,
++				    FSL_MC_BUS_MAX_MINORS,
++				    "fsl_mc_bus");
++	if (error < 0)
++		return error;
++
++	fsl_mc_bus_major = MAJOR(dev);
++
++	return 0;
++}
++
++static int fsl_mc_restool_dev_open(struct inode *inode, struct file *filep)
++{
++	struct fsl_mc_device *root_mc_device;
++	struct fsl_mc_restool *mc_restool;
++	struct fsl_mc_bus *mc_bus;
++	struct fsl_mc_io *dynamic_mc_io;
++	int error;
++
++	mc_restool = container_of(inode->i_cdev, struct fsl_mc_restool, cdev);
++	mc_bus = container_of(mc_restool, struct fsl_mc_bus, restool_misc);
++	root_mc_device = &mc_bus->mc_dev;
++
++	mutex_lock(&mc_restool->mutex);
++
++	if (!mc_restool->local_instance_in_use) {
++		filep->private_data = root_mc_device->mc_io;
++		mc_restool->local_instance_in_use = true;
++	} else {
++		dynamic_mc_io = kzalloc(sizeof(*dynamic_mc_io), GFP_KERNEL);
++		if (!dynamic_mc_io) {
++			error = -ENOMEM;
++			goto error_alloc_mc_io;
++		}
++
++		error = fsl_mc_portal_allocate(root_mc_device, 0,
++					       &dynamic_mc_io);
++		if (error) {
++			pr_err("Could not allocate MC portal\n");
++			goto error_portal_allocate;
++		}
++
++		mc_restool->dynamic_instance_count++;
++		filep->private_data = dynamic_mc_io;
++	}
++
++	mutex_unlock(&mc_restool->mutex);
++
++	return 0;
++
++error_portal_allocate:
++	kfree(dynamic_mc_io);
++
++error_alloc_mc_io:
++	mutex_unlock(&mc_restool->mutex);
++
++	return error;
++}
++
++static int fsl_mc_restool_dev_release(struct inode *inode, struct file *filep)
++{
++	struct fsl_mc_device *root_mc_device;
++	struct fsl_mc_restool *mc_restool;
++	struct fsl_mc_bus *mc_bus;
++	struct fsl_mc_io *mc_io;
++
++	mc_restool = container_of(inode->i_cdev, struct fsl_mc_restool, cdev);
++	mc_bus = container_of(mc_restool, struct fsl_mc_bus, restool_misc);
++	root_mc_device = &mc_bus->mc_dev;
++	mc_io = filep->private_data;
++
++	mutex_lock(&mc_restool->mutex);
++
++	if (WARN_ON(!mc_restool->local_instance_in_use &&
++		    mc_restool->dynamic_instance_count == 0)) {
++		mutex_unlock(&mc_restool->mutex);
++		return -EINVAL;
++	}
++
++	if (filep->private_data == root_mc_device->mc_io) {
++		mc_restool->local_instance_in_use = false;
++	} else {
++		fsl_mc_portal_free(mc_io);
++		kfree(mc_io);
++		mc_restool->dynamic_instance_count--;
++	}
++
++	filep->private_data = NULL;
++	mutex_unlock(&mc_restool->mutex);
++
++	return 0;
++}
++
++static long fsl_mc_restool_dev_ioctl(struct file *file,
++				     unsigned int cmd,
++				     unsigned long arg)
++{
++	int error;
++
++	switch (cmd) {
++	case RESTOOL_SEND_MC_COMMAND:
++		error = fsl_mc_restool_send_command(arg, file->private_data);
++		break;
++	default:
++		pr_err("%s: unexpected ioctl call number\n", __func__);
++		error = -EINVAL;
++	}
++
++	return error;
++}
++
++static const struct file_operations fsl_mc_restool_dev_fops = {
++	.owner = THIS_MODULE,
++	.open = fsl_mc_restool_dev_open,
++	.release = fsl_mc_restool_dev_release,
++	.unlocked_ioctl = fsl_mc_restool_dev_ioctl,
++};
++
++int fsl_mc_restool_create_device_file(struct fsl_mc_bus *mc_bus)
++{
++	struct fsl_mc_device *mc_dev = &mc_bus->mc_dev;
++	struct fsl_mc_restool *mc_restool = &mc_bus->restool_misc;
++	int error;
++
++	mc_restool = &mc_bus->restool_misc;
++	mc_restool->dev = MKDEV(fsl_mc_bus_major, 0);
++	cdev_init(&mc_restool->cdev, &fsl_mc_restool_dev_fops);
++
++	error = cdev_add(&mc_restool->cdev,
++			 mc_restool->dev,
++			 FSL_MC_BUS_MAX_MINORS);
++	if (error)
++		return error;
++
++	mc_restool->device = device_create(fsl_mc_bus_class,
++					   NULL,
++					   mc_restool->dev,
++					   NULL,
++					   "%s",
++					   dev_name(&mc_dev->dev));
++	if (IS_ERR(mc_restool->device)) {
++		error = PTR_ERR(mc_restool->device);
++		goto error_device_create;
++	}
++
++	mutex_init(&mc_restool->mutex);
++
++	return 0;
++
++error_device_create:
++	cdev_del(&mc_restool->cdev);
++
++	return error;
++}
++
++void fsl_mc_restool_remove_device_file(struct fsl_mc_bus *mc_bus)
++{
++	struct fsl_mc_restool *mc_restool = &mc_bus->restool_misc;
++
++	if (WARN_ON(mc_restool->local_instance_in_use))
++		return;
++
++	if (WARN_ON(mc_restool->dynamic_instance_count != 0))
++		return;
++
++	cdev_del(&mc_restool->cdev);
++}
+diff --git a/drivers/bus/fsl-mc/mc-io.c b/drivers/bus/fsl-mc/mc-io.c
+new file mode 100644
+index 00000000..7226cfc4
+--- /dev/null
++++ b/drivers/bus/fsl-mc/mc-io.c
+@@ -0,0 +1,268 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * Copyright 2013-2016 Freescale Semiconductor Inc.
++ *
++ */
++
++#include <linux/io.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++static int fsl_mc_io_set_dpmcp(struct fsl_mc_io *mc_io,
++			       struct fsl_mc_device *dpmcp_dev)
++{
++	int error;
++
++	if (mc_io->dpmcp_dev)
++		return -EINVAL;
++
++	if (dpmcp_dev->mc_io)
++		return -EINVAL;
++
++	error = dpmcp_open(mc_io,
++			   0,
++			   dpmcp_dev->obj_desc.id,
++			   &dpmcp_dev->mc_handle);
++	if (error < 0)
++		return error;
++
++	mc_io->dpmcp_dev = dpmcp_dev;
++	dpmcp_dev->mc_io = mc_io;
++	return 0;
++}
++
++static void fsl_mc_io_unset_dpmcp(struct fsl_mc_io *mc_io)
++{
++	int error;
++	struct fsl_mc_device *dpmcp_dev = mc_io->dpmcp_dev;
++
++	error = dpmcp_close(mc_io,
++			    0,
++			    dpmcp_dev->mc_handle);
++	if (error < 0) {
++		dev_err(&dpmcp_dev->dev, "dpmcp_close() failed: %d\n",
++			error);
++	}
++
++	mc_io->dpmcp_dev = NULL;
++	dpmcp_dev->mc_io = NULL;
++}
++
++/**
++ * Creates an MC I/O object
++ *
++ * @dev: device to be associated with the MC I/O object
++ * @mc_portal_phys_addr: physical address of the MC portal to use
++ * @mc_portal_size: size in bytes of the MC portal
++ * @dpmcp-dev: Pointer to the DPMCP object associated with this MC I/O
++ * object or NULL if none.
++ * @flags: flags for the new MC I/O object
++ * @new_mc_io: Area to return pointer to newly created MC I/O object
++ *
++ * Returns '0' on Success; Error code otherwise.
++ */
++int __must_check fsl_create_mc_io(struct device *dev,
++				  phys_addr_t mc_portal_phys_addr,
++				  u32 mc_portal_size,
++				  struct fsl_mc_device *dpmcp_dev,
++				  u32 flags, struct fsl_mc_io **new_mc_io)
++{
++	int error;
++	struct fsl_mc_io *mc_io;
++	void __iomem *mc_portal_virt_addr;
++	struct resource *res;
++
++	mc_io = devm_kzalloc(dev, sizeof(*mc_io), GFP_KERNEL);
++	if (!mc_io)
++		return -ENOMEM;
++
++	mc_io->dev = dev;
++	mc_io->flags = flags;
++	mc_io->portal_phys_addr = mc_portal_phys_addr;
++	mc_io->portal_size = mc_portal_size;
++	if (flags & FSL_MC_IO_ATOMIC_CONTEXT_PORTAL)
++		spin_lock_init(&mc_io->spinlock);
++	else
++		mutex_init(&mc_io->mutex);
++
++	res = devm_request_mem_region(dev,
++				      mc_portal_phys_addr,
++				      mc_portal_size,
++				      "mc_portal");
++	if (!res) {
++		dev_err(dev,
++			"devm_request_mem_region failed for MC portal %pa\n",
++			&mc_portal_phys_addr);
++		return -EBUSY;
++	}
++
++	mc_portal_virt_addr = devm_ioremap_nocache(dev,
++						   mc_portal_phys_addr,
++						   mc_portal_size);
++	if (!mc_portal_virt_addr) {
++		dev_err(dev,
++			"devm_ioremap_nocache failed for MC portal %pa\n",
++			&mc_portal_phys_addr);
++		return -ENXIO;
++	}
++
++	mc_io->portal_virt_addr = mc_portal_virt_addr;
++	if (dpmcp_dev) {
++		error = fsl_mc_io_set_dpmcp(mc_io, dpmcp_dev);
++		if (error < 0)
++			goto error_destroy_mc_io;
++	}
++
++	*new_mc_io = mc_io;
++	return 0;
++
++error_destroy_mc_io:
++	fsl_destroy_mc_io(mc_io);
++	return error;
++}
++
++/**
++ * Destroys an MC I/O object
++ *
++ * @mc_io: MC I/O object to destroy
++ */
++void fsl_destroy_mc_io(struct fsl_mc_io *mc_io)
++{
++	struct fsl_mc_device *dpmcp_dev = mc_io->dpmcp_dev;
++
++	if (dpmcp_dev)
++		fsl_mc_io_unset_dpmcp(mc_io);
++
++	devm_iounmap(mc_io->dev, mc_io->portal_virt_addr);
++	devm_release_mem_region(mc_io->dev,
++				mc_io->portal_phys_addr,
++				mc_io->portal_size);
++
++	mc_io->portal_virt_addr = NULL;
++	devm_kfree(mc_io->dev, mc_io);
++}
++
++/**
++ * fsl_mc_portal_allocate - Allocates an MC portal
++ *
++ * @mc_dev: MC device for which the MC portal is to be allocated
++ * @mc_io_flags: Flags for the fsl_mc_io object that wraps the allocated
++ * MC portal.
++ * @new_mc_io: Pointer to area where the pointer to the fsl_mc_io object
++ * that wraps the allocated MC portal is to be returned
++ *
++ * This function allocates an MC portal from the device's parent DPRC,
++ * from the corresponding MC bus' pool of MC portals and wraps
++ * it in a new fsl_mc_io object. If 'mc_dev' is a DPRC itself, the
++ * portal is allocated from its own MC bus.
++ */
++int __must_check fsl_mc_portal_allocate(struct fsl_mc_device *mc_dev,
++					u16 mc_io_flags,
++					struct fsl_mc_io **new_mc_io)
++{
++	struct fsl_mc_device *mc_bus_dev;
++	struct fsl_mc_bus *mc_bus;
++	phys_addr_t mc_portal_phys_addr;
++	size_t mc_portal_size;
++	struct fsl_mc_device *dpmcp_dev;
++	int error = -EINVAL;
++	struct fsl_mc_resource *resource = NULL;
++	struct fsl_mc_io *mc_io = NULL;
++
++	if (mc_dev->flags & FSL_MC_IS_DPRC) {
++		mc_bus_dev = mc_dev;
++	} else {
++		if (!dev_is_fsl_mc(mc_dev->dev.parent))
++			return error;
++
++		mc_bus_dev = to_fsl_mc_device(mc_dev->dev.parent);
++	}
++
++	mc_bus = to_fsl_mc_bus(mc_bus_dev);
++	*new_mc_io = NULL;
++	error = fsl_mc_resource_allocate(mc_bus, FSL_MC_POOL_DPMCP, &resource);
++	if (error < 0)
++		return error;
++
++	error = -EINVAL;
++	dpmcp_dev = resource->data;
++
++	if (dpmcp_dev->obj_desc.ver_major < DPMCP_MIN_VER_MAJOR ||
++	    (dpmcp_dev->obj_desc.ver_major == DPMCP_MIN_VER_MAJOR &&
++	     dpmcp_dev->obj_desc.ver_minor < DPMCP_MIN_VER_MINOR)) {
++		dev_err(&dpmcp_dev->dev,
++			"ERROR: Version %d.%d of DPMCP not supported.\n",
++			dpmcp_dev->obj_desc.ver_major,
++			dpmcp_dev->obj_desc.ver_minor);
++		error = -ENOTSUPP;
++		goto error_cleanup_resource;
++	}
++
++	mc_portal_phys_addr = dpmcp_dev->regions[0].start;
++	mc_portal_size = resource_size(dpmcp_dev->regions);
++
++	error = fsl_create_mc_io(&mc_bus_dev->dev,
++				 mc_portal_phys_addr,
++				 mc_portal_size, dpmcp_dev,
++				 mc_io_flags, &mc_io);
++	if (error < 0)
++		goto error_cleanup_resource;
++
++	*new_mc_io = mc_io;
++	return 0;
++
++error_cleanup_resource:
++	fsl_mc_resource_free(resource);
++	return error;
++}
++EXPORT_SYMBOL_GPL(fsl_mc_portal_allocate);
++
++/**
++ * fsl_mc_portal_free - Returns an MC portal to the pool of free MC portals
++ * of a given MC bus
++ *
++ * @mc_io: Pointer to the fsl_mc_io object that wraps the MC portal to free
++ */
++void fsl_mc_portal_free(struct fsl_mc_io *mc_io)
++{
++	struct fsl_mc_device *dpmcp_dev;
++	struct fsl_mc_resource *resource;
++
++	/*
++	 * Every mc_io obtained by calling fsl_mc_portal_allocate() is supposed
++	 * to have a DPMCP object associated with.
++	 */
++	dpmcp_dev = mc_io->dpmcp_dev;
++
++	resource = dpmcp_dev->resource;
++	if (!resource || resource->type != FSL_MC_POOL_DPMCP)
++		return;
++
++	if (resource->data != dpmcp_dev)
++		return;
++
++	fsl_destroy_mc_io(mc_io);
++	fsl_mc_resource_free(resource);
++}
++EXPORT_SYMBOL_GPL(fsl_mc_portal_free);
++
++/**
++ * fsl_mc_portal_reset - Resets the dpmcp object for a given fsl_mc_io object
++ *
++ * @mc_io: Pointer to the fsl_mc_io object that wraps the MC portal to free
++ */
++int fsl_mc_portal_reset(struct fsl_mc_io *mc_io)
++{
++	int error;
++	struct fsl_mc_device *dpmcp_dev = mc_io->dpmcp_dev;
++
++	error = dpmcp_reset(mc_io, 0, dpmcp_dev->mc_handle);
++	if (error < 0) {
++		dev_err(&dpmcp_dev->dev, "dpmcp_reset() failed: %d\n", error);
++		return error;
++	}
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(fsl_mc_portal_reset);
+diff --git a/drivers/bus/fsl-mc/mc-sys.c b/drivers/bus/fsl-mc/mc-sys.c
+new file mode 100644
+index 00000000..24307ed5
+--- /dev/null
++++ b/drivers/bus/fsl-mc/mc-sys.c
+@@ -0,0 +1,296 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * Copyright 2013-2016 Freescale Semiconductor Inc.
++ *
++ * I/O services to send MC commands to the MC hardware
++ *
++ */
++
++#include <linux/delay.h>
++#include <linux/slab.h>
++#include <linux/ioport.h>
++#include <linux/device.h>
++#include <linux/io.h>
++#include <linux/io-64-nonatomic-hi-lo.h>
++#include <linux/fsl/mc.h>
++
++#include "fsl-mc-private.h"
++
++/**
++ * Timeout in milliseconds to wait for the completion of an MC command
++ */
++#define MC_CMD_COMPLETION_TIMEOUT_MS	15000
++
++/*
++ * usleep_range() min and max values used to throttle down polling
++ * iterations while waiting for MC command completion
++ */
++#define MC_CMD_COMPLETION_POLLING_MIN_SLEEP_USECS    10
++#define MC_CMD_COMPLETION_POLLING_MAX_SLEEP_USECS    500
++
++static enum mc_cmd_status mc_cmd_hdr_read_status(struct fsl_mc_command *cmd)
++{
++	struct mc_cmd_header *hdr = (struct mc_cmd_header *)&cmd->header;
++
++	return (enum mc_cmd_status)hdr->status;
++}
++
++static u16 mc_cmd_hdr_read_cmdid(struct fsl_mc_command *cmd)
++{
++	struct mc_cmd_header *hdr = (struct mc_cmd_header *)&cmd->header;
++	u16 cmd_id = le16_to_cpu(hdr->cmd_id);
++
++	return cmd_id;
++}
++
++static int mc_status_to_error(enum mc_cmd_status status)
++{
++	static const int mc_status_to_error_map[] = {
++		[MC_CMD_STATUS_OK] = 0,
++		[MC_CMD_STATUS_AUTH_ERR] = -EACCES,
++		[MC_CMD_STATUS_NO_PRIVILEGE] = -EPERM,
++		[MC_CMD_STATUS_DMA_ERR] = -EIO,
++		[MC_CMD_STATUS_CONFIG_ERR] = -ENXIO,
++		[MC_CMD_STATUS_TIMEOUT] = -ETIMEDOUT,
++		[MC_CMD_STATUS_NO_RESOURCE] = -ENAVAIL,
++		[MC_CMD_STATUS_NO_MEMORY] = -ENOMEM,
++		[MC_CMD_STATUS_BUSY] = -EBUSY,
++		[MC_CMD_STATUS_UNSUPPORTED_OP] = -ENOTSUPP,
++		[MC_CMD_STATUS_INVALID_STATE] = -ENODEV,
++	};
++
++	if ((u32)status >= ARRAY_SIZE(mc_status_to_error_map))
++		return -EINVAL;
++
++	return mc_status_to_error_map[status];
++}
++
++static const char *mc_status_to_string(enum mc_cmd_status status)
++{
++	static const char *const status_strings[] = {
++		[MC_CMD_STATUS_OK] = "Command completed successfully",
++		[MC_CMD_STATUS_READY] = "Command ready to be processed",
++		[MC_CMD_STATUS_AUTH_ERR] = "Authentication error",
++		[MC_CMD_STATUS_NO_PRIVILEGE] = "No privilege",
++		[MC_CMD_STATUS_DMA_ERR] = "DMA or I/O error",
++		[MC_CMD_STATUS_CONFIG_ERR] = "Configuration error",
++		[MC_CMD_STATUS_TIMEOUT] = "Operation timed out",
++		[MC_CMD_STATUS_NO_RESOURCE] = "No resources",
++		[MC_CMD_STATUS_NO_MEMORY] = "No memory available",
++		[MC_CMD_STATUS_BUSY] = "Device is busy",
++		[MC_CMD_STATUS_UNSUPPORTED_OP] = "Unsupported operation",
++		[MC_CMD_STATUS_INVALID_STATE] = "Invalid state"
++	};
++
++	if ((unsigned int)status >= ARRAY_SIZE(status_strings))
++		return "Unknown MC error";
++
++	return status_strings[status];
++}
++
++/**
++ * mc_write_command - writes a command to a Management Complex (MC) portal
++ *
++ * @portal: pointer to an MC portal
++ * @cmd: pointer to a filled command
++ */
++static inline void mc_write_command(struct fsl_mc_command __iomem *portal,
++				    struct fsl_mc_command *cmd)
++{
++	int i;
++
++	/* copy command parameters into the portal */
++	for (i = 0; i < MC_CMD_NUM_OF_PARAMS; i++)
++		/*
++		 * Data is already in the expected LE byte-order. Do an
++		 * extra LE -> CPU conversion so that the CPU -> LE done in
++		 * the device io write api puts it back in the right order.
++		 */
++		writeq_relaxed(le64_to_cpu(cmd->params[i]), &portal->params[i]);
++
++	/* submit the command by writing the header */
++	writeq(le64_to_cpu(cmd->header), &portal->header);
++}
++
++/**
++ * mc_read_response - reads the response for the last MC command from a
++ * Management Complex (MC) portal
++ *
++ * @portal: pointer to an MC portal
++ * @resp: pointer to command response buffer
++ *
++ * Returns MC_CMD_STATUS_OK on Success; Error code otherwise.
++ */
++static inline enum mc_cmd_status mc_read_response(struct fsl_mc_command __iomem
++						  *portal,
++						  struct fsl_mc_command *resp)
++{
++	int i;
++	enum mc_cmd_status status;
++
++	/* Copy command response header from MC portal: */
++	resp->header = cpu_to_le64(readq_relaxed(&portal->header));
++	status = mc_cmd_hdr_read_status(resp);
++	if (status != MC_CMD_STATUS_OK)
++		return status;
++
++	/* Copy command response data from MC portal: */
++	for (i = 0; i < MC_CMD_NUM_OF_PARAMS; i++)
++		/*
++		 * Data is expected to be in LE byte-order. Do an
++		 * extra CPU -> LE to revert the LE -> CPU done in
++		 * the device io read api.
++		 */
++		resp->params[i] =
++			cpu_to_le64(readq_relaxed(&portal->params[i]));
++
++	return status;
++}
++
++/**
++ * Waits for the completion of an MC command doing preemptible polling.
++ * uslepp_range() is called between polling iterations.
++ *
++ * @mc_io: MC I/O object to be used
++ * @cmd: command buffer to receive MC response
++ * @mc_status: MC command completion status
++ */
++static int mc_polling_wait_preemptible(struct fsl_mc_io *mc_io,
++				       struct fsl_mc_command *cmd,
++				       enum mc_cmd_status *mc_status)
++{
++	enum mc_cmd_status status;
++	unsigned long jiffies_until_timeout =
++		jiffies + msecs_to_jiffies(MC_CMD_COMPLETION_TIMEOUT_MS);
++
++	/*
++	 * Wait for response from the MC hardware:
++	 */
++	for (;;) {
++		status = mc_read_response(mc_io->portal_virt_addr, cmd);
++		if (status != MC_CMD_STATUS_READY)
++			break;
++
++		/*
++		 * TODO: When MC command completion interrupts are supported
++		 * call wait function here instead of usleep_range()
++		 */
++		usleep_range(MC_CMD_COMPLETION_POLLING_MIN_SLEEP_USECS,
++			     MC_CMD_COMPLETION_POLLING_MAX_SLEEP_USECS);
++
++		if (time_after_eq(jiffies, jiffies_until_timeout)) {
++			dev_dbg(mc_io->dev,
++				"MC command timed out (portal: %pa, dprc handle: %#x, command: %#x)\n",
++				 &mc_io->portal_phys_addr,
++				 (unsigned int)mc_cmd_hdr_read_token(cmd),
++				 (unsigned int)mc_cmd_hdr_read_cmdid(cmd));
++
++			return -ETIMEDOUT;
++		}
++	}
++
++	*mc_status = status;
++	return 0;
++}
++
++/**
++ * Waits for the completion of an MC command doing atomic polling.
++ * udelay() is called between polling iterations.
++ *
++ * @mc_io: MC I/O object to be used
++ * @cmd: command buffer to receive MC response
++ * @mc_status: MC command completion status
++ */
++static int mc_polling_wait_atomic(struct fsl_mc_io *mc_io,
++				  struct fsl_mc_command *cmd,
++				  enum mc_cmd_status *mc_status)
++{
++	enum mc_cmd_status status;
++	unsigned long timeout_usecs = MC_CMD_COMPLETION_TIMEOUT_MS * 1000;
++
++	BUILD_BUG_ON((MC_CMD_COMPLETION_TIMEOUT_MS * 1000) %
++		     MC_CMD_COMPLETION_POLLING_MAX_SLEEP_USECS != 0);
++
++	for (;;) {
++		status = mc_read_response(mc_io->portal_virt_addr, cmd);
++		if (status != MC_CMD_STATUS_READY)
++			break;
++
++		udelay(MC_CMD_COMPLETION_POLLING_MAX_SLEEP_USECS);
++		timeout_usecs -= MC_CMD_COMPLETION_POLLING_MAX_SLEEP_USECS;
++		if (timeout_usecs == 0) {
++			dev_dbg(mc_io->dev,
++				"MC command timed out (portal: %pa, dprc handle: %#x, command: %#x)\n",
++				 &mc_io->portal_phys_addr,
++				 (unsigned int)mc_cmd_hdr_read_token(cmd),
++				 (unsigned int)mc_cmd_hdr_read_cmdid(cmd));
++
++			return -ETIMEDOUT;
++		}
++	}
++
++	*mc_status = status;
++	return 0;
++}
++
++/**
++ * Sends a command to the MC device using the given MC I/O object
++ *
++ * @mc_io: MC I/O object to be used
++ * @cmd: command to be sent
++ *
++ * Returns '0' on Success; Error code otherwise.
++ */
++int mc_send_command(struct fsl_mc_io *mc_io, struct fsl_mc_command *cmd)
++{
++	int error;
++	enum mc_cmd_status status;
++	unsigned long irq_flags = 0;
++
++	if (in_irq() && !(mc_io->flags & FSL_MC_IO_ATOMIC_CONTEXT_PORTAL))
++		return -EINVAL;
++
++	if (mc_io->flags & FSL_MC_IO_ATOMIC_CONTEXT_PORTAL)
++		spin_lock_irqsave(&mc_io->spinlock, irq_flags);
++	else
++		mutex_lock(&mc_io->mutex);
++
++	/*
++	 * Send command to the MC hardware:
++	 */
++	mc_write_command(mc_io->portal_virt_addr, cmd);
++
++	/*
++	 * Wait for response from the MC hardware:
++	 */
++	if (!(mc_io->flags & FSL_MC_IO_ATOMIC_CONTEXT_PORTAL))
++		error = mc_polling_wait_preemptible(mc_io, cmd, &status);
++	else
++		error = mc_polling_wait_atomic(mc_io, cmd, &status);
++
++	if (error < 0)
++		goto common_exit;
++
++	if (status != MC_CMD_STATUS_OK) {
++		dev_dbg(mc_io->dev,
++			"MC command failed: portal: %pa, dprc handle: %#x, command: %#x, status: %s (%#x)\n",
++			 &mc_io->portal_phys_addr,
++			 (unsigned int)mc_cmd_hdr_read_token(cmd),
++			 (unsigned int)mc_cmd_hdr_read_cmdid(cmd),
++			 mc_status_to_string(status),
++			 (unsigned int)status);
++
++		error = mc_status_to_error(status);
++		goto common_exit;
++	}
++
++	error = 0;
++common_exit:
++	if (mc_io->flags & FSL_MC_IO_ATOMIC_CONTEXT_PORTAL)
++		spin_unlock_irqrestore(&mc_io->spinlock, irq_flags);
++	else
++		mutex_unlock(&mc_io->mutex);
++
++	return error;
++}
++EXPORT_SYMBOL_GPL(mc_send_command);
+diff --git a/drivers/crypto/caam/caamalg_desc.c b/drivers/crypto/caam/caamalg_desc.c
+index 86cf6b3b..1ec1925a 100644
+--- a/drivers/crypto/caam/caamalg_desc.c
++++ b/drivers/crypto/caam/caamalg_desc.c
+@@ -891,10 +891,10 @@ void cnstr_shdsc_tls_decap(u32 * const desc, struct alginfo *cdata,
+ 	/* VSOL = payloadlen + icvlen + padlen */
+ 	append_math_add(desc, VARSEQOUTLEN, ZERO, REG3, 4);
+ 
+-#ifdef __LITTLE_ENDIAN
+-	append_moveb(desc, MOVE_WAITCOMP |
+-		     MOVE_SRC_MATH0 | MOVE_DEST_MATH0 | 8);
+-#endif
++	if (caam_little_end)
++		append_moveb(desc, MOVE_WAITCOMP |
++			     MOVE_SRC_MATH0 | MOVE_DEST_MATH0 | 8);
++
+ 	/* update Len field */
+ 	append_math_sub(desc, REG0, REG0, REG2, 8);
+ 
+@@ -944,15 +944,15 @@ void cnstr_shdsc_tls_decap(u32 * const desc, struct alginfo *cdata,
+ 		 * SEQ OUT PTR command, Output Pointer (2 words) and
+ 		 * Output Length into math registers.
+ 		 */
+-#ifdef __LITTLE_ENDIAN
+-		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+-			    MOVE_DEST_MATH0 | (55 * 4 << MOVE_OFFSET_SHIFT) |
+-			    20);
+-#else
+-		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+-			    MOVE_DEST_MATH0 | (54 * 4 << MOVE_OFFSET_SHIFT) |
+-			    20);
+-#endif
++		if (caam_little_end)
++			append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
++				    MOVE_DEST_MATH0 |
++				    (55 * 4 << MOVE_OFFSET_SHIFT) | 20);
++		else
++			append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
++				    MOVE_DEST_MATH0 |
++				    (54 * 4 << MOVE_OFFSET_SHIFT) | 20);
++
+ 		/* Transform SEQ OUT PTR command in SEQ IN PTR command */
+ 		append_math_and_imm_u32(desc, REG0, REG0, IMM,
+ 					~(CMD_SEQ_IN_PTR ^ CMD_SEQ_OUT_PTR));
+@@ -963,15 +963,15 @@ void cnstr_shdsc_tls_decap(u32 * const desc, struct alginfo *cdata,
+ 				    (4 << LDST_OFFSET_SHIFT));
+ 		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 1);
+ 		/* Move the updated fields back to the Job Descriptor */
+-#ifdef __LITTLE_ENDIAN
+-		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+-			    MOVE_DEST_DESCBUF | (55 * 4 << MOVE_OFFSET_SHIFT) |
+-			    24);
+-#else
+-		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+-			    MOVE_DEST_DESCBUF | (54 * 4 << MOVE_OFFSET_SHIFT) |
+-			    24);
+-#endif
++		if (caam_little_end)
++			append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
++				    MOVE_DEST_DESCBUF |
++				    (55 * 4 << MOVE_OFFSET_SHIFT) | 24);
++		else
++			append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
++				    MOVE_DEST_DESCBUF |
++				    (54 * 4 << MOVE_OFFSET_SHIFT) | 24);
++
+ 		/*
+ 		 * Read the new SEQ IN PTR command, Input Pointer, Input Length
+ 		 * and then jump back to the next command from the
+@@ -983,15 +983,15 @@ void cnstr_shdsc_tls_decap(u32 * const desc, struct alginfo *cdata,
+ 		 * Move the SEQ OUT PTR command, Output Pointer (1 word) and
+ 		 * Output Length into math registers.
+ 		 */
+-#ifdef __LITTLE_ENDIAN
+-		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+-			    MOVE_DEST_MATH0 | (54 * 4 << MOVE_OFFSET_SHIFT) |
+-			    12);
+-#else
+-		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+-			    MOVE_DEST_MATH0 | (53 * 4 << MOVE_OFFSET_SHIFT) |
+-			    12);
+-#endif
++		if (caam_little_end)
++			append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
++				    MOVE_DEST_MATH0 |
++				    (54 * 4 << MOVE_OFFSET_SHIFT) | 12);
++		else
++			append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
++				    MOVE_DEST_MATH0 |
++				    (53 * 4 << MOVE_OFFSET_SHIFT) | 12);
++
+ 		/* Transform SEQ OUT PTR command in SEQ IN PTR command */
+ 		append_math_and_imm_u64(desc, REG0, REG0, IMM,
+ 					~(((u64)(CMD_SEQ_IN_PTR ^
+@@ -1003,15 +1003,15 @@ void cnstr_shdsc_tls_decap(u32 * const desc, struct alginfo *cdata,
+ 				    (4 << LDST_OFFSET_SHIFT));
+ 		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 1);
+ 		/* Move the updated fields back to the Job Descriptor */
+-#ifdef __LITTLE_ENDIAN
+-		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+-			    MOVE_DEST_DESCBUF | (54 * 4 << MOVE_OFFSET_SHIFT) |
+-			    16);
+-#else
+-		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+-			    MOVE_DEST_DESCBUF | (53 * 4 << MOVE_OFFSET_SHIFT) |
+-			    16);
+-#endif
++		if (caam_little_end)
++			append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
++				    MOVE_DEST_DESCBUF |
++				    (54 * 4 << MOVE_OFFSET_SHIFT) | 16);
++		else
++			append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
++				    MOVE_DEST_DESCBUF |
++				    (53 * 4 << MOVE_OFFSET_SHIFT) | 16);
++
+ 		/*
+ 		 * Read the new SEQ IN PTR command, Input Pointer, Input Length
+ 		 * and then jump back to the next command from the
+diff --git a/drivers/crypto/caam/caamalg_qi.c b/drivers/crypto/caam/caamalg_qi.c
+index ad150274..d454dfbf 100644
+--- a/drivers/crypto/caam/caamalg_qi.c
++++ b/drivers/crypto/caam/caamalg_qi.c
+@@ -915,7 +915,7 @@ struct aead_edesc {
+  * @tmp: array of scatterlists used by 'scatterwalk_ffwd'
+  * @qm_sg_dma: bus physical mapped address of h/w link table
+  * @drv_req: driver-specific request structure
+- * @sgt: the h/w link table
++ * @sgt: the h/w link table, followed by IV
+  */
+ struct tls_edesc {
+ 	int src_nents;
+@@ -1376,6 +1376,7 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req, bool encrypt)
+ 	struct tls_edesc *edesc;
+ 	dma_addr_t qm_sg_dma, iv_dma = 0;
+ 	int ivsize = 0;
++	u8 *iv;
+ 	int qm_sg_index, qm_sg_ents = 0, qm_sg_bytes;
+ 	int in_len, out_len;
+ 	struct qm_sg_entry *sg_table, *fd_sgt;
+@@ -1395,7 +1396,7 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req, bool encrypt)
+ 	if (unlikely(IS_ERR_OR_NULL(drv_ctx)))
+ 		return (struct tls_edesc *)drv_ctx;
+ 
+-	/* allocate space for base edesc and hw desc commands, link tables */
++	/* allocate space for base edesc, link tables and IV */
+ 	edesc = qi_cache_alloc(GFP_DMA | flags);
+ 	if (unlikely(!edesc)) {
+ 		dev_err(qidev, "could not allocate extended descriptor\n");
+@@ -1465,16 +1466,6 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req, bool encrypt)
+ 		}
+ 	}
+ 
+-	ivsize = crypto_aead_ivsize(aead);
+-	iv_dma = dma_map_single(qidev, req->iv, ivsize, DMA_TO_DEVICE);
+-	if (dma_mapping_error(qidev, iv_dma)) {
+-		dev_err(qidev, "unable to map IV\n");
+-		caam_unmap(qidev, req->src, dst, src_nents, dst_nents, 0, 0,
+-			   op_type, 0, 0);
+-		qi_cache_free(edesc);
+-		return ERR_PTR(-ENOMEM);
+-	}
+-
+ 	/*
+ 	 * Create S/G table: IV, src, dst.
+ 	 * Input is not contiguous.
+@@ -1484,6 +1475,19 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req, bool encrypt)
+ 	sg_table = &edesc->sgt[0];
+ 	qm_sg_bytes = qm_sg_ents * sizeof(*sg_table);
+ 
++	ivsize = crypto_aead_ivsize(aead);
++	iv = (u8 *)(sg_table + qm_sg_ents);
++	/* Make sure IV is located in a DMAable area */
++	memcpy(iv, req->iv, ivsize);
++	iv_dma = dma_map_single(qidev, iv, ivsize, DMA_TO_DEVICE);
++	if (dma_mapping_error(qidev, iv_dma)) {
++		dev_err(qidev, "unable to map IV\n");
++		caam_unmap(qidev, req->src, dst, src_nents, dst_nents, 0, 0, 0,
++			   0, 0);
++		qi_cache_free(edesc);
++		return ERR_PTR(-ENOMEM);
++	}
++
+ 	edesc->src_nents = src_nents;
+ 	edesc->dst_nents = dst_nents;
+ 	edesc->dst = dst;
+diff --git a/drivers/crypto/caam/caamalg_qi2.c b/drivers/crypto/caam/caamalg_qi2.c
+index 23db390c..dd7a59bb 100644
+--- a/drivers/crypto/caam/caamalg_qi2.c
++++ b/drivers/crypto/caam/caamalg_qi2.c
+@@ -1,37 +1,10 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * Copyright 2015-2016 Freescale Semiconductor Inc.
+- * Copyright 2017 NXP
+- *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *	 notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *	 notice, this list of conditions and the following disclaimer in the
+- *	 documentation and/or other materials provided with the distribution.
+- *     * Neither the names of the above-listed copyright holders nor the
+- *	 names of any contributors may be used to endorse or promote products
+- *	 derived from this software without specific prior written permission.
+- *
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+- * POSSIBILITY OF SUCH DAMAGE.
++ * Copyright 2017-2018 NXP
+  */
+ 
++#include <linux/fsl/mc.h>
+ #include "compat.h"
+ #include "regs.h"
+ #include "caamalg_qi2.h"
+@@ -43,7 +16,6 @@
+ #include "key_gen.h"
+ #include "caamalg_desc.h"
+ #include "caamhash_desc.h"
+-#include "../../../drivers/staging/fsl-mc/include/mc.h"
+ #include "../../../drivers/staging/fsl-mc/include/dpaa2-io.h"
+ #include "../../../drivers/staging/fsl-mc/include/dpaa2-fd.h"
+ 
+@@ -85,6 +57,12 @@ struct caam_aead_alg {
+ 	bool registered;
+ };
+ 
++struct caam_skcipher_alg {
++	struct skcipher_alg skcipher;
++	struct caam_alg_entry caam;
++	bool registered;
++};
++
+ /**
+  * caam_ctx - per-session context
+  * @flc: Flow Contexts array
+@@ -153,9 +131,8 @@ static inline void qi_cache_free(void *obj)
+ static struct caam_request *to_caam_req(struct crypto_async_request *areq)
+ {
+ 	switch (crypto_tfm_alg_type(areq->tfm)) {
+-	case CRYPTO_ALG_TYPE_ABLKCIPHER:
+-	case CRYPTO_ALG_TYPE_GIVCIPHER:
+-		return ablkcipher_request_ctx(ablkcipher_request_cast(areq));
++	case CRYPTO_ALG_TYPE_SKCIPHER:
++		return skcipher_request_ctx(skcipher_request_cast(areq));
+ 	case CRYPTO_ALG_TYPE_AEAD:
+ 		return aead_request_ctx(container_of(areq, struct aead_request,
+ 						     base));
+@@ -169,8 +146,7 @@ static struct caam_request *to_caam_req(struct crypto_async_request *areq)
+ static void caam_unmap(struct device *dev, struct scatterlist *src,
+ 		       struct scatterlist *dst, int src_nents,
+ 		       int dst_nents, dma_addr_t iv_dma, int ivsize,
+-		       enum optype op_type, dma_addr_t qm_sg_dma,
+-		       int qm_sg_bytes)
++		       dma_addr_t qm_sg_dma, int qm_sg_bytes)
+ {
+ 	if (dst != src) {
+ 		if (src_nents)
+@@ -181,9 +157,7 @@ static void caam_unmap(struct device *dev, struct scatterlist *src,
+ 	}
+ 
+ 	if (iv_dma)
+-		dma_unmap_single(dev, iv_dma, ivsize,
+-				 op_type == GIVENCRYPT ? DMA_FROM_DEVICE :
+-							 DMA_TO_DEVICE);
++		dma_unmap_single(dev, iv_dma, ivsize, DMA_TO_DEVICE);
+ 
+ 	if (qm_sg_bytes)
+ 		dma_unmap_single(dev, qm_sg_dma, qm_sg_bytes, DMA_TO_DEVICE);
+@@ -265,7 +239,7 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
+ 				       ivsize, ctx->authsize, is_rfc3686, nonce,
+ 				       ctx1_iv_off, true, priv->sec_attr.era);
+ 
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[ENCRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -296,7 +270,7 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
+ 			       ivsize, ctx->authsize, alg->caam.geniv,
+ 			       is_rfc3686, nonce, ctx1_iv_off, true,
+ 			       priv->sec_attr.era);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[DECRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -399,9 +373,8 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
+ 	int qm_sg_index = 0, qm_sg_nents = 0, qm_sg_bytes;
+ 	int in_len, out_len;
+ 	struct dpaa2_sg_entry *sg_table;
+-	enum optype op_type = encrypt ? ENCRYPT : DECRYPT;
+ 
+-	/* allocate space for base edesc and link tables */
++	/* allocate space for base edesc, link tables and IV */
+ 	edesc = qi_cache_zalloc(GFP_DMA | flags);
+ 	if (unlikely(!edesc)) {
+ 		dev_err(dev, "could not allocate extended descriptor\n");
+@@ -471,17 +444,8 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
+ 		}
+ 	}
+ 
+-	if ((alg->caam.rfc3686 && encrypt) || !alg->caam.geniv) {
++	if ((alg->caam.rfc3686 && encrypt) || !alg->caam.geniv)
+ 		ivsize = crypto_aead_ivsize(aead);
+-		iv_dma = dma_map_single(dev, req->iv, ivsize, DMA_TO_DEVICE);
+-		if (dma_mapping_error(dev, iv_dma)) {
+-			dev_err(dev, "unable to map IV\n");
+-			caam_unmap(dev, req->src, req->dst, src_nents,
+-				   dst_nents, 0, 0, op_type, 0, 0);
+-			qi_cache_free(edesc);
+-			return ERR_PTR(-ENOMEM);
+-		}
+-	}
+ 
+ 	/*
+ 	 * Create S/G table: req->assoclen, [IV,] req->src [, req->dst].
+@@ -489,27 +453,45 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
+ 	 */
+ 	qm_sg_nents = 1 + !!ivsize + mapped_src_nents +
+ 		      (mapped_dst_nents > 1 ? mapped_dst_nents : 0);
+-	if (unlikely(qm_sg_nents > CAAM_QI_MAX_AEAD_SG)) {
+-		dev_err(dev, "Insufficient S/G entries: %d > %lu\n",
+-			qm_sg_nents, CAAM_QI_MAX_AEAD_SG);
+-		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, op_type, 0, 0);
++	sg_table = &edesc->sgt[0];
++	qm_sg_bytes = qm_sg_nents * sizeof(*sg_table);
++	if (unlikely(offsetof(struct aead_edesc, sgt) + qm_sg_bytes + ivsize >
++		     CAAM_QI_MEMCACHE_SIZE)) {
++		dev_err(dev, "No space for %d S/G entries and/or %dB IV\n",
++			qm_sg_nents, ivsize);
++		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents, 0,
++			   0, 0, 0);
+ 		qi_cache_free(edesc);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+-	sg_table = &edesc->sgt[0];
+-	qm_sg_bytes = qm_sg_nents * sizeof(*sg_table);
++
++	if (ivsize) {
++		u8 *iv = (u8 *)(sg_table + qm_sg_nents);
++
++		/* Make sure IV is located in a DMAable area */
++		memcpy(iv, req->iv, ivsize);
++
++		iv_dma = dma_map_single(dev, iv, ivsize, DMA_TO_DEVICE);
++		if (dma_mapping_error(dev, iv_dma)) {
++			dev_err(dev, "unable to map IV\n");
++			caam_unmap(dev, req->src, req->dst, src_nents,
++				   dst_nents, 0, 0, 0, 0);
++			qi_cache_free(edesc);
++			return ERR_PTR(-ENOMEM);
++		}
++	}
+ 
+ 	edesc->src_nents = src_nents;
+ 	edesc->dst_nents = dst_nents;
+ 	edesc->iv_dma = iv_dma;
+ 
+-	edesc->assoclen_dma = dma_map_single(dev, &req->assoclen, 4,
++	edesc->assoclen = cpu_to_caam32(req->assoclen);
++	edesc->assoclen_dma = dma_map_single(dev, &edesc->assoclen, 4,
+ 					     DMA_TO_DEVICE);
+ 	if (dma_mapping_error(dev, edesc->assoclen_dma)) {
+ 		dev_err(dev, "unable to map assoclen\n");
+ 		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, op_type, 0, 0);
++			   iv_dma, ivsize, 0, 0);
+ 		qi_cache_free(edesc);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+@@ -532,7 +514,7 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
+ 		dev_err(dev, "unable to map S/G table\n");
+ 		dma_unmap_single(dev, edesc->assoclen_dma, 4, DMA_TO_DEVICE);
+ 		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, op_type, 0, 0);
++			   iv_dma, ivsize, 0, 0);
+ 		qi_cache_free(edesc);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+@@ -592,10 +574,10 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req,
+ 	struct tls_edesc *edesc;
+ 	dma_addr_t qm_sg_dma, iv_dma = 0;
+ 	int ivsize = 0;
++	u8 *iv;
+ 	int qm_sg_index, qm_sg_ents = 0, qm_sg_bytes;
+ 	int in_len, out_len;
+ 	struct dpaa2_sg_entry *sg_table;
+-	enum optype op_type = encrypt ? ENCRYPT : DECRYPT;
+ 	struct scatterlist *dst;
+ 
+ 	if (encrypt) {
+@@ -606,7 +588,7 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req,
+ 		authsize = ctx->authsize;
+ 	}
+ 
+-	/* allocate space for base edesc and link tables */
++	/* allocate space for base edesc, link tables and IV */
+ 	edesc = qi_cache_zalloc(GFP_DMA | flags);
+ 	if (unlikely(!edesc)) {
+ 		dev_err(dev, "could not allocate extended descriptor\n");
+@@ -676,16 +658,6 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req,
+ 		}
+ 	}
+ 
+-	ivsize = crypto_aead_ivsize(tls);
+-	iv_dma = dma_map_single(dev, req->iv, ivsize, DMA_TO_DEVICE);
+-	if (dma_mapping_error(dev, iv_dma)) {
+-		dev_err(dev, "unable to map IV\n");
+-		caam_unmap(dev, req->src, dst, src_nents, dst_nents, 0, 0,
+-			   op_type, 0, 0);
+-		qi_cache_free(edesc);
+-		return ERR_PTR(-ENOMEM);
+-	}
+-
+ 	/*
+ 	 * Create S/G table: IV, src, dst.
+ 	 * Input is not contiguous.
+@@ -695,6 +667,19 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req,
+ 	sg_table = &edesc->sgt[0];
+ 	qm_sg_bytes = qm_sg_ents * sizeof(*sg_table);
+ 
++	ivsize = crypto_aead_ivsize(tls);
++	iv = (u8 *)(sg_table + qm_sg_ents);
++	/* Make sure IV is located in a DMAable area */
++	memcpy(iv, req->iv, ivsize);
++	iv_dma = dma_map_single(dev, iv, ivsize, DMA_TO_DEVICE);
++	if (dma_mapping_error(dev, iv_dma)) {
++		dev_err(dev, "unable to map IV\n");
++		caam_unmap(dev, req->src, dst, src_nents, dst_nents, 0, 0, 0,
++			   0);
++		qi_cache_free(edesc);
++		return ERR_PTR(-ENOMEM);
++	}
++
+ 	edesc->src_nents = src_nents;
+ 	edesc->dst_nents = dst_nents;
+ 	edesc->dst = dst;
+@@ -714,7 +699,7 @@ static struct tls_edesc *tls_edesc_alloc(struct aead_request *req,
+ 	if (dma_mapping_error(dev, qm_sg_dma)) {
+ 		dev_err(dev, "unable to map S/G table\n");
+ 		caam_unmap(dev, req->src, dst, src_nents, dst_nents, iv_dma,
+-			   ivsize, op_type, 0, 0);
++			   ivsize, 0, 0);
+ 		qi_cache_free(edesc);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+@@ -796,7 +781,7 @@ static int tls_set_sh_desc(struct crypto_aead *tls)
+ 	cnstr_shdsc_tls_encap(desc, &ctx->cdata, &ctx->adata,
+ 			      assoclen, ivsize, ctx->authsize, blocksize,
+ 			      priv->sec_attr.era);
+-	flc->flc[1] = desc_len(desc);
++	flc->flc[1] = cpu_to_caam32(desc_len(desc));
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[ENCRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -813,7 +798,7 @@ static int tls_set_sh_desc(struct crypto_aead *tls)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_tls_decap(desc, &ctx->cdata, &ctx->adata, assoclen, ivsize,
+ 			      ctx->authsize, blocksize, priv->sec_attr.era);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[DECRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -903,7 +888,7 @@ static int gcm_set_sh_desc(struct crypto_aead *aead)
+ 	flc = &ctx->flc[ENCRYPT];
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_gcm_encap(desc, &ctx->cdata, ivsize, ctx->authsize, true);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[ENCRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -923,7 +908,7 @@ static int gcm_set_sh_desc(struct crypto_aead *aead)
+ 	flc = &ctx->flc[DECRYPT];
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_gcm_decap(desc, &ctx->cdata, ivsize, ctx->authsize, true);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[DECRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -990,7 +975,7 @@ static int rfc4106_set_sh_desc(struct crypto_aead *aead)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_rfc4106_encap(desc, &ctx->cdata, ivsize, ctx->authsize,
+ 				  true);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[ENCRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -1010,7 +995,7 @@ static int rfc4106_set_sh_desc(struct crypto_aead *aead)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_rfc4106_decap(desc, &ctx->cdata, ivsize, ctx->authsize,
+ 				  true);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[DECRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -1086,7 +1071,7 @@ static int rfc4543_set_sh_desc(struct crypto_aead *aead)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_rfc4543_encap(desc, &ctx->cdata, ivsize, ctx->authsize,
+ 				  true);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[ENCRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -1106,7 +1091,7 @@ static int rfc4543_set_sh_desc(struct crypto_aead *aead)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_rfc4543_decap(desc, &ctx->cdata, ivsize, ctx->authsize,
+ 				  true);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[DECRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -1151,20 +1136,21 @@ static int rfc4543_setkey(struct crypto_aead *aead,
+ 	return rfc4543_set_sh_desc(aead);
+ }
+ 
+-static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
+-			     const u8 *key, unsigned int keylen)
++static int skcipher_setkey(struct crypto_skcipher *skcipher, const u8 *key,
++			   unsigned int keylen)
+ {
+-	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
+-	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(ablkcipher);
+-	const char *alg_name = crypto_tfm_alg_name(tfm);
++	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
++	struct caam_skcipher_alg *alg =
++		container_of(crypto_skcipher_alg(skcipher),
++			     struct caam_skcipher_alg, skcipher);
+ 	struct device *dev = ctx->dev;
+ 	struct caam_flc *flc;
+-	unsigned int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
++	unsigned int ivsize = crypto_skcipher_ivsize(skcipher);
+ 	u32 *desc;
+ 	u32 ctx1_iv_off = 0;
+ 	const bool ctr_mode = ((ctx->cdata.algtype & OP_ALG_AAI_MASK) ==
+ 			       OP_ALG_AAI_CTR_MOD128);
+-	const bool is_rfc3686 = (ctr_mode && strstr(alg_name, "rfc3686"));
++	const bool is_rfc3686 = alg->caam.rfc3686;
+ 
+ #ifdef DEBUG
+ 	print_hex_dump(KERN_ERR, "key in @" __stringify(__LINE__)": ",
+@@ -1192,51 +1178,40 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
+ 	ctx->cdata.key_virt = key;
+ 	ctx->cdata.key_inline = true;
+ 
+-	/* ablkcipher_encrypt shared descriptor */
++	/* skcipher_encrypt shared descriptor */
+ 	flc = &ctx->flc[ENCRYPT];
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_ablkcipher_encap(desc, &ctx->cdata, ivsize,
+ 				     is_rfc3686, ctx1_iv_off);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[ENCRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+ 
+-	/* ablkcipher_decrypt shared descriptor */
++	/* skcipher_decrypt shared descriptor */
+ 	flc = &ctx->flc[DECRYPT];
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_ablkcipher_decap(desc, &ctx->cdata, ivsize,
+ 				     is_rfc3686, ctx1_iv_off);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[DECRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+ 
+-	/* ablkcipher_givencrypt shared descriptor */
+-	flc = &ctx->flc[GIVENCRYPT];
+-	desc = flc->sh_desc;
+-	cnstr_shdsc_ablkcipher_givencap(desc, &ctx->cdata,
+-					ivsize, is_rfc3686, ctx1_iv_off);
+-	flc->flc[1] = desc_len(desc); /* SDL */
+-	dma_sync_single_for_device(dev, ctx->flc_dma[GIVENCRYPT],
+-				   sizeof(flc->flc) + desc_bytes(desc),
+-				   ctx->dir);
+-
+ 	return 0;
+ }
+ 
+-static int xts_ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
+-				 const u8 *key, unsigned int keylen)
++static int xts_skcipher_setkey(struct crypto_skcipher *skcipher, const u8 *key,
++			       unsigned int keylen)
+ {
+-	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
++	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
+ 	struct device *dev = ctx->dev;
+ 	struct caam_flc *flc;
+ 	u32 *desc;
+ 
+ 	if (keylen != 2 * AES_MIN_KEY_SIZE  && keylen != 2 * AES_MAX_KEY_SIZE) {
+ 		dev_err(dev, "key size mismatch\n");
+-		crypto_ablkcipher_set_flags(ablkcipher,
+-					    CRYPTO_TFM_RES_BAD_KEY_LEN);
++		crypto_skcipher_set_flags(skcipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 
+@@ -1244,20 +1219,20 @@ static int xts_ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
+ 	ctx->cdata.key_virt = key;
+ 	ctx->cdata.key_inline = true;
+ 
+-	/* xts_ablkcipher_encrypt shared descriptor */
++	/* xts_skcipher_encrypt shared descriptor */
+ 	flc = &ctx->flc[ENCRYPT];
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_xts_ablkcipher_encap(desc, &ctx->cdata);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[ENCRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+ 
+-	/* xts_ablkcipher_decrypt shared descriptor */
++	/* xts_skcipher_decrypt shared descriptor */
+ 	flc = &ctx->flc[DECRYPT];
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_xts_ablkcipher_decap(desc, &ctx->cdata);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(dev, ctx->flc_dma[DECRYPT],
+ 				   sizeof(flc->flc) + desc_bytes(desc),
+ 				   ctx->dir);
+@@ -1265,38 +1240,36 @@ static int xts_ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
+ 	return 0;
+ }
+ 
+-static struct ablkcipher_edesc *ablkcipher_edesc_alloc(struct ablkcipher_request
+-						       *req, bool encrypt)
++static struct skcipher_edesc *skcipher_edesc_alloc(struct skcipher_request *req)
+ {
+-	struct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);
+-	struct caam_request *req_ctx = ablkcipher_request_ctx(req);
++	struct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);
++	struct caam_request *req_ctx = skcipher_request_ctx(req);
+ 	struct dpaa2_fl_entry *in_fle = &req_ctx->fd_flt[1];
+ 	struct dpaa2_fl_entry *out_fle = &req_ctx->fd_flt[0];
+-	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
++	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
+ 	struct device *dev = ctx->dev;
+ 	gfp_t flags = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?
+ 		       GFP_KERNEL : GFP_ATOMIC;
+ 	int src_nents, mapped_src_nents, dst_nents = 0, mapped_dst_nents = 0;
+-	struct ablkcipher_edesc *edesc;
++	struct skcipher_edesc *edesc;
+ 	dma_addr_t iv_dma;
+-	bool in_contig;
+-	int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
+-	int dst_sg_idx, qm_sg_ents;
++	u8 *iv;
++	int ivsize = crypto_skcipher_ivsize(skcipher);
++	int dst_sg_idx, qm_sg_ents, qm_sg_bytes;
+ 	struct dpaa2_sg_entry *sg_table;
+-	enum optype op_type = encrypt ? ENCRYPT : DECRYPT;
+ 
+-	src_nents = sg_nents_for_len(req->src, req->nbytes);
++	src_nents = sg_nents_for_len(req->src, req->cryptlen);
+ 	if (unlikely(src_nents < 0)) {
+ 		dev_err(dev, "Insufficient bytes (%d) in src S/G\n",
+-			req->nbytes);
++			req->cryptlen);
+ 		return ERR_PTR(src_nents);
+ 	}
+ 
+ 	if (unlikely(req->dst != req->src)) {
+-		dst_nents = sg_nents_for_len(req->dst, req->nbytes);
++		dst_nents = sg_nents_for_len(req->dst, req->cryptlen);
+ 		if (unlikely(dst_nents < 0)) {
+ 			dev_err(dev, "Insufficient bytes (%d) in dst S/G\n",
+-				req->nbytes);
++				req->cryptlen);
+ 			return ERR_PTR(dst_nents);
+ 		}
+ 
+@@ -1323,236 +1296,78 @@ static struct ablkcipher_edesc *ablkcipher_edesc_alloc(struct ablkcipher_request
+ 		}
+ 	}
+ 
+-	iv_dma = dma_map_single(dev, req->info, ivsize, DMA_TO_DEVICE);
+-	if (dma_mapping_error(dev, iv_dma)) {
+-		dev_err(dev, "unable to map IV\n");
+-		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents, 0,
+-			   0, 0, 0, 0);
+-		return ERR_PTR(-ENOMEM);
+-	}
+-
+-	if (mapped_src_nents == 1 &&
+-	    iv_dma + ivsize == sg_dma_address(req->src)) {
+-		in_contig = true;
+-		qm_sg_ents = 0;
+-	} else {
+-		in_contig = false;
+-		qm_sg_ents = 1 + mapped_src_nents;
+-	}
++	qm_sg_ents = 1 + mapped_src_nents;
+ 	dst_sg_idx = qm_sg_ents;
+ 
+ 	qm_sg_ents += mapped_dst_nents > 1 ? mapped_dst_nents : 0;
+-	if (unlikely(qm_sg_ents > CAAM_QI_MAX_ABLKCIPHER_SG)) {
+-		dev_err(dev, "Insufficient S/G entries: %d > %lu\n",
+-			qm_sg_ents, CAAM_QI_MAX_ABLKCIPHER_SG);
+-		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, op_type, 0, 0);
++	qm_sg_bytes = qm_sg_ents * sizeof(struct dpaa2_sg_entry);
++	if (unlikely(offsetof(struct skcipher_edesc, sgt) + qm_sg_bytes +
++		     ivsize > CAAM_QI_MEMCACHE_SIZE)) {
++		dev_err(dev, "No space for %d S/G entries and/or %dB IV\n",
++			qm_sg_ents, ivsize);
++		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents, 0,
++			   0, 0, 0);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+-	/* allocate space for base edesc and link tables */
++	/* allocate space for base edesc, link tables and IV */
+ 	edesc = qi_cache_zalloc(GFP_DMA | flags);
+ 	if (unlikely(!edesc)) {
+ 		dev_err(dev, "could not allocate extended descriptor\n");
+-		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, op_type, 0, 0);
++		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents, 0,
++			   0, 0, 0);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+-	edesc->src_nents = src_nents;
+-	edesc->dst_nents = dst_nents;
+-	edesc->iv_dma = iv_dma;
++	/* Make sure IV is located in a DMAable area */
+ 	sg_table = &edesc->sgt[0];
+-	edesc->qm_sg_bytes = qm_sg_ents * sizeof(*sg_table);
+-
+-	if (!in_contig) {
+-		dma_to_qm_sg_one(sg_table, iv_dma, ivsize, 0);
+-		sg_to_qm_sg_last(req->src, mapped_src_nents, sg_table + 1, 0);
+-	}
+-
+-	if (mapped_dst_nents > 1)
+-		sg_to_qm_sg_last(req->dst, mapped_dst_nents, sg_table +
+-				 dst_sg_idx, 0);
+-
+-	edesc->qm_sg_dma = dma_map_single(dev, sg_table, edesc->qm_sg_bytes,
+-					  DMA_TO_DEVICE);
+-	if (dma_mapping_error(dev, edesc->qm_sg_dma)) {
+-		dev_err(dev, "unable to map S/G table\n");
+-		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, op_type, 0, 0);
+-		qi_cache_free(edesc);
+-		return ERR_PTR(-ENOMEM);
+-	}
+-
+-	memset(&req_ctx->fd_flt, 0, sizeof(req_ctx->fd_flt));
+-	dpaa2_fl_set_final(in_fle, true);
+-	dpaa2_fl_set_len(in_fle, req->nbytes + ivsize);
+-	dpaa2_fl_set_len(out_fle, req->nbytes);
+-
+-	if (!in_contig) {
+-		dpaa2_fl_set_format(in_fle, dpaa2_fl_sg);
+-		dpaa2_fl_set_addr(in_fle, edesc->qm_sg_dma);
+-	} else {
+-		dpaa2_fl_set_format(in_fle, dpaa2_fl_single);
+-		dpaa2_fl_set_addr(in_fle, iv_dma);
+-	}
+-
+-	if (req->src == req->dst) {
+-		if (!in_contig) {
+-			dpaa2_fl_set_format(out_fle, dpaa2_fl_sg);
+-			dpaa2_fl_set_addr(out_fle, edesc->qm_sg_dma +
+-					  sizeof(*sg_table));
+-		} else {
+-			dpaa2_fl_set_format(out_fle, dpaa2_fl_single);
+-			dpaa2_fl_set_addr(out_fle, sg_dma_address(req->src));
+-		}
+-	} else if (mapped_dst_nents > 1) {
+-		dpaa2_fl_set_format(out_fle, dpaa2_fl_sg);
+-		dpaa2_fl_set_addr(out_fle, edesc->qm_sg_dma + dst_sg_idx *
+-				  sizeof(*sg_table));
+-	} else {
+-		dpaa2_fl_set_format(out_fle, dpaa2_fl_single);
+-		dpaa2_fl_set_addr(out_fle, sg_dma_address(req->dst));
+-	}
+-
+-	return edesc;
+-}
+-
+-static struct ablkcipher_edesc *ablkcipher_giv_edesc_alloc(
+-	struct skcipher_givcrypt_request *greq)
+-{
+-	struct ablkcipher_request *req = &greq->creq;
+-	struct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);
+-	struct caam_request *req_ctx = ablkcipher_request_ctx(req);
+-	struct dpaa2_fl_entry *in_fle = &req_ctx->fd_flt[1];
+-	struct dpaa2_fl_entry *out_fle = &req_ctx->fd_flt[0];
+-	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
+-	struct device *dev = ctx->dev;
+-	gfp_t flags = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?
+-		       GFP_KERNEL : GFP_ATOMIC;
+-	int src_nents, mapped_src_nents, dst_nents, mapped_dst_nents;
+-	struct ablkcipher_edesc *edesc;
+-	dma_addr_t iv_dma;
+-	bool out_contig;
+-	int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
+-	struct dpaa2_sg_entry *sg_table;
+-	int dst_sg_idx, qm_sg_ents;
+-
+-	src_nents = sg_nents_for_len(req->src, req->nbytes);
+-	if (unlikely(src_nents < 0)) {
+-		dev_err(dev, "Insufficient bytes (%d) in src S/G\n",
+-			req->nbytes);
+-		return ERR_PTR(src_nents);
+-	}
+-
+-	if (unlikely(req->dst != req->src)) {
+-		dst_nents = sg_nents_for_len(req->dst, req->nbytes);
+-		if (unlikely(dst_nents < 0)) {
+-			dev_err(dev, "Insufficient bytes (%d) in dst S/G\n",
+-				req->nbytes);
+-			return ERR_PTR(dst_nents);
+-		}
+-
+-		mapped_src_nents = dma_map_sg(dev, req->src, src_nents,
+-					      DMA_TO_DEVICE);
+-		if (unlikely(!mapped_src_nents)) {
+-			dev_err(dev, "unable to map source\n");
+-			return ERR_PTR(-ENOMEM);
+-		}
+-
+-		mapped_dst_nents = dma_map_sg(dev, req->dst, dst_nents,
+-					      DMA_FROM_DEVICE);
+-		if (unlikely(!mapped_dst_nents)) {
+-			dev_err(dev, "unable to map destination\n");
+-			dma_unmap_sg(dev, req->src, src_nents, DMA_TO_DEVICE);
+-			return ERR_PTR(-ENOMEM);
+-		}
+-	} else {
+-		mapped_src_nents = dma_map_sg(dev, req->src, src_nents,
+-					      DMA_BIDIRECTIONAL);
+-		if (unlikely(!mapped_src_nents)) {
+-			dev_err(dev, "unable to map source\n");
+-			return ERR_PTR(-ENOMEM);
+-		}
+-
+-		dst_nents = src_nents;
+-		mapped_dst_nents = src_nents;
+-	}
++	iv = (u8 *)(sg_table + qm_sg_ents);
++	memcpy(iv, req->iv, ivsize);
+ 
+-	iv_dma = dma_map_single(dev, greq->giv, ivsize, DMA_FROM_DEVICE);
++	iv_dma = dma_map_single(dev, iv, ivsize, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(dev, iv_dma)) {
+ 		dev_err(dev, "unable to map IV\n");
+ 		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents, 0,
+-			   0, 0, 0, 0);
+-		return ERR_PTR(-ENOMEM);
+-	}
+-
+-	qm_sg_ents = mapped_src_nents > 1 ? mapped_src_nents : 0;
+-	dst_sg_idx = qm_sg_ents;
+-	if (mapped_dst_nents == 1 &&
+-	    iv_dma + ivsize == sg_dma_address(req->dst)) {
+-		out_contig = true;
+-	} else {
+-		out_contig = false;
+-		qm_sg_ents += 1 + mapped_dst_nents;
+-	}
+-
+-	if (unlikely(qm_sg_ents > CAAM_QI_MAX_ABLKCIPHER_SG)) {
+-		dev_err(dev, "Insufficient S/G entries: %d > %lu\n",
+-			qm_sg_ents, CAAM_QI_MAX_ABLKCIPHER_SG);
+-		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, GIVENCRYPT, 0, 0);
+-		return ERR_PTR(-ENOMEM);
+-	}
+-
+-	/* allocate space for base edesc and link tables */
+-	edesc = qi_cache_zalloc(GFP_DMA | flags);
+-	if (!edesc) {
+-		dev_err(dev, "could not allocate extended descriptor\n");
+-		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, GIVENCRYPT, 0, 0);
++			   0, 0, 0);
++		qi_cache_free(edesc);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+ 	edesc->src_nents = src_nents;
+ 	edesc->dst_nents = dst_nents;
+ 	edesc->iv_dma = iv_dma;
+-	sg_table = &edesc->sgt[0];
+-	edesc->qm_sg_bytes = qm_sg_ents * sizeof(*sg_table);
++	edesc->qm_sg_bytes = qm_sg_bytes;
+ 
+-	if (mapped_src_nents > 1)
+-		sg_to_qm_sg_last(req->src, mapped_src_nents, sg_table, 0);
++	dma_to_qm_sg_one(sg_table, iv_dma, ivsize, 0);
++	sg_to_qm_sg_last(req->src, mapped_src_nents, sg_table + 1, 0);
+ 
+-	if (!out_contig) {
+-		dma_to_qm_sg_one(sg_table + dst_sg_idx, iv_dma, ivsize, 0);
++	if (mapped_dst_nents > 1)
+ 		sg_to_qm_sg_last(req->dst, mapped_dst_nents, sg_table +
+-				 dst_sg_idx + 1, 0);
+-	}
++				 dst_sg_idx, 0);
+ 
+ 	edesc->qm_sg_dma = dma_map_single(dev, sg_table, edesc->qm_sg_bytes,
+ 					  DMA_TO_DEVICE);
+ 	if (dma_mapping_error(dev, edesc->qm_sg_dma)) {
+ 		dev_err(dev, "unable to map S/G table\n");
+ 		caam_unmap(dev, req->src, req->dst, src_nents, dst_nents,
+-			   iv_dma, ivsize, GIVENCRYPT, 0, 0);
++			   iv_dma, ivsize, 0, 0);
+ 		qi_cache_free(edesc);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+ 	memset(&req_ctx->fd_flt, 0, sizeof(req_ctx->fd_flt));
+ 	dpaa2_fl_set_final(in_fle, true);
+-	dpaa2_fl_set_len(in_fle, req->nbytes);
+-	dpaa2_fl_set_len(out_fle, ivsize + req->nbytes);
++	dpaa2_fl_set_len(in_fle, req->cryptlen + ivsize);
++	dpaa2_fl_set_len(out_fle, req->cryptlen);
+ 
+-	if (mapped_src_nents > 1) {
+-		dpaa2_fl_set_format(in_fle, dpaa2_fl_sg);
+-		dpaa2_fl_set_addr(in_fle, edesc->qm_sg_dma);
+-	} else {
+-		dpaa2_fl_set_format(in_fle, dpaa2_fl_single);
+-		dpaa2_fl_set_addr(in_fle, sg_dma_address(req->src));
+-	}
++	dpaa2_fl_set_format(in_fle, dpaa2_fl_sg);
++	dpaa2_fl_set_addr(in_fle, edesc->qm_sg_dma);
+ 
+-	if (!out_contig) {
++	if (req->src == req->dst) {
++		dpaa2_fl_set_format(out_fle, dpaa2_fl_sg);
++		dpaa2_fl_set_addr(out_fle, edesc->qm_sg_dma +
++				  sizeof(*sg_table));
++	} else if (mapped_dst_nents > 1) {
+ 		dpaa2_fl_set_format(out_fle, dpaa2_fl_sg);
+ 		dpaa2_fl_set_addr(out_fle, edesc->qm_sg_dma + dst_sg_idx *
+ 				  sizeof(*sg_table));
+@@ -1569,11 +1384,9 @@ static void aead_unmap(struct device *dev, struct aead_edesc *edesc,
+ {
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	int ivsize = crypto_aead_ivsize(aead);
+-	struct caam_request *caam_req = aead_request_ctx(req);
+ 
+ 	caam_unmap(dev, req->src, req->dst, edesc->src_nents, edesc->dst_nents,
+-		   edesc->iv_dma, ivsize, caam_req->op_type,
+-		   edesc->qm_sg_dma, edesc->qm_sg_bytes);
++		   edesc->iv_dma, ivsize, edesc->qm_sg_dma, edesc->qm_sg_bytes);
+ 	dma_unmap_single(dev, edesc->assoclen_dma, 4, DMA_TO_DEVICE);
+ }
+ 
+@@ -1582,24 +1395,20 @@ static void tls_unmap(struct device *dev, struct tls_edesc *edesc,
+ {
+ 	struct crypto_aead *tls = crypto_aead_reqtfm(req);
+ 	int ivsize = crypto_aead_ivsize(tls);
+-	struct caam_request *caam_req = aead_request_ctx(req);
+ 
+ 	caam_unmap(dev, req->src, edesc->dst, edesc->src_nents,
+-		   edesc->dst_nents, edesc->iv_dma, ivsize, caam_req->op_type,
+-		   edesc->qm_sg_dma, edesc->qm_sg_bytes);
++		   edesc->dst_nents, edesc->iv_dma, ivsize, edesc->qm_sg_dma,
++		   edesc->qm_sg_bytes);
+ }
+ 
+-static void ablkcipher_unmap(struct device *dev,
+-			     struct ablkcipher_edesc *edesc,
+-			     struct ablkcipher_request *req)
++static void skcipher_unmap(struct device *dev, struct skcipher_edesc *edesc,
++			   struct skcipher_request *req)
+ {
+-	struct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);
+-	int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
+-	struct caam_request *caam_req = ablkcipher_request_ctx(req);
++	struct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);
++	int ivsize = crypto_skcipher_ivsize(skcipher);
+ 
+ 	caam_unmap(dev, req->src, req->dst, edesc->src_nents, edesc->dst_nents,
+-		   edesc->iv_dma, ivsize, caam_req->op_type,
+-		   edesc->qm_sg_dma, edesc->qm_sg_bytes);
++		   edesc->iv_dma, ivsize, edesc->qm_sg_dma, edesc->qm_sg_bytes);
+ }
+ 
+ static void aead_encrypt_done(void *cbk_ctx, u32 status)
+@@ -1674,7 +1483,6 @@ static int aead_encrypt(struct aead_request *req)
+ 
+ 	caam_req->flc = &ctx->flc[ENCRYPT];
+ 	caam_req->flc_dma = ctx->flc_dma[ENCRYPT];
+-	caam_req->op_type = ENCRYPT;
+ 	caam_req->cbk = aead_encrypt_done;
+ 	caam_req->ctx = &req->base;
+ 	caam_req->edesc = edesc;
+@@ -1703,7 +1511,6 @@ static int aead_decrypt(struct aead_request *req)
+ 
+ 	caam_req->flc = &ctx->flc[DECRYPT];
+ 	caam_req->flc_dma = ctx->flc_dma[DECRYPT];
+-	caam_req->op_type = DECRYPT;
+ 	caam_req->cbk = aead_decrypt_done;
+ 	caam_req->ctx = &req->base;
+ 	caam_req->edesc = edesc;
+@@ -1789,7 +1596,6 @@ static int tls_encrypt(struct aead_request *req)
+ 
+ 	caam_req->flc = &ctx->flc[ENCRYPT];
+ 	caam_req->flc_dma = ctx->flc_dma[ENCRYPT];
+-	caam_req->op_type = ENCRYPT;
+ 	caam_req->cbk = tls_encrypt_done;
+ 	caam_req->ctx = &req->base;
+ 	caam_req->edesc = edesc;
+@@ -1818,7 +1624,6 @@ static int tls_decrypt(struct aead_request *req)
+ 
+ 	caam_req->flc = &ctx->flc[DECRYPT];
+ 	caam_req->flc_dma = ctx->flc_dma[DECRYPT];
+-	caam_req->op_type = DECRYPT;
+ 	caam_req->cbk = tls_decrypt_done;
+ 	caam_req->ctx = &req->base;
+ 	caam_req->edesc = edesc;
+@@ -1848,16 +1653,16 @@ static int ipsec_gcm_decrypt(struct aead_request *req)
+ 	return aead_decrypt(req);
+ }
+ 
+-static void ablkcipher_done(void *cbk_ctx, u32 status)
++static void skcipher_encrypt_done(void *cbk_ctx, u32 status)
+ {
+ 	struct crypto_async_request *areq = cbk_ctx;
+-	struct ablkcipher_request *req = ablkcipher_request_cast(areq);
++	struct skcipher_request *req = skcipher_request_cast(areq);
+ 	struct caam_request *req_ctx = to_caam_req(areq);
+-	struct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);
+-	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
+-	struct ablkcipher_edesc *edesc = req_ctx->edesc;
++	struct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);
++	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
++	struct skcipher_edesc *edesc = req_ctx->edesc;
+ 	int ecode = 0;
+-	int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
++	int ivsize = crypto_skcipher_ivsize(skcipher);
+ 
+ #ifdef DEBUG
+ 	dev_err(ctx->dev, "%s %d: err 0x%x\n", __func__, __LINE__, status);
+@@ -1870,136 +1675,135 @@ static void ablkcipher_done(void *cbk_ctx, u32 status)
+ 
+ #ifdef DEBUG
+ 	print_hex_dump(KERN_ERR, "dstiv  @" __stringify(__LINE__)": ",
+-		       DUMP_PREFIX_ADDRESS, 16, 4, req->info,
++		       DUMP_PREFIX_ADDRESS, 16, 4, req->iv,
+ 		       edesc->src_nents > 1 ? 100 : ivsize, 1);
+ 	caam_dump_sg(KERN_ERR, "dst    @" __stringify(__LINE__)": ",
+ 		     DUMP_PREFIX_ADDRESS, 16, 4, req->dst,
+-		     edesc->dst_nents > 1 ? 100 : req->nbytes, 1);
++		     edesc->dst_nents > 1 ? 100 : req->cryptlen, 1);
+ #endif
+ 
+-	ablkcipher_unmap(ctx->dev, edesc, req);
+-	qi_cache_free(edesc);
++	skcipher_unmap(ctx->dev, edesc, req);
+ 
+ 	/*
+-	 * The crypto API expects us to set the IV (req->info) to the last
++	 * The crypto API expects us to set the IV (req->iv) to the last
+ 	 * ciphertext block. This is used e.g. by the CTS mode.
+ 	 */
+-	scatterwalk_map_and_copy(req->info, req->dst, req->nbytes - ivsize,
++	scatterwalk_map_and_copy(req->iv, req->dst, req->cryptlen - ivsize,
+ 				 ivsize, 0);
+ 
+-	ablkcipher_request_complete(req, ecode);
++	qi_cache_free(edesc);
++	skcipher_request_complete(req, ecode);
+ }
+ 
+-static int ablkcipher_encrypt(struct ablkcipher_request *req)
++static void skcipher_decrypt_done(void *cbk_ctx, u32 status)
+ {
+-	struct ablkcipher_edesc *edesc;
+-	struct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);
+-	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
+-	struct caam_request *caam_req = ablkcipher_request_ctx(req);
+-	int ret;
++	struct crypto_async_request *areq = cbk_ctx;
++	struct skcipher_request *req = skcipher_request_cast(areq);
++	struct caam_request *req_ctx = to_caam_req(areq);
++	struct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);
++	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
++	struct skcipher_edesc *edesc = req_ctx->edesc;
++	int ecode = 0;
++#ifdef DEBUG
++	int ivsize = crypto_skcipher_ivsize(skcipher);
+ 
+-	/* allocate extended descriptor */
+-	edesc = ablkcipher_edesc_alloc(req, true);
+-	if (IS_ERR(edesc))
+-		return PTR_ERR(edesc);
++	dev_err(ctx->dev, "%s %d: err 0x%x\n", __func__, __LINE__, status);
++#endif
+ 
+-	caam_req->flc = &ctx->flc[ENCRYPT];
+-	caam_req->flc_dma = ctx->flc_dma[ENCRYPT];
+-	caam_req->op_type = ENCRYPT;
+-	caam_req->cbk = ablkcipher_done;
+-	caam_req->ctx = &req->base;
+-	caam_req->edesc = edesc;
+-	ret = dpaa2_caam_enqueue(ctx->dev, caam_req);
+-	if (ret != -EINPROGRESS &&
+-	    !(ret == -EBUSY && req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {
+-		ablkcipher_unmap(ctx->dev, edesc, req);
+-		qi_cache_free(edesc);
++	if (unlikely(status)) {
++		caam_qi2_strstatus(ctx->dev, status);
++		ecode = -EIO;
+ 	}
+ 
+-	return ret;
++#ifdef DEBUG
++	print_hex_dump(KERN_ERR, "dstiv  @" __stringify(__LINE__)": ",
++		       DUMP_PREFIX_ADDRESS, 16, 4, req->iv,
++		       edesc->src_nents > 1 ? 100 : ivsize, 1);
++	caam_dump_sg(KERN_ERR, "dst    @" __stringify(__LINE__)": ",
++		     DUMP_PREFIX_ADDRESS, 16, 4, req->dst,
++		     edesc->dst_nents > 1 ? 100 : req->cryptlen, 1);
++#endif
++
++	skcipher_unmap(ctx->dev, edesc, req);
++	qi_cache_free(edesc);
++	skcipher_request_complete(req, ecode);
+ }
+ 
+-static int ablkcipher_givencrypt(struct skcipher_givcrypt_request *greq)
++static int skcipher_encrypt(struct skcipher_request *req)
+ {
+-	struct ablkcipher_request *req = &greq->creq;
+-	struct ablkcipher_edesc *edesc;
+-	struct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);
+-	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
+-	struct caam_request *caam_req = ablkcipher_request_ctx(req);
++	struct skcipher_edesc *edesc;
++	struct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);
++	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
++	struct caam_request *caam_req = skcipher_request_ctx(req);
+ 	int ret;
+ 
+ 	/* allocate extended descriptor */
+-	edesc = ablkcipher_giv_edesc_alloc(greq);
++	edesc = skcipher_edesc_alloc(req);
+ 	if (IS_ERR(edesc))
+ 		return PTR_ERR(edesc);
+ 
+-	caam_req->flc = &ctx->flc[GIVENCRYPT];
+-	caam_req->flc_dma = ctx->flc_dma[GIVENCRYPT];
+-	caam_req->op_type = GIVENCRYPT;
+-	caam_req->cbk = ablkcipher_done;
++	caam_req->flc = &ctx->flc[ENCRYPT];
++	caam_req->flc_dma = ctx->flc_dma[ENCRYPT];
++	caam_req->cbk = skcipher_encrypt_done;
+ 	caam_req->ctx = &req->base;
+ 	caam_req->edesc = edesc;
+ 	ret = dpaa2_caam_enqueue(ctx->dev, caam_req);
+ 	if (ret != -EINPROGRESS &&
+ 	    !(ret == -EBUSY && req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {
+-		ablkcipher_unmap(ctx->dev, edesc, req);
++		skcipher_unmap(ctx->dev, edesc, req);
+ 		qi_cache_free(edesc);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+-static int ablkcipher_decrypt(struct ablkcipher_request *req)
++static int skcipher_decrypt(struct skcipher_request *req)
+ {
+-	struct ablkcipher_edesc *edesc;
+-	struct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);
+-	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
+-	struct caam_request *caam_req = ablkcipher_request_ctx(req);
++	struct skcipher_edesc *edesc;
++	struct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);
++	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
++	struct caam_request *caam_req = skcipher_request_ctx(req);
++	int ivsize = crypto_skcipher_ivsize(skcipher);
+ 	int ret;
+ 
+ 	/* allocate extended descriptor */
+-	edesc = ablkcipher_edesc_alloc(req, false);
++	edesc = skcipher_edesc_alloc(req);
+ 	if (IS_ERR(edesc))
+ 		return PTR_ERR(edesc);
+ 
++	/*
++	 * The crypto API expects us to set the IV (req->iv) to the last
++	 * ciphertext block.
++	 */
++	scatterwalk_map_and_copy(req->iv, req->src, req->cryptlen - ivsize,
++				 ivsize, 0);
++
+ 	caam_req->flc = &ctx->flc[DECRYPT];
+ 	caam_req->flc_dma = ctx->flc_dma[DECRYPT];
+-	caam_req->op_type = DECRYPT;
+-	caam_req->cbk = ablkcipher_done;
++	caam_req->cbk = skcipher_decrypt_done;
+ 	caam_req->ctx = &req->base;
+ 	caam_req->edesc = edesc;
+ 	ret = dpaa2_caam_enqueue(ctx->dev, caam_req);
+ 	if (ret != -EINPROGRESS &&
+ 	    !(ret == -EBUSY && req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {
+-		ablkcipher_unmap(ctx->dev, edesc, req);
++		skcipher_unmap(ctx->dev, edesc, req);
+ 		qi_cache_free(edesc);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+-struct caam_crypto_alg {
+-	struct list_head entry;
+-	struct crypto_alg crypto_alg;
+-	struct caam_alg_entry caam;
+-};
+-
+-static int caam_cra_init(struct crypto_tfm *tfm, bool uses_dkp)
++static int caam_cra_init(struct caam_ctx *ctx, struct caam_alg_entry *caam,
++			 bool uses_dkp)
+ {
+-	struct crypto_alg *alg = tfm->__crt_alg;
+-	struct caam_crypto_alg *caam_alg = container_of(alg, typeof(*caam_alg),
+-							crypto_alg);
+-	struct caam_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	dma_addr_t dma_addr;
+ 	int i;
+ 
+ 	/* copy descriptor header template value */
+-	ctx->cdata.algtype = OP_TYPE_CLASS1_ALG |
+-			     caam_alg->caam.class1_alg_type;
+-	ctx->adata.algtype = OP_TYPE_CLASS2_ALG |
+-			     caam_alg->caam.class2_alg_type;
++	ctx->cdata.algtype = OP_TYPE_CLASS1_ALG | caam->class1_alg_type;
++	ctx->adata.algtype = OP_TYPE_CLASS2_ALG | caam->class2_alg_type;
+ 
+-	ctx->dev = caam_alg->caam.dev;
++	ctx->dev = caam->dev;
+ 	ctx->dir = uses_dkp ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
+ 
+ 	dma_addr = dma_map_single_attrs(ctx->dev, ctx->flc,
+@@ -2017,21 +1821,24 @@ static int caam_cra_init(struct crypto_tfm *tfm, bool uses_dkp)
+ 	return 0;
+ }
+ 
+-static int caam_cra_init_ablkcipher(struct crypto_tfm *tfm)
++static int caam_cra_init_skcipher(struct crypto_skcipher *tfm)
+ {
+-	struct ablkcipher_tfm *ablkcipher_tfm =
+-		crypto_ablkcipher_crt(__crypto_ablkcipher_cast(tfm));
++	struct skcipher_alg *alg = crypto_skcipher_alg(tfm);
++	struct caam_skcipher_alg *caam_alg =
++		container_of(alg, typeof(*caam_alg), skcipher);
+ 
+-	ablkcipher_tfm->reqsize = sizeof(struct caam_request);
+-	return caam_cra_init(tfm, false);
++	crypto_skcipher_set_reqsize(tfm, sizeof(struct caam_request));
++	return caam_cra_init(crypto_skcipher_ctx(tfm), &caam_alg->caam, false);
+ }
+ 
+ static int caam_cra_init_aead(struct crypto_aead *tfm)
+ {
+ 	struct aead_alg *alg = crypto_aead_alg(tfm);
++	struct caam_aead_alg *caam_alg = container_of(alg, typeof(*caam_alg),
++						      aead);
+ 
+ 	crypto_aead_set_reqsize(tfm, sizeof(struct caam_request));
+-	return caam_cra_init(crypto_aead_tfm(tfm),
++	return caam_cra_init(crypto_aead_ctx(tfm), &caam_alg->caam,
+ 			     (alg->setkey == aead_setkey) ||
+ 			     (alg->setkey == tls_setkey));
+ }
+@@ -2043,9 +1850,9 @@ static void caam_exit_common(struct caam_ctx *ctx)
+ 			       DMA_ATTR_SKIP_CPU_SYNC);
+ }
+ 
+-static void caam_cra_exit(struct crypto_tfm *tfm)
++static void caam_cra_exit(struct crypto_skcipher *tfm)
+ {
+-	caam_exit_common(crypto_tfm_ctx(tfm));
++	caam_exit_common(crypto_skcipher_ctx(tfm));
+ }
+ 
+ static void caam_cra_exit_aead(struct crypto_aead *tfm)
+@@ -2053,122 +1860,111 @@ static void caam_cra_exit_aead(struct crypto_aead *tfm)
+ 	caam_exit_common(crypto_aead_ctx(tfm));
+ }
+ 
+-#define template_ablkcipher	template_u.ablkcipher
+-struct caam_alg_template {
+-	char name[CRYPTO_MAX_ALG_NAME];
+-	char driver_name[CRYPTO_MAX_ALG_NAME];
+-	unsigned int blocksize;
+-	u32 type;
+-	union {
+-		struct ablkcipher_alg ablkcipher;
+-	} template_u;
+-	u32 class1_alg_type;
+-	u32 class2_alg_type;
+-};
+-
+-static struct caam_alg_template driver_algs[] = {
+-	/* ablkcipher descriptor */
++static struct caam_skcipher_alg driver_algs[] = {
+ 	{
+-		.name = "cbc(aes)",
+-		.driver_name = "cbc-aes-caam-qi2",
+-		.blocksize = AES_BLOCK_SIZE,
+-		.type = CRYPTO_ALG_TYPE_GIVCIPHER,
+-		.template_ablkcipher = {
+-			.setkey = ablkcipher_setkey,
+-			.encrypt = ablkcipher_encrypt,
+-			.decrypt = ablkcipher_decrypt,
+-			.givencrypt = ablkcipher_givencrypt,
+-			.geniv = "<built-in>",
++		.skcipher = {
++			.base = {
++				.cra_name = "cbc(aes)",
++				.cra_driver_name = "cbc-aes-caam-qi2",
++				.cra_blocksize = AES_BLOCK_SIZE,
++			},
++			.setkey = skcipher_setkey,
++			.encrypt = skcipher_encrypt,
++			.decrypt = skcipher_decrypt,
+ 			.min_keysize = AES_MIN_KEY_SIZE,
+ 			.max_keysize = AES_MAX_KEY_SIZE,
+ 			.ivsize = AES_BLOCK_SIZE,
+ 		},
+-		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
++		.caam.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+ 	},
+ 	{
+-		.name = "cbc(des3_ede)",
+-		.driver_name = "cbc-3des-caam-qi2",
+-		.blocksize = DES3_EDE_BLOCK_SIZE,
+-		.type = CRYPTO_ALG_TYPE_GIVCIPHER,
+-		.template_ablkcipher = {
+-			.setkey = ablkcipher_setkey,
+-			.encrypt = ablkcipher_encrypt,
+-			.decrypt = ablkcipher_decrypt,
+-			.givencrypt = ablkcipher_givencrypt,
+-			.geniv = "<built-in>",
++		.skcipher = {
++			.base = {
++				.cra_name = "cbc(des3_ede)",
++				.cra_driver_name = "cbc-3des-caam-qi2",
++				.cra_blocksize = DES3_EDE_BLOCK_SIZE,
++			},
++			.setkey = skcipher_setkey,
++			.encrypt = skcipher_encrypt,
++			.decrypt = skcipher_decrypt,
+ 			.min_keysize = DES3_EDE_KEY_SIZE,
+ 			.max_keysize = DES3_EDE_KEY_SIZE,
+ 			.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		},
+-		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
++		.caam.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
+ 	},
+ 	{
+-		.name = "cbc(des)",
+-		.driver_name = "cbc-des-caam-qi2",
+-		.blocksize = DES_BLOCK_SIZE,
+-		.type = CRYPTO_ALG_TYPE_GIVCIPHER,
+-		.template_ablkcipher = {
+-			.setkey = ablkcipher_setkey,
+-			.encrypt = ablkcipher_encrypt,
+-			.decrypt = ablkcipher_decrypt,
+-			.givencrypt = ablkcipher_givencrypt,
+-			.geniv = "<built-in>",
++		.skcipher = {
++			.base = {
++				.cra_name = "cbc(des)",
++				.cra_driver_name = "cbc-des-caam-qi2",
++				.cra_blocksize = DES_BLOCK_SIZE,
++			},
++			.setkey = skcipher_setkey,
++			.encrypt = skcipher_encrypt,
++			.decrypt = skcipher_decrypt,
+ 			.min_keysize = DES_KEY_SIZE,
+ 			.max_keysize = DES_KEY_SIZE,
+ 			.ivsize = DES_BLOCK_SIZE,
+ 		},
+-		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
++		.caam.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
+ 	},
+ 	{
+-		.name = "ctr(aes)",
+-		.driver_name = "ctr-aes-caam-qi2",
+-		.blocksize = 1,
+-		.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+-		.template_ablkcipher = {
+-			.setkey = ablkcipher_setkey,
+-			.encrypt = ablkcipher_encrypt,
+-			.decrypt = ablkcipher_decrypt,
+-			.geniv = "chainiv",
++		.skcipher = {
++			.base = {
++				.cra_name = "ctr(aes)",
++				.cra_driver_name = "ctr-aes-caam-qi2",
++				.cra_blocksize = 1,
++			},
++			.setkey = skcipher_setkey,
++			.encrypt = skcipher_encrypt,
++			.decrypt = skcipher_decrypt,
+ 			.min_keysize = AES_MIN_KEY_SIZE,
+ 			.max_keysize = AES_MAX_KEY_SIZE,
+ 			.ivsize = AES_BLOCK_SIZE,
++			.chunksize = AES_BLOCK_SIZE,
+ 		},
+-		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CTR_MOD128,
++		.caam.class1_alg_type = OP_ALG_ALGSEL_AES |
++					OP_ALG_AAI_CTR_MOD128,
+ 	},
+ 	{
+-		.name = "rfc3686(ctr(aes))",
+-		.driver_name = "rfc3686-ctr-aes-caam-qi2",
+-		.blocksize = 1,
+-		.type = CRYPTO_ALG_TYPE_GIVCIPHER,
+-		.template_ablkcipher = {
+-			.setkey = ablkcipher_setkey,
+-			.encrypt = ablkcipher_encrypt,
+-			.decrypt = ablkcipher_decrypt,
+-			.givencrypt = ablkcipher_givencrypt,
+-			.geniv = "<built-in>",
++		.skcipher = {
++			.base = {
++				.cra_name = "rfc3686(ctr(aes))",
++				.cra_driver_name = "rfc3686-ctr-aes-caam-qi2",
++				.cra_blocksize = 1,
++			},
++			.setkey = skcipher_setkey,
++			.encrypt = skcipher_encrypt,
++			.decrypt = skcipher_decrypt,
+ 			.min_keysize = AES_MIN_KEY_SIZE +
+ 				       CTR_RFC3686_NONCE_SIZE,
+ 			.max_keysize = AES_MAX_KEY_SIZE +
+ 				       CTR_RFC3686_NONCE_SIZE,
+ 			.ivsize = CTR_RFC3686_IV_SIZE,
++			.chunksize = AES_BLOCK_SIZE,
++		},
++		.caam = {
++			.class1_alg_type = OP_ALG_ALGSEL_AES |
++					   OP_ALG_AAI_CTR_MOD128,
++			.rfc3686 = true,
+ 		},
+-		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CTR_MOD128,
+ 	},
+ 	{
+-		.name = "xts(aes)",
+-		.driver_name = "xts-aes-caam-qi2",
+-		.blocksize = AES_BLOCK_SIZE,
+-		.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
+-		.template_ablkcipher = {
+-			.setkey = xts_ablkcipher_setkey,
+-			.encrypt = ablkcipher_encrypt,
+-			.decrypt = ablkcipher_decrypt,
+-			.geniv = "eseqiv",
++		.skcipher = {
++			.base = {
++				.cra_name = "xts(aes)",
++				.cra_driver_name = "xts-aes-caam-qi2",
++				.cra_blocksize = AES_BLOCK_SIZE,
++			},
++			.setkey = xts_skcipher_setkey,
++			.encrypt = skcipher_encrypt,
++			.decrypt = skcipher_decrypt,
+ 			.min_keysize = 2 * AES_MIN_KEY_SIZE,
+ 			.max_keysize = 2 * AES_MAX_KEY_SIZE,
+ 			.ivsize = AES_BLOCK_SIZE,
+ 		},
+-		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_XTS,
++		.caam.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_XTS,
+ 	}
+ };
+ 
+@@ -3354,46 +3150,17 @@ static struct caam_aead_alg driver_aeads[] = {
+ 	},
+ };
+ 
+-static struct caam_crypto_alg *caam_alg_alloc(struct caam_alg_template
+-					      *template)
++static void caam_skcipher_alg_init(struct caam_skcipher_alg *t_alg)
+ {
+-	struct caam_crypto_alg *t_alg;
+-	struct crypto_alg *alg;
+-
+-	t_alg = kzalloc(sizeof(*t_alg), GFP_KERNEL);
+-	if (!t_alg)
+-		return ERR_PTR(-ENOMEM);
+-
+-	alg = &t_alg->crypto_alg;
+-
+-	snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", template->name);
+-	snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
+-		 template->driver_name);
+-	alg->cra_module = THIS_MODULE;
+-	alg->cra_exit = caam_cra_exit;
+-	alg->cra_priority = CAAM_CRA_PRIORITY;
+-	alg->cra_blocksize = template->blocksize;
+-	alg->cra_alignmask = 0;
+-	alg->cra_ctxsize = sizeof(struct caam_ctx);
+-	alg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY |
+-			 template->type;
+-	switch (template->type) {
+-	case CRYPTO_ALG_TYPE_GIVCIPHER:
+-		alg->cra_init = caam_cra_init_ablkcipher;
+-		alg->cra_type = &crypto_givcipher_type;
+-		alg->cra_ablkcipher = template->template_ablkcipher;
+-		break;
+-	case CRYPTO_ALG_TYPE_ABLKCIPHER:
+-		alg->cra_init = caam_cra_init_ablkcipher;
+-		alg->cra_type = &crypto_ablkcipher_type;
+-		alg->cra_ablkcipher = template->template_ablkcipher;
+-		break;
+-	}
++	struct skcipher_alg *alg = &t_alg->skcipher;
+ 
+-	t_alg->caam.class1_alg_type = template->class1_alg_type;
+-	t_alg->caam.class2_alg_type = template->class2_alg_type;
++	alg->base.cra_module = THIS_MODULE;
++	alg->base.cra_priority = CAAM_CRA_PRIORITY;
++	alg->base.cra_ctxsize = sizeof(struct caam_ctx);
++	alg->base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY;
+ 
+-	return t_alg;
++	alg->init = caam_cra_init_skcipher;
++	alg->exit = caam_cra_exit;
+ }
+ 
+ static void caam_aead_alg_init(struct caam_aead_alg *t_alg)
+@@ -3555,7 +3322,7 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_ahash(desc, &ctx->adata, OP_ALG_AS_UPDATE, ctx->ctx_len,
+ 			  ctx->ctx_len, true, priv->sec_attr.era);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(ctx->dev, ctx->flc_dma[UPDATE],
+ 				   desc_bytes(desc), DMA_BIDIRECTIONAL);
+ #ifdef DEBUG
+@@ -3569,7 +3336,7 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_ahash(desc, &ctx->adata, OP_ALG_AS_INIT, ctx->ctx_len,
+ 			  ctx->ctx_len, false, priv->sec_attr.era);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(ctx->dev, ctx->flc_dma[UPDATE_FIRST],
+ 				   desc_bytes(desc), DMA_BIDIRECTIONAL);
+ #ifdef DEBUG
+@@ -3583,7 +3350,7 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_ahash(desc, &ctx->adata, OP_ALG_AS_FINALIZE, digestsize,
+ 			  ctx->ctx_len, true, priv->sec_attr.era);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(ctx->dev, ctx->flc_dma[FINALIZE],
+ 				   desc_bytes(desc), DMA_BIDIRECTIONAL);
+ #ifdef DEBUG
+@@ -3597,7 +3364,7 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
+ 	desc = flc->sh_desc;
+ 	cnstr_shdsc_ahash(desc, &ctx->adata, OP_ALG_AS_INITFINAL, digestsize,
+ 			  ctx->ctx_len, false, priv->sec_attr.era);
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	dma_sync_single_for_device(ctx->dev, ctx->flc_dma[DIGEST],
+ 				   desc_bytes(desc), DMA_BIDIRECTIONAL);
+ #ifdef DEBUG
+@@ -3658,7 +3425,7 @@ static int hash_digest_key(struct caam_hash_ctx *ctx, const u8 *key_in,
+ 	append_seq_store(desc, digestsize, LDST_CLASS_2_CCB |
+ 			 LDST_SRCDST_BYTE_CONTEXT);
+ 
+-	flc->flc[1] = desc_len(desc); /* SDL */
++	flc->flc[1] = cpu_to_caam32(desc_len(desc)); /* SDL */
+ 	flc_dma = dma_map_single(ctx->dev, flc, sizeof(flc->flc) +
+ 				 desc_bytes(desc), DMA_TO_DEVICE);
+ 	if (dma_mapping_error(ctx->dev, flc_dma)) {
+@@ -5076,6 +4843,7 @@ static struct caam_hash_alg *caam_hash_alloc(struct device *dev,
+ 			 template->name);
+ 		snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
+ 			 template->driver_name);
++		t_alg->ahash_alg.setkey = NULL;
+ 	}
+ 	alg->cra_module = THIS_MODULE;
+ 	alg->cra_init = caam_hash_cra_init;
+@@ -5477,7 +5245,7 @@ static int __cold dpaa2_dpseci_setup(struct fsl_mc_device *ls_dev)
+ 
+ 	i = 0;
+ 	for_each_online_cpu(cpu) {
+-		dev_info(dev, "prio %d: rx queue %d, tx queue %d\n", i,
++		dev_info(dev, "pair %d: rx queue %d, tx queue %d\n", i,
+ 			 priv->rx_queue_attr[i].fqid,
+ 			 priv->tx_queue_attr[i].fqid);
+ 
+@@ -5559,7 +5327,6 @@ static int __cold dpaa2_dpseci_disable(struct dpaa2_caam_priv *priv)
+ 	return 0;
+ }
+ 
+-static struct list_head alg_list;
+ static struct list_head hash_list;
+ 
+ static int dpaa2_caam_probe(struct fsl_mc_device *dpseci_dev)
+@@ -5649,11 +5416,9 @@ static int dpaa2_caam_probe(struct fsl_mc_device *dpseci_dev)
+ 	}
+ 
+ 	/* register crypto algorithms the device supports */
+-	INIT_LIST_HEAD(&alg_list);
+ 	for (i = 0; i < ARRAY_SIZE(driver_algs); i++) {
+-		struct caam_crypto_alg *t_alg;
+-		struct caam_alg_template *alg = driver_algs + i;
+-		u32 alg_sel = alg->class1_alg_type & OP_ALG_ALGSEL_MASK;
++		struct caam_skcipher_alg *t_alg = driver_algs + i;
++		u32 alg_sel = t_alg->caam.class1_alg_type & OP_ALG_ALGSEL_MASK;
+ 
+ 		/* Skip DES algorithms if not supported by device */
+ 		if (!priv->sec_attr.des_acc_num &&
+@@ -5666,24 +5431,17 @@ static int dpaa2_caam_probe(struct fsl_mc_device *dpseci_dev)
+ 		    (alg_sel == OP_ALG_ALGSEL_AES))
+ 			continue;
+ 
+-		t_alg = caam_alg_alloc(alg);
+-		if (IS_ERR(t_alg)) {
+-			err = PTR_ERR(t_alg);
+-			dev_warn(dev, "%s alg allocation failed: %d\n",
+-				 alg->driver_name, err);
+-			continue;
+-		}
+ 		t_alg->caam.dev = dev;
++		caam_skcipher_alg_init(t_alg);
+ 
+-		err = crypto_register_alg(&t_alg->crypto_alg);
++		err = crypto_register_skcipher(&t_alg->skcipher);
+ 		if (err) {
+ 			dev_warn(dev, "%s alg registration failed: %d\n",
+-				 t_alg->crypto_alg.cra_driver_name, err);
+-			kfree(t_alg);
++				 t_alg->skcipher.base.cra_driver_name, err);
+ 			continue;
+ 		}
+ 
+-		list_add_tail(&t_alg->entry, &alg_list);
++		t_alg->registered = true;
+ 		registered = true;
+ 	}
+ 
+@@ -5817,14 +5575,11 @@ static int __cold dpaa2_caam_remove(struct fsl_mc_device *ls_dev)
+ 			crypto_unregister_aead(&t_alg->aead);
+ 	}
+ 
+-	if (alg_list.next) {
+-		struct caam_crypto_alg *t_alg, *n;
++	for (i = 0; i < ARRAY_SIZE(driver_algs); i++) {
++		struct caam_skcipher_alg *t_alg = driver_algs + i;
+ 
+-		list_for_each_entry_safe(t_alg, n, &alg_list, entry) {
+-			crypto_unregister_alg(&t_alg->crypto_alg);
+-			list_del(&t_alg->entry);
+-			kfree(t_alg);
+-		}
++		if (t_alg->registered)
++			crypto_unregister_skcipher(&t_alg->skcipher);
+ 	}
+ 
+ 	if (hash_list.next) {
+@@ -5879,7 +5634,7 @@ int dpaa2_caam_enqueue(struct device *dev, struct caam_request *req)
+ 	memset(&fd, 0, sizeof(fd));
+ 	dpaa2_fd_set_format(&fd, dpaa2_fd_list);
+ 	dpaa2_fd_set_addr(&fd, req->fd_flt_dma);
+-	dpaa2_fd_set_len(&fd, req->fd_flt[1].len);
++	dpaa2_fd_set_len(&fd, dpaa2_fl_get_len(&req->fd_flt[1]));
+ 	dpaa2_fd_set_flc(&fd, req->flc_dma);
+ 
+ 	/*
+diff --git a/drivers/crypto/caam/caamalg_qi2.h b/drivers/crypto/caam/caamalg_qi2.h
+index 87ef74aa..cd8880bf 100644
+--- a/drivers/crypto/caam/caamalg_qi2.h
++++ b/drivers/crypto/caam/caamalg_qi2.h
+@@ -79,8 +79,8 @@ struct dpaa2_caam_priv {
+ 
+ 	struct dpseci_attr dpseci_attr;
+ 	struct dpseci_sec_attr sec_attr;
+-	struct dpseci_rx_queue_attr rx_queue_attr[DPSECI_PRIO_NUM];
+-	struct dpseci_tx_queue_attr tx_queue_attr[DPSECI_PRIO_NUM];
++	struct dpseci_rx_queue_attr rx_queue_attr[DPSECI_MAX_QUEUE_NUM];
++	struct dpseci_tx_queue_attr tx_queue_attr[DPSECI_MAX_QUEUE_NUM];
+ 	int num_pairs;
+ 
+ 	/* congestion */
+@@ -154,8 +154,9 @@ struct dpaa2_caam_priv_per_cpu {
+  * @iv_dma: dma address of iv for checking continuity and link table
+  * @qm_sg_bytes: length of dma mapped h/w link table
+  * @qm_sg_dma: bus physical mapped address of h/w link table
++ * @assoclen: associated data length, in CAAM endianness
+  * @assoclen_dma: bus physical mapped address of req->assoclen
+- * @sgt: the h/w link table
++ * @sgt: the h/w link table, followed by IV
+  */
+ struct aead_edesc {
+ 	int src_nents;
+@@ -163,10 +164,8 @@ struct aead_edesc {
+ 	dma_addr_t iv_dma;
+ 	int qm_sg_bytes;
+ 	dma_addr_t qm_sg_dma;
++	unsigned int assoclen;
+ 	dma_addr_t assoclen_dma;
+-#define CAAM_QI_MAX_AEAD_SG						\
+-	((CAAM_QI_MEMCACHE_SIZE - offsetof(struct aead_edesc, sgt)) /	\
+-	 sizeof(struct dpaa2_sg_entry))
+ 	struct dpaa2_sg_entry sgt[0];
+ };
+ 
+@@ -179,7 +178,7 @@ struct aead_edesc {
+  * @qm_sg_dma: bus physical mapped address of h/w link table
+  * @tmp: array of scatterlists used by 'scatterwalk_ffwd'
+  * @dst: pointer to output scatterlist, usefull for unmapping
+- * @sgt: the h/w link table
++ * @sgt: the h/w link table, followed by IV
+  */
+ struct tls_edesc {
+ 	int src_nents;
+@@ -193,23 +192,20 @@ struct tls_edesc {
+ };
+ 
+ /*
+- * ablkcipher_edesc - s/w-extended ablkcipher descriptor
++ * skcipher_edesc - s/w-extended skcipher descriptor
+  * @src_nents: number of segments in input scatterlist
+  * @dst_nents: number of segments in output scatterlist
+  * @iv_dma: dma address of iv for checking continuity and link table
+  * @qm_sg_bytes: length of dma mapped qm_sg space
+  * @qm_sg_dma: I/O virtual address of h/w link table
+- * @sgt: the h/w link table
++ * @sgt: the h/w link table, followed by IV
+  */
+-struct ablkcipher_edesc {
++struct skcipher_edesc {
+ 	int src_nents;
+ 	int dst_nents;
+ 	dma_addr_t iv_dma;
+ 	int qm_sg_bytes;
+ 	dma_addr_t qm_sg_dma;
+-#define CAAM_QI_MAX_ABLKCIPHER_SG					    \
+-	((CAAM_QI_MEMCACHE_SIZE - offsetof(struct ablkcipher_edesc, sgt)) / \
+-	 sizeof(struct dpaa2_sg_entry))
+ 	struct dpaa2_sg_entry sgt[0];
+ };
+ 
+@@ -242,7 +238,6 @@ struct caam_flc {
+ enum optype {
+ 	ENCRYPT = 0,
+ 	DECRYPT,
+-	GIVENCRYPT,
+ 	NUM_OP
+ };
+ 
+@@ -255,17 +250,15 @@ enum optype {
+  * @fd_flt_dma: DMA address for the frame list table
+  * @flc: Flow Context
+  * @flc_dma: I/O virtual address of Flow Context
+- * @op_type: operation type
+  * @cbk: Callback function to invoke when job is completed
+  * @ctx: arbit context attached with request by the application
+- * @edesc: extended descriptor; points to one of {ablkcipher,aead}_edesc
++ * @edesc: extended descriptor; points to one of {skcipher,aead}_edesc
+  */
+ struct caam_request {
+ 	struct dpaa2_fl_entry fd_flt[2];
+ 	dma_addr_t fd_flt_dma;
+ 	struct caam_flc *flc;
+ 	dma_addr_t flc_dma;
+-	enum optype op_type;
+ 	void (*cbk)(void *ctx, u32 err);
+ 	void *ctx;
+ 	void *edesc;
+diff --git a/drivers/crypto/caam/caampkc.c b/drivers/crypto/caam/caampkc.c
+index 4fcb378e..e835b614 100644
+--- a/drivers/crypto/caam/caampkc.c
++++ b/drivers/crypto/caam/caampkc.c
+@@ -71,8 +71,8 @@ static void rsa_priv_f2_unmap(struct device *dev, struct rsa_edesc *edesc,
+ 	dma_unmap_single(dev, pdb->d_dma, key->d_sz, DMA_TO_DEVICE);
+ 	dma_unmap_single(dev, pdb->p_dma, p_sz, DMA_TO_DEVICE);
+ 	dma_unmap_single(dev, pdb->q_dma, q_sz, DMA_TO_DEVICE);
+-	dma_unmap_single(dev, pdb->tmp1_dma, p_sz, DMA_TO_DEVICE);
+-	dma_unmap_single(dev, pdb->tmp2_dma, q_sz, DMA_TO_DEVICE);
++	dma_unmap_single(dev, pdb->tmp1_dma, p_sz, DMA_BIDIRECTIONAL);
++	dma_unmap_single(dev, pdb->tmp2_dma, q_sz, DMA_BIDIRECTIONAL);
+ }
+ 
+ static void rsa_priv_f3_unmap(struct device *dev, struct rsa_edesc *edesc,
+@@ -90,8 +90,8 @@ static void rsa_priv_f3_unmap(struct device *dev, struct rsa_edesc *edesc,
+ 	dma_unmap_single(dev, pdb->dp_dma, p_sz, DMA_TO_DEVICE);
+ 	dma_unmap_single(dev, pdb->dq_dma, q_sz, DMA_TO_DEVICE);
+ 	dma_unmap_single(dev, pdb->c_dma, p_sz, DMA_TO_DEVICE);
+-	dma_unmap_single(dev, pdb->tmp1_dma, p_sz, DMA_TO_DEVICE);
+-	dma_unmap_single(dev, pdb->tmp2_dma, q_sz, DMA_TO_DEVICE);
++	dma_unmap_single(dev, pdb->tmp1_dma, p_sz, DMA_BIDIRECTIONAL);
++	dma_unmap_single(dev, pdb->tmp2_dma, q_sz, DMA_BIDIRECTIONAL);
+ }
+ 
+ /* RSA Job Completion handler */
+@@ -364,13 +364,13 @@ static int set_rsa_priv_f2_pdb(struct akcipher_request *req,
+ 		goto unmap_p;
+ 	}
+ 
+-	pdb->tmp1_dma = dma_map_single(dev, key->tmp1, p_sz, DMA_TO_DEVICE);
++	pdb->tmp1_dma = dma_map_single(dev, key->tmp1, p_sz, DMA_BIDIRECTIONAL);
+ 	if (dma_mapping_error(dev, pdb->tmp1_dma)) {
+ 		dev_err(dev, "Unable to map RSA tmp1 memory\n");
+ 		goto unmap_q;
+ 	}
+ 
+-	pdb->tmp2_dma = dma_map_single(dev, key->tmp2, q_sz, DMA_TO_DEVICE);
++	pdb->tmp2_dma = dma_map_single(dev, key->tmp2, q_sz, DMA_BIDIRECTIONAL);
+ 	if (dma_mapping_error(dev, pdb->tmp2_dma)) {
+ 		dev_err(dev, "Unable to map RSA tmp2 memory\n");
+ 		goto unmap_tmp1;
+@@ -398,7 +398,7 @@ static int set_rsa_priv_f2_pdb(struct akcipher_request *req,
+ 	return 0;
+ 
+ unmap_tmp1:
+-	dma_unmap_single(dev, pdb->tmp1_dma, p_sz, DMA_TO_DEVICE);
++	dma_unmap_single(dev, pdb->tmp1_dma, p_sz, DMA_BIDIRECTIONAL);
+ unmap_q:
+ 	dma_unmap_single(dev, pdb->q_dma, q_sz, DMA_TO_DEVICE);
+ unmap_p:
+@@ -451,13 +451,13 @@ static int set_rsa_priv_f3_pdb(struct akcipher_request *req,
+ 		goto unmap_dq;
+ 	}
+ 
+-	pdb->tmp1_dma = dma_map_single(dev, key->tmp1, p_sz, DMA_TO_DEVICE);
++	pdb->tmp1_dma = dma_map_single(dev, key->tmp1, p_sz, DMA_BIDIRECTIONAL);
+ 	if (dma_mapping_error(dev, pdb->tmp1_dma)) {
+ 		dev_err(dev, "Unable to map RSA tmp1 memory\n");
+ 		goto unmap_qinv;
+ 	}
+ 
+-	pdb->tmp2_dma = dma_map_single(dev, key->tmp2, q_sz, DMA_TO_DEVICE);
++	pdb->tmp2_dma = dma_map_single(dev, key->tmp2, q_sz, DMA_BIDIRECTIONAL);
+ 	if (dma_mapping_error(dev, pdb->tmp2_dma)) {
+ 		dev_err(dev, "Unable to map RSA tmp2 memory\n");
+ 		goto unmap_tmp1;
+@@ -485,7 +485,7 @@ static int set_rsa_priv_f3_pdb(struct akcipher_request *req,
+ 	return 0;
+ 
+ unmap_tmp1:
+-	dma_unmap_single(dev, pdb->tmp1_dma, p_sz, DMA_TO_DEVICE);
++	dma_unmap_single(dev, pdb->tmp1_dma, p_sz, DMA_BIDIRECTIONAL);
+ unmap_qinv:
+ 	dma_unmap_single(dev, pdb->c_dma, p_sz, DMA_TO_DEVICE);
+ unmap_dq:
+diff --git a/drivers/crypto/caam/compat.h b/drivers/crypto/caam/compat.h
+index 4e084f51..3f02b841 100644
+--- a/drivers/crypto/caam/compat.h
++++ b/drivers/crypto/caam/compat.h
+@@ -38,6 +38,7 @@
+ #include <crypto/authenc.h>
+ #include <crypto/akcipher.h>
+ #include <crypto/scatterwalk.h>
++#include <crypto/skcipher.h>
+ #include <crypto/internal/skcipher.h>
+ #include <crypto/internal/hash.h>
+ #include <crypto/internal/rsa.h>
+diff --git a/drivers/crypto/caam/ctrl.c b/drivers/crypto/caam/ctrl.c
+index f330a28a..9f43f966 100644
+--- a/drivers/crypto/caam/ctrl.c
++++ b/drivers/crypto/caam/ctrl.c
+@@ -229,12 +229,16 @@ static int instantiate_rng(struct device *ctrldev, int state_handle_mask,
+ 		 * without any error (HW optimizations for later
+ 		 * CAAM eras), then try again.
+ 		 */
++		if (ret)
++			break;
++
+ 		rdsta_val = rd_reg32(&ctrl->r4tst[0].rdsta) & RDSTA_IFMASK;
+ 		if ((status && status != JRSTA_SSRC_JUMP_HALT_CC) ||
+-		    !(rdsta_val & (1 << sh_idx)))
++		    !(rdsta_val & (1 << sh_idx))) {
+ 			ret = -EAGAIN;
+-		if (ret)
+ 			break;
++		}
++
+ 		dev_info(ctrldev, "Instantiated RNG4 SH%d\n", sh_idx);
+ 		/* Clear the contents before recreating the descriptor */
+ 		memset(desc, 0x00, CAAM_CMD_SZ * 7);
+diff --git a/drivers/crypto/caam/dpseci.c b/drivers/crypto/caam/dpseci.c
+index 410cd790..2d87d832 100644
+--- a/drivers/crypto/caam/dpseci.c
++++ b/drivers/crypto/caam/dpseci.c
+@@ -32,8 +32,7 @@
+  * POSSIBILITY OF SUCH DAMAGE.
+  */
+ 
+-#include "../../../drivers/staging/fsl-mc/include/mc-sys.h"
+-#include "../../../drivers/staging/fsl-mc/include/mc-cmd.h"
++#include <linux/fsl/mc.h>
+ #include "../../../drivers/staging/fsl-mc/include/dpopr.h"
+ #include "dpseci.h"
+ #include "dpseci_cmd.h"
+@@ -57,7 +56,7 @@
+ int dpseci_open(struct fsl_mc_io *mc_io, u32 cmd_flags, int dpseci_id,
+ 		u16 *token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_open *cmd_params;
+ 	int err;
+ 
+@@ -88,7 +87,7 @@ int dpseci_open(struct fsl_mc_io *mc_io, u32 cmd_flags, int dpseci_id,
+  */
+ int dpseci_close(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_CLOSE,
+ 					  cmd_flags,
+@@ -121,7 +120,7 @@ int dpseci_close(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token)
+ int dpseci_create(struct fsl_mc_io *mc_io, u16 dprc_token, u32 cmd_flags,
+ 		  const struct dpseci_cfg *cfg, u32 *obj_id)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_create *cmd_params;
+ 	int i, err;
+ 
+@@ -131,6 +130,8 @@ int dpseci_create(struct fsl_mc_io *mc_io, u16 dprc_token, u32 cmd_flags,
+ 	cmd_params = (struct dpseci_cmd_create *)cmd.params;
+ 	for (i = 0; i < 8; i++)
+ 		cmd_params->priorities[i] = cfg->priorities[i];
++	for (i = 0; i < 8; i++)
++		cmd_params->priorities2[i] = cfg->priorities[8 + i];
+ 	cmd_params->num_tx_queues = cfg->num_tx_queues;
+ 	cmd_params->num_rx_queues = cfg->num_rx_queues;
+ 	cmd_params->options = cpu_to_le32(cfg->options);
+@@ -161,7 +162,7 @@ int dpseci_create(struct fsl_mc_io *mc_io, u16 dprc_token, u32 cmd_flags,
+ int dpseci_destroy(struct fsl_mc_io *mc_io, u16 dprc_token, u32 cmd_flags,
+ 		   u32 object_id)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_destroy *cmd_params;
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_DESTROY,
+@@ -183,7 +184,7 @@ int dpseci_destroy(struct fsl_mc_io *mc_io, u16 dprc_token, u32 cmd_flags,
+  */
+ int dpseci_enable(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_ENABLE,
+ 					  cmd_flags,
+@@ -201,7 +202,7 @@ int dpseci_enable(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token)
+  */
+ int dpseci_disable(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_DISABLE,
+ 					  cmd_flags,
+@@ -222,7 +223,7 @@ int dpseci_disable(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token)
+ int dpseci_is_enabled(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 		      int *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_rsp_is_enabled *rsp_params;
+ 	int err;
+ 
+@@ -234,7 +235,7 @@ int dpseci_is_enabled(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 		return err;
+ 
+ 	rsp_params = (struct dpseci_rsp_is_enabled *)cmd.params;
+-	*en = le32_to_cpu(rsp_params->is_enabled);
++	*en = dpseci_get_field(rsp_params->is_enabled, ENABLE);
+ 
+ 	return 0;
+ }
+@@ -249,7 +250,7 @@ int dpseci_is_enabled(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+  */
+ int dpseci_reset(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_RESET,
+ 					  cmd_flags,
+@@ -271,7 +272,7 @@ int dpseci_reset(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token)
+ int dpseci_get_irq_enable(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			  u8 irq_index, u8 *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_irq_enable *cmd_params;
+ 	struct dpseci_rsp_get_irq_enable *rsp_params;
+ 	int err;
+@@ -309,7 +310,7 @@ int dpseci_get_irq_enable(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_set_irq_enable(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			  u8 irq_index, u8 en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_irq_enable *cmd_params;
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_SET_IRQ_ENABLE,
+@@ -338,7 +339,7 @@ int dpseci_set_irq_enable(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_get_irq_mask(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			u8 irq_index, u32 *mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_irq_mask *cmd_params;
+ 	int err;
+ 
+@@ -375,7 +376,7 @@ int dpseci_get_irq_mask(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_set_irq_mask(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			u8 irq_index, u32 mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_irq_mask *cmd_params;
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_SET_IRQ_MASK,
+@@ -403,7 +404,7 @@ int dpseci_set_irq_mask(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_get_irq_status(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			  u8 irq_index, u32 *status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_irq_status *cmd_params;
+ 	int err;
+ 
+@@ -437,7 +438,7 @@ int dpseci_get_irq_status(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_clear_irq_status(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			    u8 irq_index, u32 status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_irq_status *cmd_params;
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_CLEAR_IRQ_STATUS,
+@@ -462,7 +463,7 @@ int dpseci_clear_irq_status(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_get_attributes(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			  struct dpseci_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_rsp_get_attributes *rsp_params;
+ 	int err;
+ 
+@@ -497,7 +498,7 @@ int dpseci_get_attributes(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_set_rx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			u8 queue, const struct dpseci_rx_queue_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_queue *cmd_params;
+ 
+ 	cmd.header = mc_encode_cmd_header(DPSECI_CMDID_SET_RX_QUEUE,
+@@ -507,11 +508,12 @@ int dpseci_set_rx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 	cmd_params->dest_id = cpu_to_le32(cfg->dest_cfg.dest_id);
+ 	cmd_params->priority = cfg->dest_cfg.priority;
+ 	cmd_params->queue = queue;
+-	cmd_params->dest_type = cfg->dest_cfg.dest_type;
++	dpseci_set_field(cmd_params->dest_type, DEST_TYPE,
++			 cfg->dest_cfg.dest_type);
+ 	cmd_params->user_ctx = cpu_to_le64(cfg->user_ctx);
+ 	cmd_params->options = cpu_to_le32(cfg->options);
+-	cmd_params->order_preservation_en =
+-		cpu_to_le32(cfg->order_preservation_en);
++	dpseci_set_field(cmd_params->order_preservation_en, ORDER_PRESERVATION,
++			 cfg->order_preservation_en);
+ 
+ 	return mc_send_command(mc_io, &cmd);
+ }
+@@ -530,7 +532,7 @@ int dpseci_set_rx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_get_rx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			u8 queue, struct dpseci_rx_queue_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_queue *cmd_params;
+ 	int err;
+ 
+@@ -545,11 +547,13 @@ int dpseci_get_rx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 
+ 	attr->dest_cfg.dest_id = le32_to_cpu(cmd_params->dest_id);
+ 	attr->dest_cfg.priority = cmd_params->priority;
+-	attr->dest_cfg.dest_type = cmd_params->dest_type;
++	attr->dest_cfg.dest_type = dpseci_get_field(cmd_params->dest_type,
++						    DEST_TYPE);
+ 	attr->user_ctx = le64_to_cpu(cmd_params->user_ctx);
+ 	attr->fqid = le32_to_cpu(cmd_params->fqid);
+ 	attr->order_preservation_en =
+-		le32_to_cpu(cmd_params->order_preservation_en);
++		dpseci_get_field(cmd_params->order_preservation_en,
++				 ORDER_PRESERVATION);
+ 
+ 	return 0;
+ }
+@@ -568,7 +572,7 @@ int dpseci_get_rx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_get_tx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			u8 queue, struct dpseci_tx_queue_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_queue *cmd_params;
+ 	struct dpseci_rsp_get_tx_queue *rsp_params;
+ 	int err;
+@@ -601,7 +605,7 @@ int dpseci_get_tx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_get_sec_attr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			struct dpseci_sec_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_rsp_get_sec_attr *rsp_params;
+ 	int err;
+ 
+@@ -630,6 +634,8 @@ int dpseci_get_sec_attr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 	attr->arc4_acc_num = rsp_params->arc4_acc_num;
+ 	attr->des_acc_num = rsp_params->des_acc_num;
+ 	attr->aes_acc_num = rsp_params->aes_acc_num;
++	attr->ccha_acc_num = rsp_params->ccha_acc_num;
++	attr->ptha_acc_num = rsp_params->ptha_acc_num;
+ 
+ 	return 0;
+ }
+@@ -646,7 +652,7 @@ int dpseci_get_sec_attr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_get_sec_counters(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ 			    struct dpseci_sec_counters *counters)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_rsp_get_sec_counters *rsp_params;
+ 	int err;
+ 
+@@ -682,7 +688,7 @@ int dpseci_get_sec_counters(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+ int dpseci_get_api_version(struct fsl_mc_io *mc_io, u32 cmd_flags,
+ 			   u16 *major_ver, u16 *minor_ver)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_rsp_get_api_version *rsp_params;
+ 	int err;
+ 
+@@ -714,7 +720,7 @@ int dpseci_get_api_version(struct fsl_mc_io *mc_io, u32 cmd_flags,
+ int dpseci_set_opr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token, u8 index,
+ 		   u8 options, struct opr_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_opr *cmd_params;
+ 
+ 	cmd.header = mc_encode_cmd_header(
+@@ -747,7 +753,7 @@ int dpseci_set_opr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token, u8 index,
+ int dpseci_get_opr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token, u8 index,
+ 		   struct opr_cfg *cfg, struct opr_qry *qry)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_opr *cmd_params;
+ 	struct dpseci_rsp_get_opr *rsp_params;
+ 	int err;
+@@ -762,8 +768,8 @@ int dpseci_get_opr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token, u8 index,
+ 		return err;
+ 
+ 	rsp_params = (struct dpseci_rsp_get_opr *)cmd.params;
+-	qry->rip = dpseci_get_field(rsp_params->rip_enable, OPR_RIP);
+-	qry->enable = dpseci_get_field(rsp_params->rip_enable, OPR_ENABLE);
++	qry->rip = dpseci_get_field(rsp_params->flags, OPR_RIP);
++	qry->enable = dpseci_get_field(rsp_params->flags, OPR_ENABLE);
+ 	cfg->oloe = rsp_params->oloe;
+ 	cfg->oeane = rsp_params->oeane;
+ 	cfg->olws = rsp_params->olws;
+@@ -774,7 +780,7 @@ int dpseci_get_opr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token, u8 index,
+ 	qry->ea_tseq = le16_to_cpu(rsp_params->ea_tseq);
+ 	qry->tseq_nlis = dpseci_get_field(rsp_params->tseq_nlis, OPR_TSEQ_NLIS);
+ 	qry->ea_hseq = le16_to_cpu(rsp_params->ea_hseq);
+-	qry->hseq_nlis = dpseci_get_field(rsp_params->tseq_nlis, OPR_HSEQ_NLIS);
++	qry->hseq_nlis = dpseci_get_field(rsp_params->hseq_nlis, OPR_HSEQ_NLIS);
+ 	qry->ea_hptr = le16_to_cpu(rsp_params->ea_hptr);
+ 	qry->ea_tptr = le16_to_cpu(rsp_params->ea_tptr);
+ 	qry->opr_vid = le16_to_cpu(rsp_params->opr_vid);
+@@ -796,7 +802,7 @@ int dpseci_get_opr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token, u8 index,
+ int dpseci_set_congestion_notification(struct fsl_mc_io *mc_io, u32 cmd_flags,
+ 	u16 token, const struct dpseci_congestion_notification_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_congestion_notification *cmd_params;
+ 
+ 	cmd.header = mc_encode_cmd_header(
+@@ -831,7 +837,7 @@ int dpseci_set_congestion_notification(struct fsl_mc_io *mc_io, u32 cmd_flags,
+ int dpseci_get_congestion_notification(struct fsl_mc_io *mc_io, u32 cmd_flags,
+ 	u16 token, struct dpseci_congestion_notification_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpseci_cmd_congestion_notification *rsp_params;
+ 	int err;
+ 
+diff --git a/drivers/crypto/caam/dpseci.h b/drivers/crypto/caam/dpseci.h
+index d37489c6..3d0943b9 100644
+--- a/drivers/crypto/caam/dpseci.h
++++ b/drivers/crypto/caam/dpseci.h
+@@ -48,9 +48,9 @@ struct opr_qry;
+  */
+ 
+ /**
+- * Maximum number of Tx/Rx priorities per DPSECI object
++ * Maximum number of Tx/Rx queues per DPSECI object
+  */
+-#define DPSECI_PRIO_NUM		8
++#define DPSECI_MAX_QUEUE_NUM		16
+ 
+ /**
+  * All queues considered; see dpseci_set_rx_queue()
+@@ -94,7 +94,7 @@ struct dpseci_cfg {
+ 	u32 options;
+ 	u8 num_tx_queues;
+ 	u8 num_rx_queues;
+-	u8 priorities[DPSECI_PRIO_NUM];
++	u8 priorities[DPSECI_MAX_QUEUE_NUM];
+ };
+ 
+ int dpseci_create(struct fsl_mc_io *mc_io, u16 dprc_token, u32 cmd_flags,
+@@ -288,6 +288,10 @@ int dpseci_get_tx_queue(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+  *	this version of SEC
+  * @aes_acc_num: The number of copies of the AES module that are implemented in
+  *	this version of SEC
++ * @ccha_acc_num: The number of copies of the ChaCha20 module that are
++ *	implemented in this version of SEC.
++ * @ptha_acc_num: The number of copies of the Poly1305 module that are
++ *	implemented in this version of SEC.
+  **/
+ struct dpseci_sec_attr {
+ 	u16 ip_id;
+@@ -307,6 +311,8 @@ struct dpseci_sec_attr {
+ 	u8 arc4_acc_num;
+ 	u8 des_acc_num;
+ 	u8 aes_acc_num;
++	u8 ccha_acc_num;
++	u8 ptha_acc_num;
+ };
+ 
+ int dpseci_get_sec_attr(struct fsl_mc_io *mc_io, u32 cmd_flags, u16 token,
+@@ -355,11 +361,43 @@ enum dpseci_congestion_unit {
+ 	DPSECI_CONGESTION_UNIT_FRAMES
+ };
+ 
++/**
++ * CSCN message is written to message_iova once entering a
++ * congestion state (see 'threshold_entry')
++ */
+ #define DPSECI_CGN_MODE_WRITE_MEM_ON_ENTER		0x00000001
++
++/**
++ * CSCN message is written to message_iova once exiting a
++ * congestion state (see 'threshold_exit')
++ */
+ #define DPSECI_CGN_MODE_WRITE_MEM_ON_EXIT		0x00000002
++
++/**
++ * CSCN write will attempt to allocate into a cache (coherent write);
++ * valid only if 'DPSECI_CGN_MODE_WRITE_MEM_<X>' is selected
++ */
+ #define DPSECI_CGN_MODE_COHERENT_WRITE			0x00000004
++
++/**
++ * if 'dpseci_dest_cfg.dest_type != DPSECI_DEST_NONE' CSCN message is sent to
++ * DPIO/DPCON's WQ channel once entering a congestion state
++ * (see 'threshold_entry')
++ */
+ #define DPSECI_CGN_MODE_NOTIFY_DEST_ON_ENTER		0x00000008
++
++/**
++ * if 'dpseci_dest_cfg.dest_type != DPSECI_DEST_NONE' CSCN message is sent to
++ * DPIO/DPCON's WQ channel once exiting a congestion state
++ * (see 'threshold_exit')
++ */
+ #define DPSECI_CGN_MODE_NOTIFY_DEST_ON_EXIT		0x00000010
++
++/**
++ * if 'dpseci_dest_cfg.dest_type != DPSECI_DEST_NONE' when the CSCN is written
++ * to the sw-portal's DQRR, the DQRI interrupt is asserted immediately
++ * (if enabled)
++ */
+ #define DPSECI_CGN_MODE_INTR_COALESCING_DISABLED	0x00000020
+ 
+ /**
+diff --git a/drivers/crypto/caam/dpseci_cmd.h b/drivers/crypto/caam/dpseci_cmd.h
+index 7624315e..1008b130 100644
+--- a/drivers/crypto/caam/dpseci_cmd.h
++++ b/drivers/crypto/caam/dpseci_cmd.h
+@@ -37,42 +37,55 @@
+ 
+ /* DPSECI Version */
+ #define DPSECI_VER_MAJOR				5
+-#define DPSECI_VER_MINOR				1
++#define DPSECI_VER_MINOR				3
+ 
+ #define DPSECI_VER(maj, min)	(((maj) << 16) | (min))
+ #define DPSECI_VERSION		DPSECI_VER(DPSECI_VER_MAJOR, DPSECI_VER_MINOR)
+ 
+-/* Command IDs */
++/* Command versioning */
++#define DPSECI_CMD_BASE_VERSION		1
++#define DPSECI_CMD_BASE_VERSION_V2	2
++#define DPSECI_CMD_BASE_VERSION_V3	3
++#define DPSECI_CMD_ID_OFFSET		4
++
++#define DPSECI_CMD_V1(id)	(((id) << DPSECI_CMD_ID_OFFSET) | \
++				 DPSECI_CMD_BASE_VERSION)
++
++#define DPSECI_CMD_V2(id)	(((id) << DPSECI_CMD_ID_OFFSET) | \
++				 DPSECI_CMD_BASE_VERSION_V2)
+ 
+-#define DPSECI_CMDID_CLOSE                              0x8001
+-#define DPSECI_CMDID_OPEN                               0x8091
+-#define DPSECI_CMDID_CREATE                             0x9092
+-#define DPSECI_CMDID_DESTROY                            0x9891
+-#define DPSECI_CMDID_GET_API_VERSION                    0xa091
+-
+-#define DPSECI_CMDID_ENABLE                             0x0021
+-#define DPSECI_CMDID_DISABLE                            0x0031
+-#define DPSECI_CMDID_GET_ATTR                           0x0041
+-#define DPSECI_CMDID_RESET                              0x0051
+-#define DPSECI_CMDID_IS_ENABLED                         0x0061
+-
+-#define DPSECI_CMDID_SET_IRQ_ENABLE                     0x0121
+-#define DPSECI_CMDID_GET_IRQ_ENABLE                     0x0131
+-#define DPSECI_CMDID_SET_IRQ_MASK                       0x0141
+-#define DPSECI_CMDID_GET_IRQ_MASK                       0x0151
+-#define DPSECI_CMDID_GET_IRQ_STATUS                     0x0161
+-#define DPSECI_CMDID_CLEAR_IRQ_STATUS                   0x0171
+-
+-#define DPSECI_CMDID_SET_RX_QUEUE                       0x1941
+-#define DPSECI_CMDID_GET_RX_QUEUE                       0x1961
+-#define DPSECI_CMDID_GET_TX_QUEUE                       0x1971
+-#define DPSECI_CMDID_GET_SEC_ATTR                       0x1981
+-#define DPSECI_CMDID_GET_SEC_COUNTERS                   0x1991
+-#define DPSECI_CMDID_SET_OPR				0x19A1
+-#define DPSECI_CMDID_GET_OPR				0x19B1
+-
+-#define DPSECI_CMDID_SET_CONGESTION_NOTIFICATION	0x1701
+-#define DPSECI_CMDID_GET_CONGESTION_NOTIFICATION	0x1711
++#define DPSECI_CMD_V3(id)	(((id) << DPSECI_CMD_ID_OFFSET) | \
++				 DPSECI_CMD_BASE_VERSION_V3)
++
++/* Command IDs */
++#define DPSECI_CMDID_CLOSE				DPSECI_CMD_V1(0x800)
++#define DPSECI_CMDID_OPEN				DPSECI_CMD_V1(0x809)
++#define DPSECI_CMDID_CREATE				DPSECI_CMD_V3(0x909)
++#define DPSECI_CMDID_DESTROY				DPSECI_CMD_V1(0x989)
++#define DPSECI_CMDID_GET_API_VERSION			DPSECI_CMD_V1(0xa09)
++
++#define DPSECI_CMDID_ENABLE				DPSECI_CMD_V1(0x002)
++#define DPSECI_CMDID_DISABLE				DPSECI_CMD_V1(0x003)
++#define DPSECI_CMDID_GET_ATTR				DPSECI_CMD_V1(0x004)
++#define DPSECI_CMDID_RESET				DPSECI_CMD_V1(0x005)
++#define DPSECI_CMDID_IS_ENABLED				DPSECI_CMD_V1(0x006)
++
++#define DPSECI_CMDID_SET_IRQ_ENABLE			DPSECI_CMD_V1(0x012)
++#define DPSECI_CMDID_GET_IRQ_ENABLE			DPSECI_CMD_V1(0x013)
++#define DPSECI_CMDID_SET_IRQ_MASK			DPSECI_CMD_V1(0x014)
++#define DPSECI_CMDID_GET_IRQ_MASK			DPSECI_CMD_V1(0x015)
++#define DPSECI_CMDID_GET_IRQ_STATUS			DPSECI_CMD_V1(0x016)
++#define DPSECI_CMDID_CLEAR_IRQ_STATUS			DPSECI_CMD_V1(0x017)
++
++#define DPSECI_CMDID_SET_RX_QUEUE			DPSECI_CMD_V1(0x194)
++#define DPSECI_CMDID_GET_RX_QUEUE			DPSECI_CMD_V1(0x196)
++#define DPSECI_CMDID_GET_TX_QUEUE			DPSECI_CMD_V1(0x197)
++#define DPSECI_CMDID_GET_SEC_ATTR			DPSECI_CMD_V2(0x198)
++#define DPSECI_CMDID_GET_SEC_COUNTERS			DPSECI_CMD_V1(0x199)
++#define DPSECI_CMDID_SET_OPR				DPSECI_CMD_V1(0x19A)
++#define DPSECI_CMDID_GET_OPR				DPSECI_CMD_V1(0x19B)
++#define DPSECI_CMDID_SET_CONGESTION_NOTIFICATION	DPSECI_CMD_V1(0x170)
++#define DPSECI_CMDID_GET_CONGESTION_NOTIFICATION	DPSECI_CMD_V1(0x171)
+ 
+ /* Macros for accessing command fields smaller than 1 byte */
+ #define DPSECI_MASK(field)	\
+@@ -93,16 +106,21 @@ struct dpseci_cmd_create {
+ 	u8 priorities[8];
+ 	u8 num_tx_queues;
+ 	u8 num_rx_queues;
+-	__le16 pad;
++	u8 pad0[6];
+ 	__le32 options;
++	__le32 pad1;
++	u8 priorities2[8];
+ };
+ 
+ struct dpseci_cmd_destroy {
+ 	__le32 object_id;
+ };
+ 
++#define DPSECI_ENABLE_SHIFT	0
++#define DPSECI_ENABLE_SIZE	1
++
+ struct dpseci_rsp_is_enabled {
+-	__le32 is_enabled;
++	u8 is_enabled;
+ };
+ 
+ struct dpseci_cmd_irq_enable {
+@@ -134,6 +152,12 @@ struct dpseci_rsp_get_attributes {
+ 	__le32 options;
+ };
+ 
++#define DPSECI_DEST_TYPE_SHIFT	0
++#define DPSECI_DEST_TYPE_SIZE	4
++
++#define DPSECI_ORDER_PRESERVATION_SHIFT	0
++#define DPSECI_ORDER_PRESERVATION_SIZE	1
++
+ struct dpseci_cmd_queue {
+ 	__le32 dest_id;
+ 	u8 priority;
+@@ -145,7 +169,7 @@ struct dpseci_cmd_queue {
+ 		__le32 options;
+ 		__le32 fqid;
+ 	};
+-	__le32 order_preservation_en;
++	u8 order_preservation_en;
+ };
+ 
+ struct dpseci_rsp_get_tx_queue {
+@@ -176,6 +200,8 @@ struct dpseci_rsp_get_sec_attr {
+ 	u8 arc4_acc_num;
+ 	u8 des_acc_num;
+ 	u8 aes_acc_num;
++	u8 ccha_acc_num;
++	u8 ptha_acc_num;
+ };
+ 
+ struct dpseci_rsp_get_sec_counters {
+@@ -209,14 +235,14 @@ struct dpseci_cmd_opr {
+ #define DPSECI_OPR_RIP_SIZE		1
+ #define DPSECI_OPR_ENABLE_SHIFT		1
+ #define DPSECI_OPR_ENABLE_SIZE		1
+-#define DPSECI_OPR_TSEQ_NLIS_SHIFT	1
++#define DPSECI_OPR_TSEQ_NLIS_SHIFT	0
+ #define DPSECI_OPR_TSEQ_NLIS_SIZE	1
+-#define DPSECI_OPR_HSEQ_NLIS_SHIFT	1
++#define DPSECI_OPR_HSEQ_NLIS_SHIFT	0
+ #define DPSECI_OPR_HSEQ_NLIS_SIZE	1
+ 
+ struct dpseci_rsp_get_opr {
+ 	__le64 pad;
+-	u8 rip_enable;
++	u8 flags;
+ 	u8 pad0[2];
+ 	u8 oloe;
+ 	u8 oeane;
+diff --git a/drivers/crypto/caam/jr.c b/drivers/crypto/caam/jr.c
+index 00e87094..6da071a4 100644
+--- a/drivers/crypto/caam/jr.c
++++ b/drivers/crypto/caam/jr.c
+@@ -200,7 +200,8 @@ static void caam_jr_dequeue(unsigned long devarg)
+ 		BUG_ON(CIRC_CNT(head, tail + i, JOBR_DEPTH) <= 0);
+ 
+ 		/* Unmap just-run descriptor so we can post-process */
+-		dma_unmap_single(dev, jrp->outring[hw_idx].desc,
++		dma_unmap_single(dev,
++				 caam_dma_to_cpu(jrp->outring[hw_idx].desc),
+ 				 jrp->entinfo[sw_idx].desc_size,
+ 				 DMA_TO_DEVICE);
+ 
+diff --git a/drivers/crypto/caam/qi.c b/drivers/crypto/caam/qi.c
+index 3f73dce0..08829f24 100644
+--- a/drivers/crypto/caam/qi.c
++++ b/drivers/crypto/caam/qi.c
+@@ -586,7 +586,7 @@ static enum qman_cb_dqrr_result caam_rsp_fq_dqrr_cb(struct qman_portal *p,
+ 				fd->status);
+ 	}
+ 
+-	if (unlikely(fd->format != fd->format)) {
++	if (unlikely(fd->format != qm_fd_compound)) {
+ 		dev_err(qidev, "Non-compound FD from CAAM\n");
+ 		return qman_cb_dqrr_consume;
+ 	}
+diff --git a/drivers/dma/dpaa2-qdma/dpaa2-qdma.c b/drivers/dma/dpaa2-qdma/dpaa2-qdma.c
+index 83e82a85..c444ccf0 100644
+--- a/drivers/dma/dpaa2-qdma/dpaa2-qdma.c
++++ b/drivers/dma/dpaa2-qdma/dpaa2-qdma.c
+@@ -35,7 +35,7 @@
+ 
+ #include "../virt-dma.h"
+ 
+-#include "../../../drivers/staging/fsl-mc/include/mc.h"
++#include <linux/fsl/mc.h>
+ #include "../../../drivers/staging/fsl-mc/include/dpaa2-io.h"
+ #include "../../../drivers/staging/fsl-mc/include/dpaa2-fd.h"
+ #include "fsl_dpdmai_cmd.h"
+@@ -98,10 +98,10 @@ dpaa2_qdma_request_desc(struct dpaa2_qdma_chan *dpaa2_chan)
+ 		comp_temp->fl_bus_addr = comp_temp->fd_bus_addr +
+ 					sizeof(struct dpaa2_fd);
+ 		comp_temp->desc_virt_addr =
+-			(void *)((struct dpaa2_frame_list *)
++			(void *)((struct dpaa2_fl_entry *)
+ 				comp_temp->fl_virt_addr + 3);
+ 		comp_temp->desc_bus_addr = comp_temp->fl_bus_addr +
+-				sizeof(struct dpaa2_frame_list) * 3;
++				sizeof(struct dpaa2_fl_entry) * 3;
+ 
+ 		comp_temp->qchan = dpaa2_chan;
+ 		comp_temp->sg_blk_num = 0;
+@@ -128,19 +128,19 @@ static void dpaa2_qdma_populate_fd(uint32_t format,
+ 	memset(fd, 0, sizeof(struct dpaa2_fd));
+ 
+ 	/* fd populated */
+-	fd->simple.addr = dpaa2_comp->fl_bus_addr;
++	dpaa2_fd_set_addr(fd, dpaa2_comp->fl_bus_addr);
+ 	/* Bypass memory translation, Frame list format, short length disable */
+ 	/* we need to disable BMT if fsl-mc use iova addr */
+ 	if (smmu_disable)
+-		fd->simple.bpid = QMAN_FD_BMT_ENABLE;
+-	fd->simple.format_offset = QMAN_FD_FMT_ENABLE | QMAN_FD_SL_DISABLE;
++		dpaa2_fd_set_bpid(fd, QMAN_FD_BMT_ENABLE);
++	dpaa2_fd_set_format(fd, QMAN_FD_FMT_ENABLE | QMAN_FD_SL_DISABLE);
+ 
+-	fd->simple.frc = format | QDMA_SER_CTX;
++	dpaa2_fd_set_frc(fd, format | QDMA_SER_CTX);
+ }
+ 
+ /* first frame list for descriptor buffer */
+ static void dpaa2_qdma_populate_first_framel(
+-		struct dpaa2_frame_list *f_list,
++		struct dpaa2_fl_entry *f_list,
+ 		struct dpaa2_qdma_comp *dpaa2_comp)
+ {
+ 	struct dpaa2_qdma_sd_d *sdd;
+@@ -148,48 +148,45 @@ static void dpaa2_qdma_populate_first_framel(
+ 	sdd = (struct dpaa2_qdma_sd_d *)dpaa2_comp->desc_virt_addr;
+ 	memset(sdd, 0, 2 * (sizeof(*sdd)));
+ 	/* source and destination descriptor */
+-	sdd->cmd = QDMA_SD_CMD_RDTTYPE_COHERENT; /* source descriptor CMD */
++	sdd->cmd = cpu_to_le32(QDMA_SD_CMD_RDTTYPE_COHERENT); /* source descriptor CMD */
+ 	sdd++;
+-	sdd->cmd = QDMA_DD_CMD_WRTTYPE_COHERENT; /* dest descriptor CMD */
++	sdd->cmd = cpu_to_le32(QDMA_DD_CMD_WRTTYPE_COHERENT); /* dest descriptor CMD */
+ 
+-	memset(f_list, 0, sizeof(struct dpaa2_frame_list));
++	memset(f_list, 0, sizeof(struct dpaa2_fl_entry));
+ 	/* first frame list to source descriptor */
+-	f_list->addr_lo = dpaa2_comp->desc_bus_addr;
+-	f_list->addr_hi = (dpaa2_comp->desc_bus_addr >> 32);
+-	f_list->data_len.data_len_sl0 = 0x20; /* source/destination desc len */
+-	f_list->fmt = QDMA_FL_FMT_SBF; /* single buffer frame */
++
++	dpaa2_fl_set_addr(f_list, dpaa2_comp->desc_bus_addr);
++	dpaa2_fl_set_len(f_list, 0x20);
++	dpaa2_fl_set_format(f_list, QDMA_FL_FMT_SBF | QDMA_FL_SL_LONG);
++
+ 	if (smmu_disable)
+-		f_list->bmt = QDMA_FL_BMT_ENABLE; /* bypass memory translation */
+-	f_list->sl = QDMA_FL_SL_LONG; /* long length */
+-	f_list->f = 0; /* not the last frame list */
++		f_list->bpid = cpu_to_le16(QDMA_FL_BMT_ENABLE); /* bypass memory translation */
+ }
+ 
+ /* source and destination frame list */
+-static void dpaa2_qdma_populate_frames(struct dpaa2_frame_list *f_list,
++static void dpaa2_qdma_populate_frames(struct dpaa2_fl_entry *f_list,
+ 		dma_addr_t dst, dma_addr_t src, size_t len, uint8_t fmt)
+ {
+ 	/* source frame list to source buffer */
+-	memset(f_list, 0, sizeof(struct dpaa2_frame_list));
+-	f_list->addr_lo = src;
+-	f_list->addr_hi = (src >> 32);
+-	f_list->data_len.data_len_sl0 = len;
+-	f_list->fmt = fmt; /* single buffer frame or scatter gather frame */
++	memset(f_list, 0, sizeof(struct dpaa2_fl_entry));
++
++
++	dpaa2_fl_set_addr(f_list, src);
++	dpaa2_fl_set_len(f_list, len);
++	dpaa2_fl_set_format(f_list, (fmt | QDMA_FL_SL_LONG)); /* single buffer frame or scatter gather frame */
+ 	if (smmu_disable)
+-		f_list->bmt = QDMA_FL_BMT_ENABLE; /* bypass memory translation */
+-	f_list->sl = QDMA_FL_SL_LONG; /* long length */
+-	f_list->f = 0; /* not the last frame list */
++		f_list->bpid = cpu_to_le16(QDMA_FL_BMT_ENABLE); /* bypass memory translation */
+ 
+ 	f_list++;
+ 	/* destination frame list to destination buffer */
+-	memset(f_list, 0, sizeof(struct dpaa2_frame_list));
+-	f_list->addr_lo = dst;
+-	f_list->addr_hi = (dst >> 32);
+-	f_list->data_len.data_len_sl0 = len;
+-	f_list->fmt = fmt; /* single buffer frame or scatter gather frame */
++	memset(f_list, 0, sizeof(struct dpaa2_fl_entry));
++
++	dpaa2_fl_set_addr(f_list, dst);
++	dpaa2_fl_set_len(f_list, len);
++	dpaa2_fl_set_format(f_list, (fmt | QDMA_FL_SL_LONG));
++	dpaa2_fl_set_final(f_list, QDMA_FL_F); /* single buffer frame or scatter gather frame */
+ 	if (smmu_disable)
+-		f_list->bmt = QDMA_FL_BMT_ENABLE; /* bypass memory translation */
+-	f_list->sl = QDMA_FL_SL_LONG; /* long length */
+-	f_list->f = QDMA_FL_F; /* Final bit: 1, for last frame list */
++		f_list->bpid = cpu_to_le16(QDMA_FL_BMT_ENABLE); /* bypass memory translation */
+ }
+ 
+ static struct dma_async_tx_descriptor *dpaa2_qdma_prep_memcpy(
+@@ -198,7 +195,7 @@ static struct dma_async_tx_descriptor *dpaa2_qdma_prep_memcpy(
+ {
+ 	struct dpaa2_qdma_chan *dpaa2_chan = to_dpaa2_qdma_chan(chan);
+ 	struct dpaa2_qdma_comp *dpaa2_comp;
+-	struct dpaa2_frame_list *f_list;
++	struct dpaa2_fl_entry *f_list;
+ 	uint32_t format;
+ 
+ 	dpaa2_comp = dpaa2_qdma_request_desc(dpaa2_chan);
+@@ -211,7 +208,7 @@ static struct dma_async_tx_descriptor *dpaa2_qdma_prep_memcpy(
+ 	/* populate Frame descriptor */
+ 	dpaa2_qdma_populate_fd(format, dpaa2_comp);
+ 
+-	f_list = (struct dpaa2_frame_list *)dpaa2_comp->fl_virt_addr;
++	f_list = (struct dpaa2_fl_entry *)dpaa2_comp->fl_virt_addr;
+ 
+ #ifdef LONG_FORMAT
+ 	/* first frame list for descriptor buffer (logn format) */
+@@ -533,7 +530,8 @@ static void dpaa2_qdma_fqdan_cb(struct dpaa2_io_notification_ctx *ctx)
+ 
+ 		/* obtain FD and process the error */
+ 		fd = dpaa2_dq_fd(dq);
+-		status = fd->simple.ctrl & 0xff;
++
++		status = dpaa2_fd_get_ctrl(fd) & 0xff;
+ 		if (status)
+ 			dev_err(priv->dev, "FD error occurred\n");
+ 		found = 0;
+@@ -549,8 +547,8 @@ static void dpaa2_qdma_fqdan_cb(struct dpaa2_io_notification_ctx *ctx)
+ 				fd_eq = (struct dpaa2_fd *)
+ 					dpaa2_comp->fd_virt_addr;
+ 
+-				if (fd_eq->simple.addr ==
+-					fd->simple.addr) {
++				if (le64_to_cpu(fd_eq->simple.addr) ==
++						le64_to_cpu(fd->simple.addr)) {
+ 
+ 					list_del(&dpaa2_comp->list);
+ 					list_add_tail(&dpaa2_comp->list,
+diff --git a/drivers/dma/dpaa2-qdma/dpaa2-qdma.h b/drivers/dma/dpaa2-qdma/dpaa2-qdma.h
+index f0f238d4..245ac7c4 100644
+--- a/drivers/dma/dpaa2-qdma/dpaa2-qdma.h
++++ b/drivers/dma/dpaa2-qdma/dpaa2-qdma.h
+@@ -99,7 +99,7 @@ struct dpaa2_qdma_sg {
+ 	} ctrl;
+ } __attribute__((__packed__));
+ 
+-#define QMAN_FD_FMT_ENABLE (1 << 12) /* frame list table enable */
++#define QMAN_FD_FMT_ENABLE (1) /* frame list table enable */
+ #define QMAN_FD_BMT_ENABLE (1 << 15) /* bypass memory translation */
+ #define QMAN_FD_BMT_DISABLE (0 << 15) /* bypass memory translation */
+ #define QMAN_FD_SL_DISABLE (0 << 14) /* short lengthe disabled */
+@@ -124,49 +124,14 @@ struct dpaa2_qdma_sg {
+ #define QMAN_FD_CBMT_DISABLE (0 << 15) /* Flow Context: 64bit virtual address */
+ #define QMAN_FD_SC_DISABLE (0 << 27) /* stashing control */
+ 
+-#define QDMA_FL_FMT_SBF 0x0	/* Single buffer frame */
++#define QDMA_FL_FMT_SBF (0x0) /* Single buffer frame */
+ #define QDMA_FL_FMT_SGE 0x2 /* Scatter gather frame */
+-#define QDMA_FL_BMT_ENABLE 0x1 /* enable bypass memory translation */
++#define QDMA_FL_BMT_ENABLE (0x1 << 15)/* enable bypass memory translation */
+ #define QDMA_FL_BMT_DISABLE 0x0 /* enable bypass memory translation */
+-#define QDMA_FL_SL_LONG 0x0 /* long length */
++#define QDMA_FL_SL_LONG (0x0 << 2)/* long length */
+ #define QDMA_FL_SL_SHORT 0x1 /* short length */
+-#define QDMA_FL_F 0x1 /* last frame list bit */
++#define QDMA_FL_F (0x1)/* last frame list bit */
+ /*Description of Frame list table structure*/
+-struct dpaa2_frame_list  {
+-	uint32_t addr_lo;	/* lower 32 bits of address */
+-	uint32_t addr_hi:17; /* upper 17 bits of address */
+-	uint32_t resrvd:15;
+-	union {
+-		uint32_t data_len_sl0; /* If SL=0, then data length is 32 */
+-		struct {
+-			uint32_t data_len:18; /* IF SL=1; length is 18bit */
+-			uint32_t resrvd:2;
+-			uint32_t mem:12; /* Valid only when SL=1 */
+-		} data_len_sl1;
+-	} data_len;
+-	/* word 4 */
+-	uint32_t bpid:14; /* Frame buffer pool ID */
+-	uint32_t ivp:1; /* Invalid Pool ID. */
+-	uint32_t bmt:1; /* Bypass Memory Translation */
+-	uint32_t offset:12; /* Frame offset */
+-	uint32_t fmt:2; /* Frame Format */
+-	uint32_t sl:1; /* Short Length */
+-	uint32_t f:1; /* Final bit */
+-
+-	uint32_t frc; /* Frame Context */
+-	/* word 6 */
+-	uint32_t err:8; /* Frame errors */
+-	uint32_t resrvd0:8;
+-	uint32_t asal:4; /* accelerator-specific annotation length */
+-	uint32_t resrvd1:1;
+-	uint32_t ptv2:1;
+-	uint32_t ptv1:1;
+-	uint32_t pta:1; /* pass-through annotation */
+-	uint32_t resrvd2:8;
+-
+-	uint32_t flc_lo; /* lower 32 bits fo flow context */
+-	uint32_t flc_hi; /* higher 32 bits fo flow context */
+-} __attribute__((__packed__));
+ 
+ struct dpaa2_qdma_chan {
+ 	struct virt_dma_chan		vchan;
+@@ -253,7 +218,7 @@ struct dpaa2_qdma_priv_per_prio {
+ 
+ /* FD pool size: one FD + 3 Frame list + 2 source/destination descriptor */
+ #define FD_POOL_SIZE (sizeof(struct dpaa2_fd) + \
+-		sizeof(struct dpaa2_frame_list) * 3 + \
++		sizeof(struct dpaa2_fl_entry) * 3 + \
+ 		sizeof(struct dpaa2_qdma_sd_d) * 2)
+ 
+ /* qdma_sg_blk + 16 SGs */
+diff --git a/drivers/dma/dpaa2-qdma/dpdmai.c b/drivers/dma/dpaa2-qdma/dpdmai.c
+index 7bb1436a..815b7a24 100644
+--- a/drivers/dma/dpaa2-qdma/dpdmai.c
++++ b/drivers/dma/dpaa2-qdma/dpdmai.c
+@@ -33,22 +33,56 @@
+ #include <linux/io.h>
+ #include "fsl_dpdmai.h"
+ #include "fsl_dpdmai_cmd.h"
+-#include "../../../drivers/staging/fsl-mc/include/mc-sys.h"
+-#include "../../../drivers/staging/fsl-mc/include/mc-cmd.h"
++#include <linux/fsl/mc.h>
++
++struct dpdmai_cmd_open {
++	__le32 dpdmai_id;
++};
++
++struct dpdmai_rsp_get_attributes {
++	__le32 id;
++	u8 num_of_priorities;
++	u8 pad0[3];
++	__le16 major;
++	__le16 minor;
++};
++
++
++struct dpdmai_cmd_queue {
++	__le32 dest_id;
++	u8 priority;
++	u8 queue;
++	u8 dest_type;
++	u8 pad;
++	__le64 user_ctx;
++	union {
++		__le32 options;
++		__le32 fqid;
++	};
++};
++
++struct dpdmai_rsp_get_tx_queue {
++	__le64 pad;
++	__le32 fqid;
++};
++
+ 
+ int dpdmai_open(struct fsl_mc_io *mc_io,
+ 		uint32_t cmd_flags,
+ 		int dpdmai_id,
+ 		uint16_t *token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
++	struct dpdmai_cmd_open *cmd_params;
+ 	int err;
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_OPEN,
+ 					  cmd_flags,
+ 					  0);
+-	DPDMAI_CMD_OPEN(cmd, dpdmai_id);
++
++	cmd_params = (struct dpdmai_cmd_open *)cmd.params;
++	cmd_params->dpdmai_id = cpu_to_le32(dpdmai_id);
+ 
+ 	/* send command to mc*/
+ 	err = mc_send_command(mc_io, &cmd);
+@@ -56,8 +90,7 @@ int dpdmai_open(struct fsl_mc_io *mc_io,
+ 		return err;
+ 
+ 	/* retrieve response parameters */
+-	*token = MC_CMD_HDR_READ_TOKEN(cmd.header);
+-
++	*token = mc_cmd_hdr_read_token(&cmd);
+ 	return 0;
+ }
+ 
+@@ -65,7 +98,7 @@ int dpdmai_close(struct fsl_mc_io *mc_io,
+ 		 uint32_t cmd_flags,
+ 		 uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_CLOSE,
+@@ -80,7 +113,7 @@ int dpdmai_create(struct fsl_mc_io *mc_io,
+ 		  const struct dpdmai_cfg *cfg,
+ 		  uint16_t *token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -104,7 +137,7 @@ int dpdmai_destroy(struct fsl_mc_io *mc_io,
+ 		   uint32_t cmd_flags,
+ 		   uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_DESTROY,
+@@ -119,7 +152,7 @@ int dpdmai_enable(struct fsl_mc_io *mc_io,
+ 		  uint32_t cmd_flags,
+ 		  uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_ENABLE,
+@@ -134,7 +167,7 @@ int dpdmai_disable(struct fsl_mc_io *mc_io,
+ 		   uint32_t cmd_flags,
+ 		   uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_DISABLE,
+@@ -150,7 +183,7 @@ int dpdmai_is_enabled(struct fsl_mc_io *mc_io,
+ 		      uint16_t token,
+ 		      int *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_IS_ENABLED,
+@@ -172,7 +205,7 @@ int dpdmai_reset(struct fsl_mc_io *mc_io,
+ 		 uint32_t cmd_flags,
+ 		 uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_RESET,
+@@ -190,7 +223,7 @@ int dpdmai_get_irq(struct fsl_mc_io *mc_io,
+ 		   int *type,
+ 		   struct dpdmai_irq_cfg	*irq_cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -216,7 +249,7 @@ int dpdmai_set_irq(struct fsl_mc_io *mc_io,
+ 		   uint8_t irq_index,
+ 		   struct dpdmai_irq_cfg *irq_cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_SET_IRQ,
+@@ -234,7 +267,7 @@ int dpdmai_get_irq_enable(struct fsl_mc_io *mc_io,
+ 			  uint8_t irq_index,
+ 			  uint8_t *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -260,7 +293,7 @@ int dpdmai_set_irq_enable(struct fsl_mc_io *mc_io,
+ 			  uint8_t irq_index,
+ 			  uint8_t en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_SET_IRQ_ENABLE,
+@@ -278,7 +311,7 @@ int dpdmai_get_irq_mask(struct fsl_mc_io *mc_io,
+ 			uint8_t irq_index,
+ 			uint32_t *mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -304,7 +337,7 @@ int dpdmai_set_irq_mask(struct fsl_mc_io *mc_io,
+ 			uint8_t irq_index,
+ 			uint32_t mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_SET_IRQ_MASK,
+@@ -322,7 +355,7 @@ int dpdmai_get_irq_status(struct fsl_mc_io *mc_io,
+ 			  uint8_t irq_index,
+ 			  uint32_t *status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -348,7 +381,7 @@ int dpdmai_clear_irq_status(struct fsl_mc_io *mc_io,
+ 			    uint8_t irq_index,
+ 			    uint32_t status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_CLEAR_IRQ_STATUS,
+@@ -365,8 +398,9 @@ int dpdmai_get_attributes(struct fsl_mc_io *mc_io,
+ 			  uint16_t token,
+ 			  struct dpdmai_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
++	struct dpdmai_rsp_get_attributes *rsp_params;
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_GET_ATTR,
+@@ -379,7 +413,12 @@ int dpdmai_get_attributes(struct fsl_mc_io *mc_io,
+ 		return err;
+ 
+ 	/* retrieve response parameters */
+-	DPDMAI_RSP_GET_ATTR(cmd, attr);
++	rsp_params = (struct dpdmai_rsp_get_attributes *)cmd.params;
++	attr->id = le32_to_cpu(rsp_params->id);
++	attr->version.major = le16_to_cpu(rsp_params->major);
++	attr->version.minor = le16_to_cpu(rsp_params->minor);
++	attr->num_of_priorities = rsp_params->num_of_priorities;
++
+ 
+ 	return 0;
+ }
+@@ -390,13 +429,22 @@ int dpdmai_set_rx_queue(struct fsl_mc_io *mc_io,
+ 			uint8_t priority,
+ 			const struct dpdmai_rx_queue_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
++	struct dpdmai_cmd_queue *cmd_params;
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_SET_RX_QUEUE,
+ 					  cmd_flags,
+ 					  token);
+-	DPDMAI_CMD_SET_RX_QUEUE(cmd, priority, cfg);
++
++	cmd_params = (struct dpdmai_cmd_queue *)cmd.params;
++	cmd_params->dest_id = cpu_to_le32(cfg->dest_cfg.dest_id);
++	cmd_params->priority = cfg->dest_cfg.priority;
++	cmd_params->queue = priority;
++	cmd_params->dest_type = cfg->dest_cfg.dest_type;
++	cmd_params->user_ctx = cpu_to_le64(cfg->user_ctx);
++	cmd_params->options = cpu_to_le32(cfg->options);
++
+ 
+ 	/* send command to mc*/
+ 	return mc_send_command(mc_io, &cmd);
+@@ -407,14 +455,17 @@ int dpdmai_get_rx_queue(struct fsl_mc_io *mc_io,
+ 			uint16_t token,
+ 			uint8_t priority, struct dpdmai_rx_queue_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
++	struct dpdmai_cmd_queue *cmd_params;
+ 	int err;
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_GET_RX_QUEUE,
+ 					  cmd_flags,
+ 					  token);
+-	DPDMAI_CMD_GET_RX_QUEUE(cmd, priority);
++
++	cmd_params = (struct dpdmai_cmd_queue *)cmd.params;
++	cmd_params->queue = priority;
+ 
+ 	/* send command to mc*/
+ 	err = mc_send_command(mc_io, &cmd);
+@@ -422,7 +473,11 @@ int dpdmai_get_rx_queue(struct fsl_mc_io *mc_io,
+ 		return err;
+ 
+ 	/* retrieve response parameters */
+-	DPDMAI_RSP_GET_RX_QUEUE(cmd, attr);
++	attr->dest_cfg.dest_id = le32_to_cpu(cmd_params->dest_id);
++	attr->dest_cfg.priority = cmd_params->priority;
++	attr->dest_cfg.dest_type = cmd_params->dest_type;
++	attr->user_ctx = le64_to_cpu(cmd_params->user_ctx);
++	attr->fqid = le32_to_cpu(cmd_params->fqid);
+ 
+ 	return 0;
+ }
+@@ -433,14 +488,18 @@ int dpdmai_get_tx_queue(struct fsl_mc_io *mc_io,
+ 			uint8_t priority,
+ 			struct dpdmai_tx_queue_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
++	struct dpdmai_cmd_queue *cmd_params;
++	struct dpdmai_rsp_get_tx_queue *rsp_params;
+ 	int err;
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMAI_CMDID_GET_TX_QUEUE,
+ 					  cmd_flags,
+ 					  token);
+-	DPDMAI_CMD_GET_TX_QUEUE(cmd, priority);
++
++	cmd_params = (struct dpdmai_cmd_queue *)cmd.params;
++	cmd_params->queue = priority;
+ 
+ 	/* send command to mc*/
+ 	err = mc_send_command(mc_io, &cmd);
+@@ -448,7 +507,9 @@ int dpdmai_get_tx_queue(struct fsl_mc_io *mc_io,
+ 		return err;
+ 
+ 	/* retrieve response parameters */
+-	DPDMAI_RSP_GET_TX_QUEUE(cmd, attr);
++
++	rsp_params = (struct dpdmai_rsp_get_tx_queue *)cmd.params;
++	attr->fqid = le32_to_cpu(rsp_params->fqid);
+ 
+ 	return 0;
+ }
+diff --git a/drivers/iommu/arm-smmu.c b/drivers/iommu/arm-smmu.c
+index 382a1d3a..342319a1 100644
+--- a/drivers/iommu/arm-smmu.c
++++ b/drivers/iommu/arm-smmu.c
+@@ -49,7 +49,7 @@
+ #include <linux/spinlock.h>
+ 
+ #include <linux/amba/bus.h>
+-#include "../staging/fsl-mc/include/mc-bus.h"
++#include <linux/fsl/mc.h>
+ 
+ #include "io-pgtable.h"
+ 
+diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
+index e6a8c225..1857cc01 100644
+--- a/drivers/iommu/iommu.c
++++ b/drivers/iommu/iommu.c
+@@ -33,6 +33,7 @@
+ #include <linux/bitops.h>
+ #include <linux/property.h>
+ #include <trace/events/iommu.h>
++#include <linux/fsl/mc.h>
+ 
+ static struct kset *iommu_group_kset;
+ static DEFINE_IDA(iommu_group_ida);
+@@ -976,6 +977,26 @@ struct iommu_group *pci_device_group(struct device *dev)
+ 	return group;
+ }
+ 
++/* Get the IOMMU group for device on fsl-mc bus */
++struct iommu_group *fsl_mc_device_group(struct device *dev)
++{
++	struct device *cont_dev = fsl_mc_cont_dev(dev);
++	struct iommu_group *group;
++
++	/* Container device is responsible for creating the iommu group */
++	if (fsl_mc_is_cont_dev(dev)) {
++		group = iommu_group_alloc();
++		if (IS_ERR(group))
++			return NULL;
++	} else {
++		get_device(cont_dev);
++		group = iommu_group_get(cont_dev);
++		put_device(cont_dev);
++	}
++
++	return group;
++}
++
+ /**
+  * iommu_group_get_for_dev - Find or create the IOMMU group for a device
+  * @dev: target device
+diff --git a/drivers/mmc/host/sdhci-of-esdhc.c b/drivers/mmc/host/sdhci-of-esdhc.c
+index ae3fee6a..af121017 100644
+--- a/drivers/mmc/host/sdhci-of-esdhc.c
++++ b/drivers/mmc/host/sdhci-of-esdhc.c
+@@ -29,11 +29,56 @@
+ #define VENDOR_V_22	0x12
+ #define VENDOR_V_23	0x13
+ 
++#define MMC_TIMING_NUM (MMC_TIMING_MMC_HS400 + 1)
++
++struct esdhc_clk_fixup {
++	const unsigned int sd_dflt_max_clk;
++	const unsigned int max_clk[MMC_TIMING_NUM];
++};
++
++static const struct esdhc_clk_fixup ls1021a_esdhc_clk = {
++	.sd_dflt_max_clk = 25000000,
++	.max_clk[MMC_TIMING_MMC_HS] = 46500000,
++	.max_clk[MMC_TIMING_SD_HS] = 46500000,
++};
++
++static const struct esdhc_clk_fixup ls1046a_esdhc_clk = {
++	.sd_dflt_max_clk = 25000000,
++	.max_clk[MMC_TIMING_UHS_SDR104] = 167000000,
++	.max_clk[MMC_TIMING_MMC_HS200] = 167000000,
++};
++
++static const struct esdhc_clk_fixup ls1012a_esdhc_clk = {
++	.sd_dflt_max_clk = 25000000,
++	.max_clk[MMC_TIMING_UHS_SDR104] = 125000000,
++	.max_clk[MMC_TIMING_MMC_HS200] = 125000000,
++};
++
++static const struct esdhc_clk_fixup p1010_esdhc_clk = {
++	.sd_dflt_max_clk = 20000000,
++	.max_clk[MMC_TIMING_LEGACY] = 20000000,
++	.max_clk[MMC_TIMING_MMC_HS] = 42000000,
++	.max_clk[MMC_TIMING_SD_HS] = 40000000,
++};
++
++static const struct of_device_id sdhci_esdhc_of_match[] = {
++	{ .compatible = "fsl,ls1021a-esdhc", .data = &ls1021a_esdhc_clk},
++	{ .compatible = "fsl,ls1046a-esdhc", .data = &ls1046a_esdhc_clk},
++	{ .compatible = "fsl,ls1012a-esdhc", .data = &ls1012a_esdhc_clk},
++	{ .compatible = "fsl,p1010-esdhc",   .data = &p1010_esdhc_clk},
++	{ .compatible = "fsl,mpc8379-esdhc" },
++	{ .compatible = "fsl,mpc8536-esdhc" },
++	{ .compatible = "fsl,esdhc" },
++	{ }
++};
++MODULE_DEVICE_TABLE(of, sdhci_esdhc_of_match);
++
+ struct sdhci_esdhc {
+ 	u8 vendor_ver;
+ 	u8 spec_ver;
+ 	bool quirk_incorrect_hostver;
+ 	unsigned int peripheral_clock;
++	const struct esdhc_clk_fixup *clk_fixup;
+ };
+ 
+ static void esdhc_clock_enable(struct sdhci_host *host, bool enable);
+@@ -467,6 +512,7 @@ static void esdhc_of_set_clock(struct sdhci_host *host, unsigned int clock)
+ 	int pre_div = 1;
+ 	int div = 1;
+ 	ktime_t timeout;
++	long fixup = 0;
+ 	u32 temp;
+ 
+ 	host->mmc->actual_clock = 0;
+@@ -480,27 +526,14 @@ static void esdhc_of_set_clock(struct sdhci_host *host, unsigned int clock)
+ 	if (esdhc->vendor_ver < VENDOR_V_23)
+ 		pre_div = 2;
+ 
+-	/*
+-	 * Limit SD clock to 167MHz for ls1046a according to its datasheet
+-	 */
+-	if (clock > 167000000 &&
+-	    of_find_compatible_node(NULL, NULL, "fsl,ls1046a-esdhc"))
+-		clock = 167000000;
++	if (host->mmc->card && mmc_card_sd(host->mmc->card) &&
++		esdhc->clk_fixup && host->mmc->ios.timing == MMC_TIMING_LEGACY)
++		fixup = esdhc->clk_fixup->sd_dflt_max_clk;
++	else if (esdhc->clk_fixup)
++		fixup = esdhc->clk_fixup->max_clk[host->mmc->ios.timing];
+ 
+-	/*
+-	 * Limit SD clock to 125MHz for ls1012a according to its datasheet
+-	 */
+-	if (clock > 125000000 &&
+-	    of_find_compatible_node(NULL, NULL, "fsl,ls1012a-esdhc"))
+-		clock = 125000000;
+-
+-	/* Workaround to reduce the clock frequency for p1010 esdhc */
+-	if (of_find_compatible_node(NULL, NULL, "fsl,p1010-esdhc")) {
+-		if (clock > 20000000)
+-			clock -= 5000000;
+-		if (clock > 40000000)
+-			clock -= 5000000;
+-	}
++	if (fixup && clock > fixup)
++		clock = fixup;
+ 
+ 	/* Workaround to reduce the clock frequency for ls1021a esdhc */
+ 	if (of_find_compatible_node(NULL, NULL, "fsl,ls1021a-esdhc")) {
+@@ -788,6 +821,7 @@ static struct soc_device_attribute soc_incorrect_hostver[] = {
+ 
+ static void esdhc_init(struct platform_device *pdev, struct sdhci_host *host)
+ {
++	const struct of_device_id *match;
+ 	struct sdhci_pltfm_host *pltfm_host;
+ 	struct sdhci_esdhc *esdhc;
+ 	struct device_node *np;
+@@ -807,6 +841,9 @@ static void esdhc_init(struct platform_device *pdev, struct sdhci_host *host)
+ 	else
+ 		esdhc->quirk_incorrect_hostver = false;
+ 
++	match = of_match_node(sdhci_esdhc_of_match, pdev->dev.of_node);
++	if (match)
++		esdhc->clk_fixup = match->data;
+ 	np = pdev->dev.of_node;
+ 	clk = of_clk_get(np, 0);
+ 	if (!IS_ERR(clk)) {
+@@ -906,14 +943,6 @@ static int sdhci_esdhc_probe(struct platform_device *pdev)
+ 	return ret;
+ }
+ 
+-static const struct of_device_id sdhci_esdhc_of_match[] = {
+-	{ .compatible = "fsl,mpc8379-esdhc" },
+-	{ .compatible = "fsl,mpc8536-esdhc" },
+-	{ .compatible = "fsl,esdhc" },
+-	{ }
+-};
+-MODULE_DEVICE_TABLE(of, sdhci_esdhc_of_match);
+-
+ static struct platform_driver sdhci_esdhc_driver = {
+ 	.driver = {
+ 		.name = "sdhci-esdhc",
+diff --git a/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
+index 627a9612..99dd02d8 100644
+--- a/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
++++ b/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
+@@ -132,6 +132,9 @@ bool dpaa_errata_a010022;
+ /* Default alignment for start of data in an Rx FD */
+ #define DPAA_FD_DATA_ALIGNMENT  16
+ 
++/* The DPAA requires 256 bytes reserved and mapped for the SGT */
++#define DPAA_SGT_SIZE 256
++
+ /* Values for the L3R field of the FM Parse Results
+  */
+ /* L3 Type field: First IP Present IPv4 */
+@@ -165,7 +168,7 @@ bool dpaa_errata_a010022;
+ #define DPAA_RX_PRIV_DATA_SIZE	(u16)(DPAA_TX_PRIV_DATA_SIZE + \
+ 					dpaa_rx_extra_headroom)
+ 
+-#define DPAA_ETH_RX_QUEUES	128
++#define DPAA_ETH_PCD_RXQ_NUM	128
+ 
+ #define DPAA_ENQUEUE_RETRIES	100000
+ 
+@@ -176,6 +179,7 @@ struct fm_port_fqs {
+ 	struct dpaa_fq *tx_errq;
+ 	struct dpaa_fq *rx_defq;
+ 	struct dpaa_fq *rx_errq;
++	struct dpaa_fq *rx_pcdq;
+ };
+ 
+ /* All the dpa bps in use at any moment */
+@@ -242,7 +246,7 @@ static int dpaa_netdev_init(struct net_device *net_dev,
+ 	net_dev->max_mtu = dpaa_get_max_mtu();
+ 
+ 	net_dev->hw_features |= (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+-				 NETIF_F_LLTX);
++				 NETIF_F_LLTX | NETIF_F_RXHASH);
+ 
+ 	net_dev->hw_features |= NETIF_F_SG | NETIF_F_HIGHDMA;
+ 	/* The kernels enables GSO automatically, if we declare NETIF_F_SG.
+@@ -386,34 +390,19 @@ static int dpaa_setup_tc(struct net_device *net_dev, u32 handle, __be16 proto,
+ 
+ static struct mac_device *dpaa_mac_dev_get(struct platform_device *pdev)
+ {
+-	struct platform_device *of_dev;
+ 	struct dpaa_eth_data *eth_data;
+-	struct device *dpaa_dev, *dev;
+-	struct device_node *mac_node;
++	struct device *dpaa_dev;
+ 	struct mac_device *mac_dev;
+ 
+ 	dpaa_dev = &pdev->dev;
+ 	eth_data = dpaa_dev->platform_data;
+-	if (!eth_data)
++	if (!eth_data) {
++		dev_err(dpaa_dev, "eth_data missing\n");
+ 		return ERR_PTR(-ENODEV);
+-
+-	mac_node = eth_data->mac_node;
+-
+-	of_dev = of_find_device_by_node(mac_node);
+-	if (!of_dev) {
+-		dev_err(dpaa_dev, "of_find_device_by_node(%s) failed\n",
+-			mac_node->full_name);
+-		of_node_put(mac_node);
+-		return ERR_PTR(-EINVAL);
+ 	}
+-	of_node_put(mac_node);
+-
+-	dev = &of_dev->dev;
+-
+-	mac_dev = dev_get_drvdata(dev);
++	mac_dev = eth_data->mac_dev;
+ 	if (!mac_dev) {
+-		dev_err(dpaa_dev, "dev_get_drvdata(%s) failed\n",
+-			dev_name(dev));
++		dev_err(dpaa_dev, "mac_dev missing\n");
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+@@ -470,6 +459,16 @@ static void dpaa_set_rx_mode(struct net_device *net_dev)
+ 				  err);
+ 	}
+ 
++	if (!!(net_dev->flags & IFF_ALLMULTI) != priv->mac_dev->allmulti) {
++		priv->mac_dev->allmulti = !priv->mac_dev->allmulti;
++		err = priv->mac_dev->set_allmulti(priv->mac_dev->fman_mac,
++						  priv->mac_dev->allmulti);
++		if (err < 0)
++			netif_err(priv, drv, net_dev,
++				  "mac_dev->set_allmulti() = %d\n",
++				  err);
++	}
++
+ 	err = priv->mac_dev->set_multi(net_dev, priv->mac_dev);
+ 	if (err < 0)
+ 		netif_err(priv, drv, net_dev, "mac_dev->set_multi() = %d\n",
+@@ -630,6 +629,7 @@ static inline void dpaa_assign_wq(struct dpaa_fq *fq, int idx)
+ 		fq->wq = 5;
+ 		break;
+ 	case FQ_TYPE_RX_DEFAULT:
++	case FQ_TYPE_RX_PCD:
+ 		fq->wq = 6;
+ 		break;
+ 	case FQ_TYPE_TX:
+@@ -690,6 +690,7 @@ static int dpaa_alloc_all_fqs(struct device *dev, struct list_head *list,
+ 			      struct fm_port_fqs *port_fqs)
+ {
+ 	struct dpaa_fq *dpaa_fq;
++	u32 fq_base, fq_base_aligned, i;
+ 
+ 	dpaa_fq = dpaa_fq_alloc(dev, 0, 1, list, FQ_TYPE_RX_ERROR);
+ 	if (!dpaa_fq)
+@@ -703,6 +704,26 @@ static int dpaa_alloc_all_fqs(struct device *dev, struct list_head *list,
+ 
+ 	port_fqs->rx_defq = &dpaa_fq[0];
+ 
++	/* the PCD FQIDs range needs to be aligned for correct operation */
++	if (qman_alloc_fqid_range(&fq_base, 2 * DPAA_ETH_PCD_RXQ_NUM))
++		goto fq_alloc_failed;
++
++	fq_base_aligned = ALIGN(fq_base, DPAA_ETH_PCD_RXQ_NUM);
++
++	for (i = fq_base; i < fq_base_aligned; i++)
++		qman_release_fqid(i);
++
++	for (i = fq_base_aligned + DPAA_ETH_PCD_RXQ_NUM;
++	     i < (fq_base + 2 * DPAA_ETH_PCD_RXQ_NUM); i++)
++		qman_release_fqid(i);
++
++	dpaa_fq = dpaa_fq_alloc(dev, fq_base_aligned, DPAA_ETH_PCD_RXQ_NUM,
++				list, FQ_TYPE_RX_PCD);
++	if (!dpaa_fq)
++		goto fq_alloc_failed;
++
++	port_fqs->rx_pcdq = &dpaa_fq[0];
++
+ 	if (!dpaa_fq_alloc(dev, 0, DPAA_ETH_TXQ_NUM, list, FQ_TYPE_TX_CONF_MQ))
+ 		goto fq_alloc_failed;
+ 
+@@ -872,13 +893,14 @@ static void dpaa_fq_setup(struct dpaa_priv *priv,
+ 			  const struct dpaa_fq_cbs *fq_cbs,
+ 			  struct fman_port *tx_port)
+ {
+-	int egress_cnt = 0, conf_cnt = 0, num_portals = 0, cpu;
++	int egress_cnt = 0, conf_cnt = 0, num_portals = 0, portal_cnt = 0, cpu;
+ 	const cpumask_t *affine_cpus = qman_affine_cpus();
+-	u16 portals[NR_CPUS];
++	u16 channels[NR_CPUS];
+ 	struct dpaa_fq *fq;
+ 
+ 	for_each_cpu(cpu, affine_cpus)
+-		portals[num_portals++] = qman_affine_channel(cpu);
++		channels[num_portals++] = qman_affine_channel(cpu);
++
+ 	if (num_portals == 0)
+ 		dev_err(priv->net_dev->dev.parent,
+ 			"No Qman software (affine) channels found");
+@@ -892,6 +914,12 @@ static void dpaa_fq_setup(struct dpaa_priv *priv,
+ 		case FQ_TYPE_RX_ERROR:
+ 			dpaa_setup_ingress(priv, fq, &fq_cbs->rx_errq);
+ 			break;
++		case FQ_TYPE_RX_PCD:
++			if (!num_portals)
++				continue;
++			dpaa_setup_ingress(priv, fq, &fq_cbs->rx_defq);
++			fq->channel = channels[portal_cnt++ % num_portals];
++			break;
+ 		case FQ_TYPE_TX:
+ 			dpaa_setup_egress(priv, fq, tx_port,
+ 					  &fq_cbs->egress_ern);
+@@ -1041,7 +1069,8 @@ static int dpaa_fq_init(struct dpaa_fq *dpaa_fq, bool td_enable)
+ 		/* Put all the ingress queues in our "ingress CGR". */
+ 		if (priv->use_ingress_cgr &&
+ 		    (dpaa_fq->fq_type == FQ_TYPE_RX_DEFAULT ||
+-		     dpaa_fq->fq_type == FQ_TYPE_RX_ERROR)) {
++		     dpaa_fq->fq_type == FQ_TYPE_RX_ERROR ||
++		     dpaa_fq->fq_type == FQ_TYPE_RX_PCD)) {
+ 			initfq.we_mask |= cpu_to_be16(QM_INITFQ_WE_CGID);
+ 			initfq.fqd.fq_ctrl |= cpu_to_be16(QM_FQCTRL_CGE);
+ 			initfq.fqd.cgid = (u8)priv->ingress_cgr.cgrid;
+@@ -1172,7 +1201,7 @@ static int dpaa_eth_init_tx_port(struct fman_port *port, struct dpaa_fq *errq,
+ 
+ static int dpaa_eth_init_rx_port(struct fman_port *port, struct dpaa_bp **bps,
+ 				 size_t count, struct dpaa_fq *errq,
+-				 struct dpaa_fq *defq,
++				 struct dpaa_fq *defq, struct dpaa_fq *pcdq,
+ 				 struct dpaa_buffer_layout *buf_layout)
+ {
+ 	struct fman_buffer_prefix_content buf_prefix_content;
+@@ -1192,6 +1221,10 @@ static int dpaa_eth_init_rx_port(struct fman_port *port, struct dpaa_bp **bps,
+ 	rx_p = &params.specific_params.rx_params;
+ 	rx_p->err_fqid = errq->fqid;
+ 	rx_p->dflt_fqid = defq->fqid;
++	if (pcdq) {
++		rx_p->pcd_base_fqid = pcdq->fqid;
++		rx_p->pcd_fqs_count = DPAA_ETH_PCD_RXQ_NUM;
++	}
+ 
+ 	count = min(ARRAY_SIZE(rx_p->ext_buf_pools.ext_buf_pool), count);
+ 	rx_p->ext_buf_pools.num_of_pools_used = (u8)count;
+@@ -1236,7 +1269,8 @@ static int dpaa_eth_init_ports(struct mac_device *mac_dev,
+ 		return err;
+ 
+ 	err = dpaa_eth_init_rx_port(rxport, bps, count, port_fqs->rx_errq,
+-				    port_fqs->rx_defq, &buf_layout[RX]);
++				    port_fqs->rx_defq, port_fqs->rx_pcdq,
++				    &buf_layout[RX]);
+ 
+ 	return err;
+ }
+@@ -1600,8 +1634,8 @@ static struct sk_buff *dpaa_cleanup_tx_fd(const struct dpaa_priv *priv,
+ 
+ 	if (unlikely(qm_fd_get_format(fd) == qm_fd_sg)) {
+ 		nr_frags = skb_shinfo(skb)->nr_frags;
+-		dma_unmap_single(dev, addr, qm_fd_get_offset(fd) +
+-				 sizeof(struct qm_sg_entry) * (1 + nr_frags),
++		dma_unmap_single(dev, addr,
++				 qm_fd_get_offset(fd) + DPAA_SGT_SIZE,
+ 				 dma_dir);
+ 
+ 		/* The sgt buffer has been allocated with netdev_alloc_frag(),
+@@ -1718,6 +1752,7 @@ static struct sk_buff *sg_fd_to_skb(const struct dpaa_priv *priv,
+ 
+ 	/* Iterate through the SGT entries and add data buffers to the skb */
+ 	sgt = vaddr + fd_off;
++	skb = NULL;
+ 	for (i = 0; i < DPAA_SGT_MAX_ENTRIES; i++) {
+ 		/* Extension bit is not supported */
+ 		WARN_ON(qm_sg_entry_is_ext(&sgt[i]));
+@@ -1735,7 +1770,7 @@ static struct sk_buff *sg_fd_to_skb(const struct dpaa_priv *priv,
+ 		count_ptr = this_cpu_ptr(dpaa_bp->percpu_count);
+ 		dma_unmap_single(dpaa_bp->dev, sg_addr, dpaa_bp->size,
+ 				 DMA_FROM_DEVICE);
+-		if (i == 0) {
++		if (!skb) {
+ 			sz = dpaa_bp->size +
+ 				SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+ 			skb = build_skb(sg_vaddr, sz);
+@@ -1897,8 +1932,7 @@ static int skb_to_sg_fd(struct dpaa_priv *priv,
+ 	} else {
+ #endif
+ 		/* get a page frag to store the SGTable */
+-		sz = SKB_DATA_ALIGN(priv->tx_headroom +
+-			sizeof(struct qm_sg_entry) * (1 + nr_frags));
++		sz = SKB_DATA_ALIGN(priv->tx_headroom + DPAA_SGT_SIZE);
+ 		sgt_buf = netdev_alloc_frag(sz);
+ 		if (unlikely(!sgt_buf)) {
+ 			netdev_err(net_dev,
+@@ -1924,8 +1958,10 @@ static int skb_to_sg_fd(struct dpaa_priv *priv,
+ 		goto csum_failed;
+ 	}
+ 
++	/* SGT[0] is used by the linear part */
+ 	sgt = (struct qm_sg_entry *)(sgt_buf + priv->tx_headroom);
+-	qm_sg_entry_set_len(&sgt[0], skb_headlen(skb));
++	frag_len = skb_headlen(skb);
++	qm_sg_entry_set_len(&sgt[0], frag_len);
+ 	sgt[0].bpid = FSL_DPAA_BPID_INV;
+ 	sgt[0].offset = 0;
+ 	addr = dma_map_single(dev, skb->data,
+@@ -1938,9 +1974,9 @@ static int skb_to_sg_fd(struct dpaa_priv *priv,
+ 	qm_sg_entry_set64(&sgt[0], addr);
+ 
+ 	/* populate the rest of SGT entries */
+-	frag = &skb_shinfo(skb)->frags[0];
+-	frag_len = frag->size;
+-	for (i = 1; i <= nr_frags; i++, frag++) {
++	for (i = 0; i < nr_frags; i++) {
++		frag = &skb_shinfo(skb)->frags[i];
++		frag_len = frag->size;
+ 		WARN_ON(!skb_frag_page(frag));
+ 		addr = skb_frag_dma_map(dev, frag, 0,
+ 					frag_len, dma_dir);
+@@ -1950,15 +1986,16 @@ static int skb_to_sg_fd(struct dpaa_priv *priv,
+ 			goto sg_map_failed;
+ 		}
+ 
+-		qm_sg_entry_set_len(&sgt[i], frag_len);
+-		sgt[i].bpid = FSL_DPAA_BPID_INV;
+-		sgt[i].offset = 0;
++		qm_sg_entry_set_len(&sgt[i + 1], frag_len);
++		sgt[i + 1].bpid = FSL_DPAA_BPID_INV;
++		sgt[i + 1].offset = 0;
+ 
+ 		/* keep the offset in the address */
+-		qm_sg_entry_set64(&sgt[i], addr);
+-		frag_len = frag->size;
++		qm_sg_entry_set64(&sgt[i + 1], addr);
+ 	}
+-	qm_sg_entry_set_f(&sgt[i - 1], frag_len);
++
++	/* Set the final bit in the last used entry of the SGT */
++	qm_sg_entry_set_f(&sgt[nr_frags], frag_len);
+ 
+ 	qm_fd_set_sg(fd, priv->tx_headroom, skb->len);
+ 
+@@ -1967,9 +2004,8 @@ static int skb_to_sg_fd(struct dpaa_priv *priv,
+ 	skbh = (struct sk_buff **)buffer_start;
+ 	*skbh = skb;
+ 
+-	addr = dma_map_single(dev, buffer_start, priv->tx_headroom +
+-			      sizeof(struct qm_sg_entry) * (1 + nr_frags),
+-			      dma_dir);
++	addr = dma_map_single(dev, buffer_start,
++			      priv->tx_headroom + DPAA_SGT_SIZE, dma_dir);
+ 	if (unlikely(dma_mapping_error(dev, addr))) {
+ 		dev_err(dev, "DMA mapping failed");
+ 		err = -EINVAL;
+@@ -2016,7 +2052,6 @@ static inline int dpaa_xmit(struct dpaa_priv *priv,
+ 	}
+ 
+ 	if (unlikely(err < 0)) {
+-		percpu_stats->tx_errors++;
+ 		percpu_stats->tx_fifo_errors++;
+ 		return err;
+ 	}
+@@ -2338,14 +2373,8 @@ static enum qman_cb_dqrr_result rx_error_dqrr(struct qman_portal *portal,
+ 	if (dpaa_eth_napi_schedule(percpu_priv, portal))
+ 		return qman_cb_dqrr_stop;
+ 
+-	if (dpaa_eth_refill_bpools(priv))
+-		/* Unable to refill the buffer pool due to insufficient
+-		 * system memory. Just release the frame back into the pool,
+-		 * otherwise we'll soon end up with an empty buffer pool.
+-		 */
+-		dpaa_fd_release(net_dev, &dq->fd);
+-	else
+-		dpaa_rx_error(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
++	dpaa_eth_refill_bpools(priv);
++	dpaa_rx_error(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
+ 
+ 	return qman_cb_dqrr_consume;
+ }
+@@ -2360,12 +2389,13 @@ static enum qman_cb_dqrr_result rx_default_dqrr(struct qman_portal *portal,
+ 	dma_addr_t addr = qm_fd_addr(fd);
+ 	enum qm_fd_format fd_format;
+ 	struct net_device *net_dev;
+-	u32 fd_status;
++	u32 fd_status, hash_offset;
+ 	struct dpaa_bp *dpaa_bp;
+ 	struct dpaa_priv *priv;
+ 	unsigned int skb_len;
+ 	struct sk_buff *skb;
+ 	int *count_ptr;
++	void *vaddr;
+ 
+ 	fd_status = be32_to_cpu(fd->status);
+ 	fd_format = qm_fd_get_format(fd);
+@@ -2411,9 +2441,9 @@ static enum qman_cb_dqrr_result rx_default_dqrr(struct qman_portal *portal,
+ 	dma_unmap_single(dpaa_bp->dev, addr, dpaa_bp->size, DMA_FROM_DEVICE);
+ 
+ 	/* prefetch the first 64 bytes of the frame or the SGT start */
+-	prefetch(phys_to_virt(addr) + qm_fd_get_offset(fd));
++	vaddr = phys_to_virt(addr);
++	prefetch(vaddr + qm_fd_get_offset(fd));
+ 
+-	fd_format = qm_fd_get_format(fd);
+ 	/* The only FD types that we may receive are contig and S/G */
+ 	WARN_ON((fd_format != qm_fd_contig) && (fd_format != qm_fd_sg));
+ 
+@@ -2432,10 +2462,24 @@ static enum qman_cb_dqrr_result rx_default_dqrr(struct qman_portal *portal,
+ 
+ 	skb->protocol = eth_type_trans(skb, net_dev);
+ 
++	if (net_dev->features & NETIF_F_RXHASH && priv->keygen_in_use &&
++	    !fman_port_get_hash_result_offset(priv->mac_dev->port[RX],
++					      &hash_offset)) {
++		enum pkt_hash_types type;
++
++		/* if L4 exists, it was used in the hash generation */
++		type = be32_to_cpu(fd->status) & FM_FD_STAT_L4CV ?
++			PKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3;
++		skb_set_hash(skb, be32_to_cpu(*(u32 *)(vaddr + hash_offset)),
++			     type);
++	}
++
+ 	skb_len = skb->len;
+ 
+-	if (unlikely(netif_receive_skb(skb) == NET_RX_DROP))
++	if (unlikely(netif_receive_skb(skb) == NET_RX_DROP)) {
++		percpu_stats->rx_dropped++;
+ 		return qman_cb_dqrr_consume;
++	}
+ 
+ 	percpu_stats->rx_packets++;
+ 	percpu_stats->rx_bytes += skb_len;
+@@ -2544,6 +2588,44 @@ static void dpaa_eth_napi_disable(struct dpaa_priv *priv)
+ 	}
+ }
+ 
++static void dpaa_adjust_link(struct net_device *net_dev)
++{
++	struct mac_device *mac_dev;
++	struct dpaa_priv *priv;
++
++	priv = netdev_priv(net_dev);
++	mac_dev = priv->mac_dev;
++	mac_dev->adjust_link(mac_dev);
++}
++
++static int dpaa_phy_init(struct net_device *net_dev)
++{
++	struct mac_device *mac_dev;
++	struct phy_device *phy_dev;
++	struct dpaa_priv *priv;
++
++	priv = netdev_priv(net_dev);
++	mac_dev = priv->mac_dev;
++
++	phy_dev = of_phy_connect(net_dev, mac_dev->phy_node,
++				 &dpaa_adjust_link, 0,
++				 mac_dev->phy_if);
++	if (!phy_dev) {
++		netif_err(priv, ifup, net_dev, "init_phy() failed\n");
++		return -ENODEV;
++	}
++
++	/* Remove any features not supported by the controller */
++	phy_dev->supported &= mac_dev->if_support;
++	phy_dev->supported |= (SUPPORTED_Pause | SUPPORTED_Asym_Pause);
++	phy_dev->advertising = phy_dev->supported;
++
++	mac_dev->phy_dev = phy_dev;
++	net_dev->phydev = phy_dev;
++
++	return 0;
++}
++
+ static int dpaa_open(struct net_device *net_dev)
+ {
+ 	struct mac_device *mac_dev;
+@@ -2554,12 +2636,9 @@ static int dpaa_open(struct net_device *net_dev)
+ 	mac_dev = priv->mac_dev;
+ 	dpaa_eth_napi_enable(priv);
+ 
+-	net_dev->phydev = mac_dev->init_phy(net_dev, priv->mac_dev);
+-	if (!net_dev->phydev) {
+-		netif_err(priv, ifup, net_dev, "init_phy() failed\n");
+-		err = -ENODEV;
++	err = dpaa_phy_init(net_dev);
++	if (err)
+ 		goto phy_init_failed;
+-	}
+ 
+ 	for (i = 0; i < ARRAY_SIZE(mac_dev->port); i++) {
+ 		err = fman_port_enable(mac_dev->port[i]);
+@@ -2670,6 +2749,9 @@ static struct dpaa_bp *dpaa_bp_alloc(struct device *dev)
+ 
+ 	dpaa_bp->bpid = FSL_DPAA_BPID_INV;
+ 	dpaa_bp->percpu_count = devm_alloc_percpu(dev, *dpaa_bp->percpu_count);
++	if (!dpaa_bp->percpu_count)
++		return ERR_PTR(-ENOMEM);
++
+ 	dpaa_bp->config_count = FSL_DPAA_ETH_MAX_BUF_COUNT;
+ 
+ 	dpaa_bp->seed_cb = dpaa_bp_seed;
+@@ -2755,7 +2837,6 @@ static inline u16 dpaa_get_headroom(struct dpaa_buffer_layout *bl)
+ static int dpaa_eth_probe(struct platform_device *pdev)
+ {
+ 	struct dpaa_bp *dpaa_bps[DPAA_BPS_NUM] = {NULL};
+-	struct dpaa_percpu_priv *percpu_priv;
+ 	struct net_device *net_dev = NULL;
+ 	struct dpaa_fq *dpaa_fq, *tmp;
+ 	struct dpaa_priv *priv = NULL;
+@@ -2764,7 +2845,13 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	int err = 0, i, channel;
+ 	struct device *dev;
+ 
+-	dev = &pdev->dev;
++	/* device used for DMA mapping */
++	dev = pdev->dev.parent;
++	err = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(40));
++	if (err) {
++		dev_err(dev, "dma_coerce_mask_and_coherent() failed\n");
++		return err;
++	}
+ 
+ 	/* Allocate this early, so we can store relevant information in
+ 	 * the private area
+@@ -2772,7 +2859,7 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	net_dev = alloc_etherdev_mq(sizeof(*priv), DPAA_ETH_TXQ_NUM);
+ 	if (!net_dev) {
+ 		dev_err(dev, "alloc_etherdev_mq() failed\n");
+-		goto alloc_etherdev_mq_failed;
++		return -ENOMEM;
+ 	}
+ 
+ 	/* Do this here, so we can be verbose early */
+@@ -2788,7 +2875,7 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	if (IS_ERR(mac_dev)) {
+ 		dev_err(dev, "dpaa_mac_dev_get() failed\n");
+ 		err = PTR_ERR(mac_dev);
+-		goto mac_probe_failed;
++		goto free_netdev;
+ 	}
+ 
+ 	/* If fsl_fm_max_frm is set to a higher value than the all-common 1500,
+@@ -2806,21 +2893,13 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	priv->buf_layout[RX].priv_data_size = DPAA_RX_PRIV_DATA_SIZE; /* Rx */
+ 	priv->buf_layout[TX].priv_data_size = DPAA_TX_PRIV_DATA_SIZE; /* Tx */
+ 
+-	/* device used for DMA mapping */
+-	arch_setup_dma_ops(dev, 0, 0, NULL, false);
+-	err = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(40));
+-	if (err) {
+-		dev_err(dev, "dma_coerce_mask_and_coherent() failed\n");
+-		goto dev_mask_failed;
+-	}
+-
+ 	/* bp init */
+ 	for (i = 0; i < DPAA_BPS_NUM; i++) {
+-		int err;
+-
+ 		dpaa_bps[i] = dpaa_bp_alloc(dev);
+-		if (IS_ERR(dpaa_bps[i]))
+-			return PTR_ERR(dpaa_bps[i]);
++		if (IS_ERR(dpaa_bps[i])) {
++			err = PTR_ERR(dpaa_bps[i]);
++			goto free_dpaa_bps;
++		}
+ 		/* the raw size of the buffers used for reception */
+ 		dpaa_bps[i]->raw_size = bpool_buffer_raw_size(i, DPAA_BPS_NUM);
+ 		/* avoid runtime computations by keeping the usable size here */
+@@ -2828,11 +2907,8 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 		dpaa_bps[i]->dev = dev;
+ 
+ 		err = dpaa_bp_alloc_pool(dpaa_bps[i]);
+-		if (err < 0) {
+-			dpaa_bps_free(priv);
+-			priv->dpaa_bps[i] = NULL;
+-			goto bp_create_failed;
+-		}
++		if (err < 0)
++			goto free_dpaa_bps;
+ 		priv->dpaa_bps[i] = dpaa_bps[i];
+ 	}
+ 
+@@ -2843,7 +2919,7 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	err = dpaa_alloc_all_fqs(dev, &priv->dpaa_fq_list, &port_fqs);
+ 	if (err < 0) {
+ 		dev_err(dev, "dpaa_alloc_all_fqs() failed\n");
+-		goto fq_probe_failed;
++		goto free_dpaa_bps;
+ 	}
+ 
+ 	priv->mac_dev = mac_dev;
+@@ -2852,12 +2928,12 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	if (channel < 0) {
+ 		dev_err(dev, "dpaa_get_channel() failed\n");
+ 		err = channel;
+-		goto get_channel_failed;
++		goto free_dpaa_bps;
+ 	}
+ 
+ 	priv->channel = (u16)channel;
+ 
+-	/* Start a thread that will walk the CPUs with affine portals
++	/* Walk the CPUs with affine portals
+ 	 * and add this pool channel to each's dequeue mask.
+ 	 */
+ 	dpaa_eth_add_channel(priv->channel);
+@@ -2872,20 +2948,20 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	err = dpaa_eth_cgr_init(priv);
+ 	if (err < 0) {
+ 		dev_err(dev, "Error initializing CGR\n");
+-		goto tx_cgr_init_failed;
++		goto free_dpaa_bps;
+ 	}
+ 
+ 	err = dpaa_ingress_cgr_init(priv);
+ 	if (err < 0) {
+ 		dev_err(dev, "Error initializing ingress CGR\n");
+-		goto rx_cgr_init_failed;
++		goto delete_egress_cgr;
+ 	}
+ 
+ 	/* Add the FQs to the interface, and make them active */
+ 	list_for_each_entry_safe(dpaa_fq, tmp, &priv->dpaa_fq_list, list) {
+ 		err = dpaa_fq_init(dpaa_fq, false);
+ 		if (err < 0)
+-			goto fq_alloc_failed;
++			goto free_dpaa_fqs;
+ 	}
+ 
+ 	priv->tx_headroom = dpaa_get_headroom(&priv->buf_layout[TX]);
+@@ -2895,17 +2971,16 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	err = dpaa_eth_init_ports(mac_dev, dpaa_bps, DPAA_BPS_NUM, &port_fqs,
+ 				  &priv->buf_layout[0], dev);
+ 	if (err)
+-		goto init_ports_failed;
++		goto free_dpaa_fqs;
++
++	/* Rx traffic distribution based on keygen hashing defaults to on */
++	priv->keygen_in_use = true;
+ 
+ 	priv->percpu_priv = devm_alloc_percpu(dev, *priv->percpu_priv);
+ 	if (!priv->percpu_priv) {
+ 		dev_err(dev, "devm_alloc_percpu() failed\n");
+ 		err = -ENOMEM;
+-		goto alloc_percpu_failed;
+-	}
+-	for_each_possible_cpu(i) {
+-		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+-		memset(percpu_priv, 0, sizeof(*percpu_priv));
++		goto free_dpaa_fqs;
+ 	}
+ 
+ 	priv->num_tc = 1;
+@@ -2914,11 +2989,11 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 	/* Initialize NAPI */
+ 	err = dpaa_napi_add(net_dev);
+ 	if (err < 0)
+-		goto napi_add_failed;
++		goto delete_dpaa_napi;
+ 
+ 	err = dpaa_netdev_init(net_dev, &dpaa_ops, tx_timeout);
+ 	if (err < 0)
+-		goto netdev_init_failed;
++		goto delete_dpaa_napi;
+ 
+ 	dpaa_eth_sysfs_init(&net_dev->dev);
+ 
+@@ -2927,32 +3002,21 @@ static int dpaa_eth_probe(struct platform_device *pdev)
+ 
+ 	return 0;
+ 
+-netdev_init_failed:
+-napi_add_failed:
++delete_dpaa_napi:
+ 	dpaa_napi_del(net_dev);
+-alloc_percpu_failed:
+-init_ports_failed:
++free_dpaa_fqs:
+ 	dpaa_fq_free(dev, &priv->dpaa_fq_list);
+-fq_alloc_failed:
+ 	qman_delete_cgr_safe(&priv->ingress_cgr);
+ 	qman_release_cgrid(priv->ingress_cgr.cgrid);
+-rx_cgr_init_failed:
++delete_egress_cgr:
+ 	qman_delete_cgr_safe(&priv->cgr_data.cgr);
+ 	qman_release_cgrid(priv->cgr_data.cgr.cgrid);
+-tx_cgr_init_failed:
+-get_channel_failed:
++free_dpaa_bps:
+ 	dpaa_bps_free(priv);
+-bp_create_failed:
+-fq_probe_failed:
+-dev_mask_failed:
+-mac_probe_failed:
++free_netdev:
+ 	dev_set_drvdata(dev, NULL);
+ 	free_netdev(net_dev);
+-alloc_etherdev_mq_failed:
+-	for (i = 0; i < DPAA_BPS_NUM && dpaa_bps[i]; i++) {
+-		if (atomic_read(&dpaa_bps[i]->refs) == 0)
+-			devm_kfree(dev, dpaa_bps[i]);
+-	}
++
+ 	return err;
+ }
+ 
+@@ -2963,7 +3027,7 @@ static int dpaa_remove(struct platform_device *pdev)
+ 	struct device *dev;
+ 	int err;
+ 
+-	dev = &pdev->dev;
++	dev = pdev->dev.parent;
+ 	net_dev = dev_get_drvdata(dev);
+ 
+ 	priv = netdev_priv(net_dev);
+@@ -3006,7 +3070,7 @@ static bool __init soc_has_errata_a010022(void)
+ }
+ #endif
+ 
+-static struct platform_device_id dpaa_devtype[] = {
++static const struct platform_device_id dpaa_devtype[] = {
+ 	{
+ 		.name = "dpaa-ethernet",
+ 		.driver_data = 0,
+diff --git a/drivers/net/ethernet/freescale/dpaa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpaa/dpaa_eth.h
+index 9941a786..bd942208 100644
+--- a/drivers/net/ethernet/freescale/dpaa/dpaa_eth.h
++++ b/drivers/net/ethernet/freescale/dpaa/dpaa_eth.h
+@@ -52,6 +52,7 @@
+ enum dpaa_fq_type {
+ 	FQ_TYPE_RX_DEFAULT = 1, /* Rx Default FQs */
+ 	FQ_TYPE_RX_ERROR,	/* Rx Error FQs */
++	FQ_TYPE_RX_PCD,		/* Rx Parse Classify Distribute FQs */
+ 	FQ_TYPE_TX,		/* "Real" Tx FQs */
+ 	FQ_TYPE_TX_CONFIRM,	/* Tx default Conf FQ (actually an Rx FQ) */
+ 	FQ_TYPE_TX_CONF_MQ,	/* Tx conf FQs (one for each Tx FQ) */
+@@ -158,6 +159,7 @@ struct dpaa_priv {
+ 	struct list_head dpaa_fq_list;
+ 
+ 	u8 num_tc;
++	bool keygen_in_use;
+ 	u32 msg_enable;	/* net_device message level */
+ 
+ 	struct {
+diff --git a/drivers/net/ethernet/freescale/dpaa/dpaa_eth_sysfs.c b/drivers/net/ethernet/freescale/dpaa/dpaa_eth_sysfs.c
+index ec75d1c6..0d9b185e 100644
+--- a/drivers/net/ethernet/freescale/dpaa/dpaa_eth_sysfs.c
++++ b/drivers/net/ethernet/freescale/dpaa/dpaa_eth_sysfs.c
+@@ -71,6 +71,9 @@ static ssize_t dpaa_eth_show_fqids(struct device *dev,
+ 		case FQ_TYPE_RX_ERROR:
+ 			str = "Rx error";
+ 			break;
++		case FQ_TYPE_RX_PCD:
++			str = "Rx PCD";
++			break;
+ 		case FQ_TYPE_TX_CONFIRM:
+ 			str = "Tx default confirmation";
+ 			break;
+diff --git a/drivers/net/ethernet/freescale/dpaa/dpaa_ethtool.c b/drivers/net/ethernet/freescale/dpaa/dpaa_ethtool.c
+index 15571e25..d23fc047 100644
+--- a/drivers/net/ethernet/freescale/dpaa/dpaa_ethtool.c
++++ b/drivers/net/ethernet/freescale/dpaa/dpaa_ethtool.c
+@@ -213,7 +213,7 @@ static int dpaa_set_pauseparam(struct net_device *net_dev,
+ 	if (epause->rx_pause)
+ 		newadv = ADVERTISED_Pause | ADVERTISED_Asym_Pause;
+ 	if (epause->tx_pause)
+-		newadv |= ADVERTISED_Asym_Pause;
++		newadv ^= ADVERTISED_Asym_Pause;
+ 
+ 	oldadv = phydev->advertising &
+ 			(ADVERTISED_Pause | ADVERTISED_Asym_Pause);
+@@ -346,7 +346,7 @@ static void dpaa_get_ethtool_stats(struct net_device *net_dev,
+ 
+ 	/* gather congestion related counters */
+ 	cg_num    = 0;
+-	cg_status = 0;
++	cg_status = false;
+ 	cg_time   = jiffies_to_msecs(priv->cgr_data.congested_jiffies);
+ 	if (qman_query_cgr_congested(&priv->cgr_data.cgr, &cg_status) == 0) {
+ 		cg_num    = priv->cgr_data.cgr_congested_count;
+@@ -401,6 +401,122 @@ static void dpaa_get_strings(struct net_device *net_dev, u32 stringset,
+ 	memcpy(strings, dpaa_stats_global, size);
+ }
+ 
++static int dpaa_get_hash_opts(struct net_device *dev,
++			      struct ethtool_rxnfc *cmd)
++{
++	struct dpaa_priv *priv = netdev_priv(dev);
++
++	cmd->data = 0;
++
++	switch (cmd->flow_type) {
++	case TCP_V4_FLOW:
++	case TCP_V6_FLOW:
++	case UDP_V4_FLOW:
++	case UDP_V6_FLOW:
++		if (priv->keygen_in_use)
++			cmd->data |= RXH_L4_B_0_1 | RXH_L4_B_2_3;
++		/* Fall through */
++	case IPV4_FLOW:
++	case IPV6_FLOW:
++	case SCTP_V4_FLOW:
++	case SCTP_V6_FLOW:
++	case AH_ESP_V4_FLOW:
++	case AH_ESP_V6_FLOW:
++	case AH_V4_FLOW:
++	case AH_V6_FLOW:
++	case ESP_V4_FLOW:
++	case ESP_V6_FLOW:
++		if (priv->keygen_in_use)
++			cmd->data |= RXH_IP_SRC | RXH_IP_DST;
++		break;
++	default:
++		cmd->data = 0;
++		break;
++	}
++
++	return 0;
++}
++
++static int dpaa_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd,
++			  u32 *unused)
++{
++	int ret = -EOPNOTSUPP;
++
++	switch (cmd->cmd) {
++	case ETHTOOL_GRXFH:
++		ret = dpaa_get_hash_opts(dev, cmd);
++		break;
++	default:
++		break;
++	}
++
++	return ret;
++}
++
++static void dpaa_set_hash(struct net_device *net_dev, bool enable)
++{
++	struct mac_device *mac_dev;
++	struct fman_port *rxport;
++	struct dpaa_priv *priv;
++
++	priv = netdev_priv(net_dev);
++	mac_dev = priv->mac_dev;
++	rxport = mac_dev->port[0];
++
++	fman_port_use_kg_hash(rxport, enable);
++	priv->keygen_in_use = enable;
++}
++
++static int dpaa_set_hash_opts(struct net_device *dev,
++			      struct ethtool_rxnfc *nfc)
++{
++	int ret = -EINVAL;
++
++	/* we support hashing on IPv4/v6 src/dest IP and L4 src/dest port */
++	if (nfc->data &
++	    ~(RXH_IP_SRC | RXH_IP_DST | RXH_L4_B_0_1 | RXH_L4_B_2_3))
++		return -EINVAL;
++
++	switch (nfc->flow_type) {
++	case TCP_V4_FLOW:
++	case TCP_V6_FLOW:
++	case UDP_V4_FLOW:
++	case UDP_V6_FLOW:
++	case IPV4_FLOW:
++	case IPV6_FLOW:
++	case SCTP_V4_FLOW:
++	case SCTP_V6_FLOW:
++	case AH_ESP_V4_FLOW:
++	case AH_ESP_V6_FLOW:
++	case AH_V4_FLOW:
++	case AH_V6_FLOW:
++	case ESP_V4_FLOW:
++	case ESP_V6_FLOW:
++		dpaa_set_hash(dev, !!nfc->data);
++		ret = 0;
++		break;
++	default:
++		break;
++	}
++
++	return ret;
++}
++
++static int dpaa_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd)
++{
++	int ret = -EOPNOTSUPP;
++
++	switch (cmd->cmd) {
++	case ETHTOOL_SRXFH:
++		ret = dpaa_set_hash_opts(dev, cmd);
++		break;
++	default:
++		break;
++	}
++
++	return ret;
++}
++
+ const struct ethtool_ops dpaa_ethtool_ops = {
+ 	.get_drvinfo = dpaa_get_drvinfo,
+ 	.get_msglevel = dpaa_get_msglevel,
+@@ -414,4 +530,6 @@ const struct ethtool_ops dpaa_ethtool_ops = {
+ 	.get_strings = dpaa_get_strings,
+ 	.get_link_ksettings = dpaa_get_link_ksettings,
+ 	.set_link_ksettings = dpaa_set_link_ksettings,
++	.get_rxnfc = dpaa_get_rxnfc,
++	.set_rxnfc = dpaa_set_rxnfc,
+ };
+diff --git a/drivers/net/ethernet/freescale/fec_main.c b/drivers/net/ethernet/freescale/fec_main.c
+index 91709187..fe00f71b 100644
+--- a/drivers/net/ethernet/freescale/fec_main.c
++++ b/drivers/net/ethernet/freescale/fec_main.c
+@@ -2371,6 +2371,10 @@ static int fec_enet_get_sset_count(struct net_device *dev, int sset)
+ static inline void fec_enet_update_ethtool_stats(struct net_device *dev)
+ {
+ }
++
++static inline void fec_enet_clear_ethtool_stats(struct net_device *dev)
++{
++}
+ #endif /* !defined(CONFIG_M5272) */
+ 
+ static int fec_enet_nway_reset(struct net_device *dev)
+@@ -3209,7 +3213,7 @@ static int fec_enet_init(struct net_device *ndev)
+ }
+ 
+ #ifdef CONFIG_OF
+-static void fec_reset_phy(struct platform_device *pdev)
++static int fec_reset_phy(struct platform_device *pdev)
+ {
+ 	int err, phy_reset;
+ 	bool active_high = false;
+@@ -3217,7 +3221,7 @@ static void fec_reset_phy(struct platform_device *pdev)
+ 	struct device_node *np = pdev->dev.of_node;
+ 
+ 	if (!np)
+-		return;
++		return 0;
+ 
+ 	of_property_read_u32(np, "phy-reset-duration", &msec);
+ 	/* A sane reset duration should not be longer than 1s */
+@@ -3225,8 +3229,10 @@ static void fec_reset_phy(struct platform_device *pdev)
+ 		msec = 1;
+ 
+ 	phy_reset = of_get_named_gpio(np, "phy-reset-gpios", 0);
+-	if (!gpio_is_valid(phy_reset))
+-		return;
++	if (phy_reset == -EPROBE_DEFER)
++		return phy_reset;
++	else if (!gpio_is_valid(phy_reset))
++		return 0;
+ 
+ 	active_high = of_property_read_bool(np, "phy-reset-active-high");
+ 
+@@ -3235,7 +3241,7 @@ static void fec_reset_phy(struct platform_device *pdev)
+ 			"phy-reset");
+ 	if (err) {
+ 		dev_err(&pdev->dev, "failed to get phy-reset-gpios: %d\n", err);
+-		return;
++		return err;
+ 	}
+ 
+ 	if (msec > 20)
+@@ -3244,14 +3250,17 @@ static void fec_reset_phy(struct platform_device *pdev)
+ 		usleep_range(msec * 1000, msec * 1000 + 1000);
+ 
+ 	gpio_set_value_cansleep(phy_reset, !active_high);
++
++	return 0;
+ }
+ #else /* CONFIG_OF */
+-static void fec_reset_phy(struct platform_device *pdev)
++static int fec_reset_phy(struct platform_device *pdev)
+ {
+ 	/*
+ 	 * In case of platform probe, the reset has been done
+ 	 * by machine code.
+ 	 */
++	return 0;
+ }
+ #endif /* CONFIG_OF */
+ 
+@@ -3422,6 +3431,7 @@ fec_probe(struct platform_device *pdev)
+ 		if (ret) {
+ 			dev_err(&pdev->dev,
+ 				"Failed to enable phy regulator: %d\n", ret);
++			clk_disable_unprepare(fep->clk_ipg);
+ 			goto failed_regulator;
+ 		}
+ 	} else {
+@@ -3434,7 +3444,9 @@ fec_probe(struct platform_device *pdev)
+ 	pm_runtime_set_active(&pdev->dev);
+ 	pm_runtime_enable(&pdev->dev);
+ 
+-	fec_reset_phy(pdev);
++	ret = fec_reset_phy(pdev);
++	if (ret)
++		goto failed_reset;
+ 
+ 	if (fep->bufdesc_ex)
+ 		fec_ptp_init(pdev);
+@@ -3495,8 +3507,10 @@ fec_probe(struct platform_device *pdev)
+ 	fec_ptp_stop(pdev);
+ 	if (fep->reg_phy)
+ 		regulator_disable(fep->reg_phy);
++failed_reset:
++	pm_runtime_put(&pdev->dev);
++	pm_runtime_disable(&pdev->dev);
+ failed_regulator:
+-	clk_disable_unprepare(fep->clk_ipg);
+ failed_clk_ipg:
+ 	fec_enet_clk_enable(ndev, false);
+ failed_clk:
+@@ -3523,6 +3537,8 @@ fec_drv_remove(struct platform_device *pdev)
+ 	fec_enet_mii_remove(fep);
+ 	if (fep->reg_phy)
+ 		regulator_disable(fep->reg_phy);
++	pm_runtime_put(&pdev->dev);
++	pm_runtime_disable(&pdev->dev);
+ 	if (of_phy_is_fixed_link(np))
+ 		of_phy_deregister_fixed_link(np);
+ 	of_node_put(fep->phy_node);
+diff --git a/drivers/net/ethernet/freescale/fman/Makefile b/drivers/net/ethernet/freescale/fman/Makefile
+index 60491779..4ae524a3 100644
+--- a/drivers/net/ethernet/freescale/fman/Makefile
++++ b/drivers/net/ethernet/freescale/fman/Makefile
+@@ -1,9 +1,9 @@
+ subdir-ccflags-y +=  -I$(srctree)/drivers/net/ethernet/freescale/fman
+ 
+-obj-$(CONFIG_FSL_FMAN) += fsl_fman.o
+-obj-$(CONFIG_FSL_FMAN) += fsl_fman_port.o
+-obj-$(CONFIG_FSL_FMAN) += fsl_mac.o
++obj-$(CONFIG_FSL_FMAN) += fsl_dpaa_fman.o
++obj-$(CONFIG_FSL_FMAN) += fsl_dpaa_fman_port.o
++obj-$(CONFIG_FSL_FMAN) += fsl_dpaa_mac.o
+ 
+-fsl_fman-objs	:= fman_muram.o fman.o fman_sp.o
+-fsl_fman_port-objs := fman_port.o
+-fsl_mac-objs:= mac.o fman_dtsec.o fman_memac.o fman_tgec.o
++fsl_dpaa_fman-objs	:= fman_muram.o fman.o fman_sp.o fman_keygen.o
++fsl_dpaa_fman_port-objs := fman_port.o
++fsl_dpaa_mac-objs:= mac.o fman_dtsec.o fman_memac.o fman_tgec.o
+diff --git a/drivers/net/ethernet/freescale/fman/fman.c b/drivers/net/ethernet/freescale/fman/fman.c
+index 4aefe243..4e44c947 100644
+--- a/drivers/net/ethernet/freescale/fman/fman.c
++++ b/drivers/net/ethernet/freescale/fman/fman.c
+@@ -32,9 +32,6 @@
+ 
+ #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+ 
+-#include "fman.h"
+-#include "fman_muram.h"
+-
+ #include <linux/fsl/guts.h>
+ #include <linux/slab.h>
+ #include <linux/delay.h>
+@@ -46,6 +43,10 @@
+ #include <linux/interrupt.h>
+ #include <linux/libfdt_env.h>
+ 
++#include "fman.h"
++#include "fman_muram.h"
++#include "fman_keygen.h"
++
+ /* General defines */
+ #define FMAN_LIODN_TBL			64	/* size of LIODN table */
+ #define MAX_NUM_OF_MACS			10
+@@ -56,6 +57,7 @@
+ /* Modules registers offsets */
+ #define BMI_OFFSET		0x00080000
+ #define QMI_OFFSET		0x00080400
++#define KG_OFFSET		0x000C1000
+ #define DMA_OFFSET		0x000C2000
+ #define FPM_OFFSET		0x000C3000
+ #define IMEM_OFFSET		0x000C4000
+@@ -564,80 +566,6 @@ struct fman_cfg {
+ 	u32 qmi_def_tnums_thresh;
+ };
+ 
+-/* Structure that holds information received from device tree */
+-struct fman_dts_params {
+-	void __iomem *base_addr;		/* FMan virtual address */
+-	struct resource *res;			/* FMan memory resource */
+-	u8 id;					/* FMan ID */
+-
+-	int err_irq;				/* FMan Error IRQ */
+-
+-	u16 clk_freq;				/* FMan clock freq (In Mhz) */
+-
+-	u32 qman_channel_base;			/* QMan channels base */
+-	u32 num_of_qman_channels;		/* Number of QMan channels */
+-
+-	struct resource muram_res;		/* MURAM resource */
+-};
+-
+-/** fman_exceptions_cb
+- * fman		- Pointer to FMan
+- * exception	- The exception.
+- *
+- * Exceptions user callback routine, will be called upon an exception
+- * passing the exception identification.
+- *
+- * Return: irq status
+- */
+-typedef irqreturn_t (fman_exceptions_cb)(struct fman *fman,
+-					 enum fman_exceptions exception);
+-
+-/** fman_bus_error_cb
+- * fman		- Pointer to FMan
+- * port_id	- Port id
+- * addr		- Address that caused the error
+- * tnum		- Owner of error
+- * liodn	- Logical IO device number
+- *
+- * Bus error user callback routine, will be called upon bus error,
+- * passing parameters describing the errors and the owner.
+- *
+- * Return: IRQ status
+- */
+-typedef irqreturn_t (fman_bus_error_cb)(struct fman *fman, u8 port_id,
+-					u64 addr, u8 tnum, u16 liodn);
+-
+-struct fman {
+-	struct device *dev;
+-	void __iomem *base_addr;
+-	struct fman_intr_src intr_mng[FMAN_EV_CNT];
+-
+-	struct fman_fpm_regs __iomem *fpm_regs;
+-	struct fman_bmi_regs __iomem *bmi_regs;
+-	struct fman_qmi_regs __iomem *qmi_regs;
+-	struct fman_dma_regs __iomem *dma_regs;
+-	struct fman_hwp_regs __iomem *hwp_regs;
+-	fman_exceptions_cb *exception_cb;
+-	fman_bus_error_cb *bus_error_cb;
+-	/* Spinlock for FMan use */
+-	spinlock_t spinlock;
+-	struct fman_state_struct *state;
+-
+-	struct fman_cfg *cfg;
+-	struct muram_info *muram;
+-	/* cam section in muram */
+-	unsigned long cam_offset;
+-	size_t cam_size;
+-	/* Fifo in MURAM */
+-	unsigned long fifo_offset;
+-	size_t fifo_size;
+-
+-	u32 liodn_base[64];
+-	u32 liodn_offset[64];
+-
+-	struct fman_dts_params dts_params;
+-};
+-
+ static irqreturn_t fman_exceptions(struct fman *fman,
+ 				   enum fman_exceptions exception)
+ {
+@@ -1811,6 +1739,7 @@ static int fman_config(struct fman *fman)
+ 	fman->qmi_regs = base_addr + QMI_OFFSET;
+ 	fman->dma_regs = base_addr + DMA_OFFSET;
+ 	fman->hwp_regs = base_addr + HWP_OFFSET;
++	fman->kg_regs = base_addr + KG_OFFSET;
+ 	fman->base_addr = base_addr;
+ 
+ 	spin_lock_init(&fman->spinlock);
+@@ -2083,6 +2012,11 @@ static int fman_init(struct fman *fman)
+ 	/* Init HW Parser */
+ 	hwp_init(fman->hwp_regs);
+ 
++	/* Init KeyGen */
++	fman->keygen = keygen_init(fman->kg_regs);
++	if (!fman->keygen)
++		return -EINVAL;
++
+ 	err = enable(fman, cfg);
+ 	if (err != 0)
+ 		return err;
+@@ -2434,15 +2368,21 @@ u32 fman_get_qman_channel_id(struct fman *fman, u32 port_id)
+ 	int i;
+ 
+ 	if (fman->state->rev_info.major >= 6) {
+-		u32 port_ids[] = {0x30, 0x31, 0x28, 0x29, 0x2a, 0x2b,
+-				  0x2c, 0x2d, 0x2, 0x3, 0x4, 0x5, 0x7, 0x7};
++		static const u32 port_ids[] = {
++			0x30, 0x31, 0x28, 0x29, 0x2a, 0x2b,
++			0x2c, 0x2d, 0x2, 0x3, 0x4, 0x5, 0x7, 0x7
++		};
++
+ 		for (i = 0; i < fman->state->num_of_qman_channels; i++) {
+ 			if (port_ids[i] == port_id)
+ 				break;
+ 		}
+ 	} else {
+-		u32 port_ids[] = {0x30, 0x28, 0x29, 0x2a, 0x2b, 0x2c, 0x1,
+-				  0x2, 0x3, 0x4, 0x5, 0x7, 0x7};
++		static const u32 port_ids[] = {
++			0x30, 0x28, 0x29, 0x2a, 0x2b, 0x2c, 0x1,
++			0x2, 0x3, 0x4, 0x5, 0x7, 0x7
++		};
++
+ 		for (i = 0; i < fman->state->num_of_qman_channels; i++) {
+ 			if (port_ids[i] == port_id)
+ 				break;
+diff --git a/drivers/net/ethernet/freescale/fman/fman.h b/drivers/net/ethernet/freescale/fman/fman.h
+index f53e1473..bfa02e00 100644
+--- a/drivers/net/ethernet/freescale/fman/fman.h
++++ b/drivers/net/ethernet/freescale/fman/fman.h
+@@ -34,6 +34,8 @@
+ #define __FM_H
+ 
+ #include <linux/io.h>
++#include <linux/interrupt.h>
++#include <linux/of_irq.h>
+ 
+ /* FM Frame descriptor macros  */
+ /* Frame queue Context Override */
+@@ -274,6 +276,81 @@ struct fman_intr_src {
+ 	void *src_handle;
+ };
+ 
++/** fman_exceptions_cb
++ * fman         - Pointer to FMan
++ * exception    - The exception.
++ *
++ * Exceptions user callback routine, will be called upon an exception
++ * passing the exception identification.
++ *
++ * Return: irq status
++ */
++typedef irqreturn_t (fman_exceptions_cb)(struct fman *fman,
++					 enum fman_exceptions exception);
++/** fman_bus_error_cb
++ * fman         - Pointer to FMan
++ * port_id      - Port id
++ * addr         - Address that caused the error
++ * tnum         - Owner of error
++ * liodn        - Logical IO device number
++ *
++ * Bus error user callback routine, will be called upon bus error,
++ * passing parameters describing the errors and the owner.
++ *
++ * Return: IRQ status
++ */
++typedef irqreturn_t (fman_bus_error_cb)(struct fman *fman, u8 port_id,
++					u64 addr, u8 tnum, u16 liodn);
++
++/* Structure that holds information received from device tree */
++struct fman_dts_params {
++	void __iomem *base_addr;                /* FMan virtual address */
++	struct resource *res;                   /* FMan memory resource */
++	u8 id;                                  /* FMan ID */
++
++	int err_irq;                            /* FMan Error IRQ */
++
++	u16 clk_freq;                           /* FMan clock freq (In Mhz) */
++
++	u32 qman_channel_base;                  /* QMan channels base */
++	u32 num_of_qman_channels;               /* Number of QMan channels */
++
++	struct resource muram_res;              /* MURAM resource */
++};
++
++struct fman {
++	struct device *dev;
++	void __iomem *base_addr;
++	struct fman_intr_src intr_mng[FMAN_EV_CNT];
++
++	struct fman_fpm_regs __iomem *fpm_regs;
++	struct fman_bmi_regs __iomem *bmi_regs;
++	struct fman_qmi_regs __iomem *qmi_regs;
++	struct fman_dma_regs __iomem *dma_regs;
++	struct fman_hwp_regs __iomem *hwp_regs;
++	struct fman_kg_regs __iomem *kg_regs;
++	fman_exceptions_cb *exception_cb;
++	fman_bus_error_cb *bus_error_cb;
++	/* Spinlock for FMan use */
++	spinlock_t spinlock;
++	struct fman_state_struct *state;
++
++	struct fman_cfg *cfg;
++	struct muram_info *muram;
++	struct fman_keygen *keygen;
++	/* cam section in muram */
++	unsigned long cam_offset;
++	size_t cam_size;
++	/* Fifo in MURAM */
++	unsigned long fifo_offset;
++	size_t fifo_size;
++
++	u32 liodn_base[64];
++	u32 liodn_offset[64];
++
++	struct fman_dts_params dts_params;
++};
++
+ /* Structure for port-FM communication during fman_port_init. */
+ struct fman_port_init_params {
+ 	u8 port_id;			/* port Id */
+diff --git a/drivers/net/ethernet/freescale/fman/fman_dtsec.c b/drivers/net/ethernet/freescale/fman/fman_dtsec.c
+index 98bba10f..57b1e2b4 100644
+--- a/drivers/net/ethernet/freescale/fman/fman_dtsec.c
++++ b/drivers/net/ethernet/freescale/fman/fman_dtsec.c
+@@ -123,7 +123,7 @@
+ #define DTSEC_ECNTRL_R100M		0x00000008
+ #define DTSEC_ECNTRL_QSGMIIM		0x00000001
+ 
+-#define DTSEC_TCTRL_GTS			0x00000020
++#define TCTRL_GTS			0x00000020
+ 
+ #define RCTRL_PAL_MASK			0x001f0000
+ #define RCTRL_PAL_SHIFT			16
+@@ -863,6 +863,52 @@ int dtsec_cfg_pad_and_crc(struct fman_mac *dtsec, bool new_val)
+ 	return 0;
+ }
+ 
++static void graceful_start(struct fman_mac *dtsec, enum comm_mode mode)
++{
++	struct dtsec_regs __iomem *regs = dtsec->regs;
++
++	if (mode & COMM_MODE_TX)
++		iowrite32be(ioread32be(&regs->tctrl) &
++				~TCTRL_GTS, &regs->tctrl);
++	if (mode & COMM_MODE_RX)
++		iowrite32be(ioread32be(&regs->rctrl) &
++				~RCTRL_GRS, &regs->rctrl);
++}
++
++static void graceful_stop(struct fman_mac *dtsec, enum comm_mode mode)
++{
++	struct dtsec_regs __iomem *regs = dtsec->regs;
++	u32 tmp;
++
++	/* Graceful stop - Assert the graceful Rx stop bit */
++	if (mode & COMM_MODE_RX) {
++		tmp = ioread32be(&regs->rctrl) | RCTRL_GRS;
++		iowrite32be(tmp, &regs->rctrl);
++
++		if (dtsec->fm_rev_info.major == 2) {
++			/* Workaround for dTSEC Errata A002 */
++			usleep_range(100, 200);
++		} else {
++			/* Workaround for dTSEC Errata A004839 */
++			usleep_range(10, 50);
++		}
++	}
++
++	/* Graceful stop - Assert the graceful Tx stop bit */
++	if (mode & COMM_MODE_TX) {
++		if (dtsec->fm_rev_info.major == 2) {
++			/* dTSEC Errata A004: Do not use TCTRL[GTS]=1 */
++			pr_debug("GTS not supported due to DTSEC_A004 Errata.\n");
++		} else {
++			tmp = ioread32be(&regs->tctrl) | TCTRL_GTS;
++			iowrite32be(tmp, &regs->tctrl);
++
++			/* Workaround for dTSEC Errata A0012, A0014 */
++			usleep_range(10, 50);
++		}
++	}
++}
++
+ int dtsec_enable(struct fman_mac *dtsec, enum comm_mode mode)
+ {
+ 	struct dtsec_regs __iomem *regs = dtsec->regs;
+@@ -880,13 +926,8 @@ int dtsec_enable(struct fman_mac *dtsec, enum comm_mode mode)
+ 
+ 	iowrite32be(tmp, &regs->maccfg1);
+ 
+-	/* Graceful start - clear the graceful receive stop bit */
+-	if (mode & COMM_MODE_TX)
+-		iowrite32be(ioread32be(&regs->tctrl) & ~DTSEC_TCTRL_GTS,
+-			    &regs->tctrl);
+-	if (mode & COMM_MODE_RX)
+-		iowrite32be(ioread32be(&regs->rctrl) & ~RCTRL_GRS,
+-			    &regs->rctrl);
++	/* Graceful start - clear the graceful Rx/Tx stop bit */
++	graceful_start(dtsec, mode);
+ 
+ 	return 0;
+ }
+@@ -899,23 +940,8 @@ int dtsec_disable(struct fman_mac *dtsec, enum comm_mode mode)
+ 	if (!is_init_done(dtsec->dtsec_drv_param))
+ 		return -EINVAL;
+ 
+-	/* Gracefull stop - Assert the graceful transmit stop bit */
+-	if (mode & COMM_MODE_RX) {
+-		tmp = ioread32be(&regs->rctrl) | RCTRL_GRS;
+-		iowrite32be(tmp, &regs->rctrl);
+-
+-		if (dtsec->fm_rev_info.major == 2)
+-			usleep_range(100, 200);
+-		else
+-			udelay(10);
+-	}
+-
+-	if (mode & COMM_MODE_TX) {
+-		if (dtsec->fm_rev_info.major == 2)
+-			pr_debug("GTS not supported due to DTSEC_A004 errata.\n");
+-		else
+-			pr_debug("GTS not supported due to DTSEC_A0014 errata.\n");
+-	}
++	/* Graceful stop - Assert the graceful Rx/Tx stop bit */
++	graceful_stop(dtsec, mode);
+ 
+ 	tmp = ioread32be(&regs->maccfg1);
+ 	if (mode & COMM_MODE_RX)
+@@ -933,11 +959,19 @@ int dtsec_set_tx_pause_frames(struct fman_mac *dtsec,
+ 			      u16 pause_time, u16 __maybe_unused thresh_time)
+ {
+ 	struct dtsec_regs __iomem *regs = dtsec->regs;
++	enum comm_mode mode = COMM_MODE_NONE;
+ 	u32 ptv = 0;
+ 
+ 	if (!is_init_done(dtsec->dtsec_drv_param))
+ 		return -EINVAL;
+ 
++	if ((ioread32be(&regs->rctrl) & RCTRL_GRS) == 0)
++		mode |= COMM_MODE_RX;
++	if ((ioread32be(&regs->tctrl) & TCTRL_GTS) == 0)
++		mode |= COMM_MODE_TX;
++
++	graceful_stop(dtsec, mode);
++
+ 	if (pause_time) {
+ 		/* FM_BAD_TX_TS_IN_B_2_B_ERRATA_DTSEC_A003 Errata workaround */
+ 		if (dtsec->fm_rev_info.major == 2 && pause_time <= 320) {
+@@ -958,17 +992,27 @@ int dtsec_set_tx_pause_frames(struct fman_mac *dtsec,
+ 		iowrite32be(ioread32be(&regs->maccfg1) & ~MACCFG1_TX_FLOW,
+ 			    &regs->maccfg1);
+ 
++	graceful_start(dtsec, mode);
++
+ 	return 0;
+ }
+ 
+ int dtsec_accept_rx_pause_frames(struct fman_mac *dtsec, bool en)
+ {
+ 	struct dtsec_regs __iomem *regs = dtsec->regs;
++	enum comm_mode mode = COMM_MODE_NONE;
+ 	u32 tmp;
+ 
+ 	if (!is_init_done(dtsec->dtsec_drv_param))
+ 		return -EINVAL;
+ 
++	if ((ioread32be(&regs->rctrl) & RCTRL_GRS) == 0)
++		mode |= COMM_MODE_RX;
++	if ((ioread32be(&regs->tctrl) & TCTRL_GTS) == 0)
++		mode |= COMM_MODE_TX;
++
++	graceful_stop(dtsec, mode);
++
+ 	tmp = ioread32be(&regs->maccfg1);
+ 	if (en)
+ 		tmp |= MACCFG1_RX_FLOW;
+@@ -976,20 +1020,34 @@ int dtsec_accept_rx_pause_frames(struct fman_mac *dtsec, bool en)
+ 		tmp &= ~MACCFG1_RX_FLOW;
+ 	iowrite32be(tmp, &regs->maccfg1);
+ 
++	graceful_start(dtsec, mode);
++
+ 	return 0;
+ }
+ 
+ int dtsec_modify_mac_address(struct fman_mac *dtsec, enet_addr_t *enet_addr)
+ {
++	struct dtsec_regs __iomem *regs = dtsec->regs;
++	enum comm_mode mode = COMM_MODE_NONE;
++
+ 	if (!is_init_done(dtsec->dtsec_drv_param))
+ 		return -EINVAL;
+ 
++	if ((ioread32be(&regs->rctrl) & RCTRL_GRS) == 0)
++		mode |= COMM_MODE_RX;
++	if ((ioread32be(&regs->tctrl) & TCTRL_GTS) == 0)
++		mode |= COMM_MODE_TX;
++
++	graceful_stop(dtsec, mode);
++
+ 	/* Initialize MAC Station Address registers (1 & 2)
+ 	 * Station address have to be swapped (big endian to little endian
+ 	 */
+ 	dtsec->addr = ENET_ADDR_TO_UINT64(*enet_addr);
+ 	set_mac_address(dtsec->regs, (u8 *)(*enet_addr));
+ 
++	graceful_start(dtsec, mode);
++
+ 	return 0;
+ }
+ 
+@@ -1042,7 +1100,7 @@ int dtsec_add_hash_mac_address(struct fman_mac *dtsec, enet_addr_t *eth_addr)
+ 	set_bucket(dtsec->regs, bucket, true);
+ 
+ 	/* Create element to be added to the driver hash table */
+-	hash_entry = kmalloc(sizeof(*hash_entry), GFP_KERNEL);
++	hash_entry = kmalloc(sizeof(*hash_entry), GFP_ATOMIC);
+ 	if (!hash_entry)
+ 		return -ENOMEM;
+ 	hash_entry->addr = addr;
+@@ -1059,6 +1117,25 @@ int dtsec_add_hash_mac_address(struct fman_mac *dtsec, enet_addr_t *eth_addr)
+ 	return 0;
+ }
+ 
++int dtsec_set_allmulti(struct fman_mac *dtsec, bool enable)
++{
++	u32 tmp;
++	struct dtsec_regs __iomem *regs = dtsec->regs;
++
++	if (!is_init_done(dtsec->dtsec_drv_param))
++		return -EINVAL;
++
++	tmp = ioread32be(&regs->rctrl);
++	if (enable)
++		tmp |= RCTRL_MPROM;
++	else
++		tmp &= ~RCTRL_MPROM;
++
++	iowrite32be(tmp, &regs->rctrl);
++
++	return 0;
++}
++
+ int dtsec_del_hash_mac_address(struct fman_mac *dtsec, enet_addr_t *eth_addr)
+ {
+ 	struct dtsec_regs __iomem *regs = dtsec->regs;
+@@ -1162,11 +1239,19 @@ int dtsec_set_promiscuous(struct fman_mac *dtsec, bool new_val)
+ int dtsec_adjust_link(struct fman_mac *dtsec, u16 speed)
+ {
+ 	struct dtsec_regs __iomem *regs = dtsec->regs;
++	enum comm_mode mode = COMM_MODE_NONE;
+ 	u32 tmp;
+ 
+ 	if (!is_init_done(dtsec->dtsec_drv_param))
+ 		return -EINVAL;
+ 
++	if ((ioread32be(&regs->rctrl) & RCTRL_GRS) == 0)
++		mode |= COMM_MODE_RX;
++	if ((ioread32be(&regs->tctrl) & TCTRL_GTS) == 0)
++		mode |= COMM_MODE_TX;
++
++	graceful_stop(dtsec, mode);
++
+ 	tmp = ioread32be(&regs->maccfg2);
+ 
+ 	/* Full Duplex */
+@@ -1186,6 +1271,8 @@ int dtsec_adjust_link(struct fman_mac *dtsec, u16 speed)
+ 		tmp &= ~DTSEC_ECNTRL_R100M;
+ 	iowrite32be(tmp, &regs->ecntrl);
+ 
++	graceful_start(dtsec, mode);
++
+ 	return 0;
+ }
+ 
+diff --git a/drivers/net/ethernet/freescale/fman/fman_dtsec.h b/drivers/net/ethernet/freescale/fman/fman_dtsec.h
+index c4467c07..1a689adf 100644
+--- a/drivers/net/ethernet/freescale/fman/fman_dtsec.h
++++ b/drivers/net/ethernet/freescale/fman/fman_dtsec.h
+@@ -55,5 +55,6 @@ int dtsec_set_exception(struct fman_mac *dtsec,
+ int dtsec_add_hash_mac_address(struct fman_mac *dtsec, enet_addr_t *eth_addr);
+ int dtsec_del_hash_mac_address(struct fman_mac *dtsec, enet_addr_t *eth_addr);
+ int dtsec_get_version(struct fman_mac *dtsec, u32 *mac_version);
++int dtsec_set_allmulti(struct fman_mac *dtsec, bool enable);
+ 
+ #endif /* __DTSEC_H */
+diff --git a/drivers/net/ethernet/freescale/fman/fman_keygen.c b/drivers/net/ethernet/freescale/fman/fman_keygen.c
+new file mode 100644
+index 00000000..f54da3c6
+--- /dev/null
++++ b/drivers/net/ethernet/freescale/fman/fman_keygen.c
+@@ -0,0 +1,783 @@
++/*
++ * Copyright 2017 NXP
++ *
++ * Redistribution and use in source and binary forms, with or without
++ * modification, are permitted provided that the following conditions are met:
++ *     * Redistributions of source code must retain the above copyright
++ *       notice, this list of conditions and the following disclaimer.
++ *     * Redistributions in binary form must reproduce the above copyright
++ *       notice, this list of conditions and the following disclaimer in the
++ *       documentation and/or other materials provided with the distribution.
++ *     * Neither the name of NXP nor the
++ *       names of its contributors may be used to endorse or promote products
++ *       derived from this software without specific prior written permission.
++ *
++ *
++ * ALTERNATIVELY, this software may be distributed under the terms of the
++ * GNU General Public License ("GPL") as published by the Free Software
++ * Foundation, either version 2 of that License or (at your option) any
++ * later version.
++ *
++ * THIS SOFTWARE IS PROVIDED BY NXP ``AS IS'' AND ANY
++ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
++ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
++ * DISCLAIMED. IN NO EVENT SHALL NXP BE LIABLE FOR ANY
++ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
++ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
++ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
++ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
++ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
++ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
++ */
++
++#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
++
++#include <linux/slab.h>
++
++#include "fman_keygen.h"
++
++/* Maximum number of HW Ports */
++#define FMAN_MAX_NUM_OF_HW_PORTS		64
++
++/* Maximum number of KeyGen Schemes */
++#define FM_KG_MAX_NUM_OF_SCHEMES		32
++
++/* Number of generic KeyGen Generic Extract Command Registers */
++#define FM_KG_NUM_OF_GENERIC_REGS		8
++
++/* Dummy port ID */
++#define DUMMY_PORT_ID				0
++
++/* Select Scheme Value Register */
++#define KG_SCH_DEF_USE_KGSE_DV_0		2
++#define KG_SCH_DEF_USE_KGSE_DV_1		3
++
++/* Registers Shifting values */
++#define FM_KG_KGAR_NUM_SHIFT			16
++#define KG_SCH_DEF_L4_PORT_SHIFT		8
++#define KG_SCH_DEF_IP_ADDR_SHIFT		18
++#define KG_SCH_HASH_CONFIG_SHIFT_SHIFT		24
++
++/* KeyGen Registers bit field masks: */
++
++/* Enable bit field mask for KeyGen General Configuration Register */
++#define FM_KG_KGGCR_EN				0x80000000
++
++/* KeyGen Global Registers bit field masks */
++#define FM_KG_KGAR_GO				0x80000000
++#define FM_KG_KGAR_READ				0x40000000
++#define FM_KG_KGAR_WRITE			0x00000000
++#define FM_KG_KGAR_SEL_SCHEME_ENTRY		0x00000000
++#define FM_KG_KGAR_SCM_WSEL_UPDATE_CNT		0x00008000
++
++#define FM_KG_KGAR_ERR				0x20000000
++#define FM_KG_KGAR_SEL_CLS_PLAN_ENTRY		0x01000000
++#define FM_KG_KGAR_SEL_PORT_ENTRY		0x02000000
++#define FM_KG_KGAR_SEL_PORT_WSEL_SP		0x00008000
++#define FM_KG_KGAR_SEL_PORT_WSEL_CPP		0x00004000
++
++/* Error events exceptions */
++#define FM_EX_KG_DOUBLE_ECC			0x80000000
++#define FM_EX_KG_KEYSIZE_OVERFLOW		0x40000000
++
++/* Scheme Registers bit field masks */
++#define KG_SCH_MODE_EN				0x80000000
++#define KG_SCH_VSP_NO_KSP_EN			0x80000000
++#define KG_SCH_HASH_CONFIG_SYM			0x40000000
++
++/* Known Protocol field codes */
++#define KG_SCH_KN_PORT_ID		0x80000000
++#define KG_SCH_KN_MACDST		0x40000000
++#define KG_SCH_KN_MACSRC		0x20000000
++#define KG_SCH_KN_TCI1			0x10000000
++#define KG_SCH_KN_TCI2			0x08000000
++#define KG_SCH_KN_ETYPE			0x04000000
++#define KG_SCH_KN_PPPSID		0x02000000
++#define KG_SCH_KN_PPPID			0x01000000
++#define KG_SCH_KN_MPLS1			0x00800000
++#define KG_SCH_KN_MPLS2			0x00400000
++#define KG_SCH_KN_MPLS_LAST		0x00200000
++#define KG_SCH_KN_IPSRC1		0x00100000
++#define KG_SCH_KN_IPDST1		0x00080000
++#define KG_SCH_KN_PTYPE1		0x00040000
++#define KG_SCH_KN_IPTOS_TC1		0x00020000
++#define KG_SCH_KN_IPV6FL1		0x00010000
++#define KG_SCH_KN_IPSRC2		0x00008000
++#define KG_SCH_KN_IPDST2		0x00004000
++#define KG_SCH_KN_PTYPE2		0x00002000
++#define KG_SCH_KN_IPTOS_TC2		0x00001000
++#define KG_SCH_KN_IPV6FL2		0x00000800
++#define KG_SCH_KN_GREPTYPE		0x00000400
++#define KG_SCH_KN_IPSEC_SPI		0x00000200
++#define KG_SCH_KN_IPSEC_NH		0x00000100
++#define KG_SCH_KN_IPPID			0x00000080
++#define KG_SCH_KN_L4PSRC		0x00000004
++#define KG_SCH_KN_L4PDST		0x00000002
++#define KG_SCH_KN_TFLG			0x00000001
++
++/* NIA values */
++#define NIA_ENG_BMI			0x00500000
++#define NIA_BMI_AC_ENQ_FRAME		0x00000002
++#define ENQUEUE_KG_DFLT_NIA		(NIA_ENG_BMI | NIA_BMI_AC_ENQ_FRAME)
++
++/* Hard-coded configuration:
++ * These values are used as hard-coded values for KeyGen configuration
++ * and they replace user selections for this hard-coded version
++ */
++
++/* Hash distribution shift */
++#define DEFAULT_HASH_DIST_FQID_SHIFT		0
++
++/* Hash shift */
++#define DEFAULT_HASH_SHIFT			0
++
++/* Symmetric hash usage:
++ * Warning:
++ * - the value for symmetric hash usage must be in accordance with hash
++ *	key defined below
++ * - according to tests performed, spreading is not working if symmetric
++ *	hash is set on true
++ * So ultimately symmetric hash functionality should be always disabled:
++ */
++#define DEFAULT_SYMMETRIC_HASH			false
++
++/* Hash Key extraction fields: */
++#define DEFAULT_HASH_KEY_EXTRACT_FIELDS		\
++	(KG_SCH_KN_IPSRC1 | KG_SCH_KN_IPDST1 | \
++	    KG_SCH_KN_L4PSRC | KG_SCH_KN_L4PDST)
++
++/* Default values to be used as hash key in case IPv4 or L4 (TCP, UDP)
++ * don't exist in the frame
++ */
++/* Default IPv4 address */
++#define DEFAULT_HASH_KEY_IPv4_ADDR		0x0A0A0A0A
++/* Default L4 port */
++#define DEFAULT_HASH_KEY_L4_PORT		0x0B0B0B0B
++
++/* KeyGen Memory Mapped Registers: */
++
++/* Scheme Configuration RAM Registers */
++struct fman_kg_scheme_regs {
++	u32 kgse_mode;		/* 0x100: MODE */
++	u32 kgse_ekfc;		/* 0x104: Extract Known Fields Command */
++	u32 kgse_ekdv;		/* 0x108: Extract Known Default Value */
++	u32 kgse_bmch;		/* 0x10C: Bit Mask Command High */
++	u32 kgse_bmcl;		/* 0x110: Bit Mask Command Low */
++	u32 kgse_fqb;		/* 0x114: Frame Queue Base */
++	u32 kgse_hc;		/* 0x118: Hash Command */
++	u32 kgse_ppc;		/* 0x11C: Policer Profile Command */
++	u32 kgse_gec[FM_KG_NUM_OF_GENERIC_REGS];
++			/* 0x120: Generic Extract Command */
++	u32 kgse_spc;
++		/* 0x140: KeyGen Scheme Entry Statistic Packet Counter */
++	u32 kgse_dv0;	/* 0x144: KeyGen Scheme Entry Default Value 0 */
++	u32 kgse_dv1;	/* 0x148: KeyGen Scheme Entry Default Value 1 */
++	u32 kgse_ccbs;
++		/* 0x14C: KeyGen Scheme Entry Coarse Classification Bit*/
++	u32 kgse_mv;	/* 0x150: KeyGen Scheme Entry Match vector */
++	u32 kgse_om;	/* 0x154: KeyGen Scheme Entry Operation Mode bits */
++	u32 kgse_vsp;
++		/* 0x158: KeyGen Scheme Entry Virtual Storage Profile */
++};
++
++/* Port Partition Configuration Registers */
++struct fman_kg_pe_regs {
++	u32 fmkg_pe_sp;		/* 0x100: KeyGen Port entry Scheme Partition */
++	u32 fmkg_pe_cpp;
++		/* 0x104: KeyGen Port Entry Classification Plan Partition */
++};
++
++/* General Configuration and Status Registers
++ * Global Statistic Counters
++ * KeyGen Global Registers
++ */
++struct fman_kg_regs {
++	u32 fmkg_gcr;	/* 0x000: KeyGen General Configuration Register */
++	u32 res004;	/* 0x004: Reserved */
++	u32 res008;	/* 0x008: Reserved */
++	u32 fmkg_eer;	/* 0x00C: KeyGen Error Event Register */
++	u32 fmkg_eeer;	/* 0x010: KeyGen Error Event Enable Register */
++	u32 res014;	/* 0x014: Reserved */
++	u32 res018;	/* 0x018: Reserved */
++	u32 fmkg_seer;	/* 0x01C: KeyGen Scheme Error Event Register */
++	u32 fmkg_seeer;	/* 0x020: KeyGen Scheme Error Event Enable Register */
++	u32 fmkg_gsr;	/* 0x024: KeyGen Global Status Register */
++	u32 fmkg_tpc;	/* 0x028: Total Packet Counter Register */
++	u32 fmkg_serc;	/* 0x02C: Soft Error Capture Register */
++	u32 res030[4];	/* 0x030: Reserved */
++	u32 fmkg_fdor;	/* 0x034: Frame Data Offset Register */
++	u32 fmkg_gdv0r;	/* 0x038: Global Default Value Register 0 */
++	u32 fmkg_gdv1r;	/* 0x03C: Global Default Value Register 1 */
++	u32 res04c[6];	/* 0x040: Reserved */
++	u32 fmkg_feer;	/* 0x044: Force Error Event Register */
++	u32 res068[38];	/* 0x048: Reserved */
++	union {
++		u32 fmkg_indirect[63];	/* 0x100: Indirect Access Registers */
++		struct fman_kg_scheme_regs fmkg_sch; /* Scheme Registers */
++		struct fman_kg_pe_regs fmkg_pe; /* Port Partition Registers */
++	};
++	u32 fmkg_ar;	/* 0x1FC: KeyGen Action Register */
++};
++
++/* KeyGen Scheme data */
++struct keygen_scheme {
++	bool used;	/* Specifies if this scheme is used */
++	u8 hw_port_id;
++		/* Hardware port ID
++		 * schemes sharing between multiple ports is not
++		 * currently supported
++		 * so we have only one port id bound to a scheme
++		 */
++	u32 base_fqid;
++		/* Base FQID:
++		 * Must be between 1 and 2^24-1
++		 * If hash is used and an even distribution is
++		 * expected according to hash_fqid_count,
++		 * base_fqid must be aligned to hash_fqid_count
++		 */
++	u32 hash_fqid_count;
++		/* FQ range for hash distribution:
++		 * Must be a power of 2
++		 * Represents the range of queues for spreading
++		 */
++	bool use_hashing;	/* Usage of Hashing and spreading over FQ */
++	bool symmetric_hash;	/* Symmetric Hash option usage */
++	u8 hashShift;
++		/* Hash result right shift.
++		 * Select the 24 bits out of the 64 hash result.
++		 * 0 means using the 24 LSB's, otherwise
++		 * use the 24 LSB's after shifting right
++		 */
++	u32 match_vector;	/* Match Vector */
++};
++
++/* KeyGen driver data */
++struct fman_keygen {
++	struct keygen_scheme schemes[FM_KG_MAX_NUM_OF_SCHEMES];
++				/* Array of schemes */
++	struct fman_kg_regs __iomem *keygen_regs;	/* KeyGen registers */
++};
++
++/* keygen_write_ar_wait
++ *
++ * Write Action Register with specified value, wait for GO bit field to be
++ * idle and then read the error
++ *
++ * regs: KeyGen registers
++ * fmkg_ar: Action Register value
++ *
++ * Return: Zero for success or error code in case of failure
++ */
++static int keygen_write_ar_wait(struct fman_kg_regs __iomem *regs, u32 fmkg_ar)
++{
++	iowrite32be(fmkg_ar, &regs->fmkg_ar);
++
++	/* Wait for GO bit field to be idle */
++	while (fmkg_ar & FM_KG_KGAR_GO)
++		fmkg_ar = ioread32be(&regs->fmkg_ar);
++
++	if (fmkg_ar & FM_KG_KGAR_ERR)
++		return -EINVAL;
++
++	return 0;
++}
++
++/* build_ar_scheme
++ *
++ * Build Action Register value for scheme settings
++ *
++ * scheme_id: Scheme ID
++ * update_counter: update scheme counter
++ * write: true for action to write the scheme or false for read action
++ *
++ * Return: AR value
++ */
++static u32 build_ar_scheme(u8 scheme_id, bool update_counter, bool write)
++{
++	u32 rw = (u32)(write ? FM_KG_KGAR_WRITE : FM_KG_KGAR_READ);
++
++	return (u32)(FM_KG_KGAR_GO |
++			rw |
++			FM_KG_KGAR_SEL_SCHEME_ENTRY |
++			DUMMY_PORT_ID |
++			((u32)scheme_id << FM_KG_KGAR_NUM_SHIFT) |
++			(update_counter ? FM_KG_KGAR_SCM_WSEL_UPDATE_CNT : 0));
++}
++
++/* build_ar_bind_scheme
++ *
++ * Build Action Register value for port binding to schemes
++ *
++ * hwport_id: HW Port ID
++ * write: true for action to write the bind or false for read action
++ *
++ * Return: AR value
++ */
++static u32 build_ar_bind_scheme(u8 hwport_id, bool write)
++{
++	u32 rw = write ? (u32)FM_KG_KGAR_WRITE : (u32)FM_KG_KGAR_READ;
++
++	return (u32)(FM_KG_KGAR_GO |
++			rw |
++			FM_KG_KGAR_SEL_PORT_ENTRY |
++			hwport_id |
++			FM_KG_KGAR_SEL_PORT_WSEL_SP);
++}
++
++/* keygen_write_sp
++ *
++ * Write Scheme Partition Register with specified value
++ *
++ * regs: KeyGen Registers
++ * sp: Scheme Partition register value
++ * add: true to add a scheme partition or false to clear
++ *
++ * Return: none
++ */
++static void keygen_write_sp(struct fman_kg_regs __iomem *regs, u32 sp, bool add)
++{
++	u32 tmp;
++
++	tmp = ioread32be(&regs->fmkg_pe.fmkg_pe_sp);
++
++	if (add)
++		tmp |= sp;
++	else
++		tmp &= ~sp;
++
++	iowrite32be(tmp, &regs->fmkg_pe.fmkg_pe_sp);
++}
++
++/* build_ar_bind_cls_plan
++ *
++ * Build Action Register value for Classification Plan
++ *
++ * hwport_id: HW Port ID
++ * write: true for action to write the CP or false for read action
++ *
++ * Return: AR value
++ */
++static u32 build_ar_bind_cls_plan(u8 hwport_id, bool write)
++{
++	u32 rw = write ? (u32)FM_KG_KGAR_WRITE : (u32)FM_KG_KGAR_READ;
++
++	return (u32)(FM_KG_KGAR_GO |
++			rw |
++			FM_KG_KGAR_SEL_PORT_ENTRY |
++			hwport_id |
++			FM_KG_KGAR_SEL_PORT_WSEL_CPP);
++}
++
++/* keygen_write_cpp
++ *
++ * Write Classification Plan Partition Register with specified value
++ *
++ * regs: KeyGen Registers
++ * cpp: CPP register value
++ *
++ * Return: none
++ */
++static void keygen_write_cpp(struct fman_kg_regs __iomem *regs, u32 cpp)
++{
++	iowrite32be(cpp, &regs->fmkg_pe.fmkg_pe_cpp);
++}
++
++/* keygen_write_scheme
++ *
++ * Write all Schemes Registers with specified values
++ *
++ * regs: KeyGen Registers
++ * scheme_id: Scheme ID
++ * scheme_regs: Scheme registers values desired to be written
++ * update_counter: update scheme counter
++ *
++ * Return: Zero for success or error code in case of failure
++ */
++static int keygen_write_scheme(struct fman_kg_regs __iomem *regs, u8 scheme_id,
++			       struct fman_kg_scheme_regs *scheme_regs,
++				bool update_counter)
++{
++	u32 ar_reg;
++	int err, i;
++
++	/* Write indirect scheme registers */
++	iowrite32be(scheme_regs->kgse_mode, &regs->fmkg_sch.kgse_mode);
++	iowrite32be(scheme_regs->kgse_ekfc, &regs->fmkg_sch.kgse_ekfc);
++	iowrite32be(scheme_regs->kgse_ekdv, &regs->fmkg_sch.kgse_ekdv);
++	iowrite32be(scheme_regs->kgse_bmch, &regs->fmkg_sch.kgse_bmch);
++	iowrite32be(scheme_regs->kgse_bmcl, &regs->fmkg_sch.kgse_bmcl);
++	iowrite32be(scheme_regs->kgse_fqb, &regs->fmkg_sch.kgse_fqb);
++	iowrite32be(scheme_regs->kgse_hc, &regs->fmkg_sch.kgse_hc);
++	iowrite32be(scheme_regs->kgse_ppc, &regs->fmkg_sch.kgse_ppc);
++	iowrite32be(scheme_regs->kgse_spc, &regs->fmkg_sch.kgse_spc);
++	iowrite32be(scheme_regs->kgse_dv0, &regs->fmkg_sch.kgse_dv0);
++	iowrite32be(scheme_regs->kgse_dv1, &regs->fmkg_sch.kgse_dv1);
++	iowrite32be(scheme_regs->kgse_ccbs, &regs->fmkg_sch.kgse_ccbs);
++	iowrite32be(scheme_regs->kgse_mv, &regs->fmkg_sch.kgse_mv);
++	iowrite32be(scheme_regs->kgse_om, &regs->fmkg_sch.kgse_om);
++	iowrite32be(scheme_regs->kgse_vsp, &regs->fmkg_sch.kgse_vsp);
++
++	for (i = 0 ; i < FM_KG_NUM_OF_GENERIC_REGS ; i++)
++		iowrite32be(scheme_regs->kgse_gec[i],
++			    &regs->fmkg_sch.kgse_gec[i]);
++
++	/* Write AR (Action register) */
++	ar_reg = build_ar_scheme(scheme_id, update_counter, true);
++	err = keygen_write_ar_wait(regs, ar_reg);
++	if (err != 0) {
++		pr_err("Writing Action Register failed\n");
++		return err;
++	}
++
++	return err;
++}
++
++/* get_free_scheme_id
++ *
++ * Find the first free scheme available to be used
++ *
++ * keygen: KeyGen handle
++ * scheme_id: pointer to scheme id
++ *
++ * Return: 0 on success, -EINVAL when the are no available free schemes
++ */
++static int get_free_scheme_id(struct fman_keygen *keygen, u8 *scheme_id)
++{
++	u8 i;
++
++	for (i = 0; i < FM_KG_MAX_NUM_OF_SCHEMES; i++)
++		if (!keygen->schemes[i].used) {
++			*scheme_id = i;
++			return 0;
++		}
++
++	return -EINVAL;
++}
++
++/* get_scheme
++ *
++ * Provides the scheme for specified ID
++ *
++ * keygen: KeyGen handle
++ * scheme_id: Scheme ID
++ *
++ * Return: handle to required scheme
++ */
++static struct keygen_scheme *get_scheme(struct fman_keygen *keygen,
++					u8 scheme_id)
++{
++	if (scheme_id >= FM_KG_MAX_NUM_OF_SCHEMES)
++		return NULL;
++	return &keygen->schemes[scheme_id];
++}
++
++/* keygen_bind_port_to_schemes
++ *
++ * Bind the port to schemes
++ *
++ * keygen: KeyGen handle
++ * scheme_id: id of the scheme to bind to
++ * bind: true to bind the port or false to unbind it
++ *
++ * Return: Zero for success or error code in case of failure
++ */
++static int keygen_bind_port_to_schemes(struct fman_keygen *keygen,
++				       u8 scheme_id,
++					bool bind)
++{
++	struct fman_kg_regs __iomem *keygen_regs = keygen->keygen_regs;
++	struct keygen_scheme *scheme;
++	u32 ar_reg;
++	u32 schemes_vector = 0;
++	int err;
++
++	scheme = get_scheme(keygen, scheme_id);
++	if (!scheme) {
++		pr_err("Requested Scheme does not exist\n");
++		return -EINVAL;
++	}
++	if (!scheme->used) {
++		pr_err("Cannot bind port to an invalid scheme\n");
++		return -EINVAL;
++	}
++
++	schemes_vector |= 1 << (31 - scheme_id);
++
++	ar_reg = build_ar_bind_scheme(scheme->hw_port_id, false);
++	err = keygen_write_ar_wait(keygen_regs, ar_reg);
++	if (err != 0) {
++		pr_err("Reading Action Register failed\n");
++		return err;
++	}
++
++	keygen_write_sp(keygen_regs, schemes_vector, bind);
++
++	ar_reg = build_ar_bind_scheme(scheme->hw_port_id, true);
++	err = keygen_write_ar_wait(keygen_regs, ar_reg);
++	if (err != 0) {
++		pr_err("Writing Action Register failed\n");
++		return err;
++	}
++
++	return 0;
++}
++
++/* keygen_scheme_setup
++ *
++ * Setup the scheme according to required configuration
++ *
++ * keygen: KeyGen handle
++ * scheme_id: scheme ID
++ * enable: true to enable scheme or false to disable it
++ *
++ * Return: Zero for success or error code in case of failure
++ */
++static int keygen_scheme_setup(struct fman_keygen *keygen, u8 scheme_id,
++			       bool enable)
++{
++	struct fman_kg_regs __iomem *keygen_regs = keygen->keygen_regs;
++	struct fman_kg_scheme_regs scheme_regs;
++	struct keygen_scheme *scheme;
++	u32 tmp_reg;
++	int err;
++
++	scheme = get_scheme(keygen, scheme_id);
++	if (!scheme) {
++		pr_err("Requested Scheme does not exist\n");
++		return -EINVAL;
++	}
++	if (enable && scheme->used) {
++		pr_err("The requested Scheme is already used\n");
++		return -EINVAL;
++	}
++
++	/* Clear scheme registers */
++	memset(&scheme_regs, 0, sizeof(struct fman_kg_scheme_regs));
++
++	/* Setup all scheme registers: */
++	tmp_reg = 0;
++
++	if (enable) {
++		/* Enable Scheme */
++		tmp_reg |= KG_SCH_MODE_EN;
++		/* Enqueue frame NIA */
++		tmp_reg |= ENQUEUE_KG_DFLT_NIA;
++	}
++
++	scheme_regs.kgse_mode = tmp_reg;
++
++	scheme_regs.kgse_mv = scheme->match_vector;
++
++	/* Scheme don't override StorageProfile:
++	 * valid only for DPAA_VERSION >= 11
++	 */
++	scheme_regs.kgse_vsp = KG_SCH_VSP_NO_KSP_EN;
++
++	/* Configure Hard-Coded Rx Hashing: */
++
++	if (scheme->use_hashing) {
++		/* configure kgse_ekfc */
++		scheme_regs.kgse_ekfc = DEFAULT_HASH_KEY_EXTRACT_FIELDS;
++
++		/* configure kgse_ekdv */
++		tmp_reg = 0;
++		tmp_reg |= (KG_SCH_DEF_USE_KGSE_DV_0 <<
++				KG_SCH_DEF_IP_ADDR_SHIFT);
++		tmp_reg |= (KG_SCH_DEF_USE_KGSE_DV_1 <<
++				KG_SCH_DEF_L4_PORT_SHIFT);
++		scheme_regs.kgse_ekdv = tmp_reg;
++
++		/* configure kgse_dv0 */
++		scheme_regs.kgse_dv0 = DEFAULT_HASH_KEY_IPv4_ADDR;
++		/* configure kgse_dv1 */
++		scheme_regs.kgse_dv1 = DEFAULT_HASH_KEY_L4_PORT;
++
++		/* configure kgse_hc  */
++		tmp_reg = 0;
++		tmp_reg |= ((scheme->hash_fqid_count - 1) <<
++				DEFAULT_HASH_DIST_FQID_SHIFT);
++		tmp_reg |= scheme->hashShift << KG_SCH_HASH_CONFIG_SHIFT_SHIFT;
++
++		if (scheme->symmetric_hash) {
++			/* Normally extraction key should be verified if
++			 * complies with symmetric hash
++			 * But because extraction is hard-coded, we are sure
++			 * the key is symmetric
++			 */
++			tmp_reg |= KG_SCH_HASH_CONFIG_SYM;
++		}
++		scheme_regs.kgse_hc = tmp_reg;
++	} else {
++		scheme_regs.kgse_ekfc = 0;
++		scheme_regs.kgse_hc = 0;
++		scheme_regs.kgse_ekdv = 0;
++		scheme_regs.kgse_dv0 = 0;
++		scheme_regs.kgse_dv1 = 0;
++	}
++
++	/* configure kgse_fqb: Scheme FQID base */
++	tmp_reg = 0;
++	tmp_reg |= scheme->base_fqid;
++	scheme_regs.kgse_fqb = tmp_reg;
++
++	/* features not used by hard-coded configuration */
++	scheme_regs.kgse_bmch = 0;
++	scheme_regs.kgse_bmcl = 0;
++	scheme_regs.kgse_spc = 0;
++
++	/* Write scheme registers */
++	err = keygen_write_scheme(keygen_regs, scheme_id, &scheme_regs, true);
++	if (err != 0) {
++		pr_err("Writing scheme registers failed\n");
++		return err;
++	}
++
++	/* Update used field for Scheme */
++	scheme->used = enable;
++
++	return 0;
++}
++
++/* keygen_init
++ *
++ * KeyGen initialization:
++ * Initializes and enables KeyGen, allocate driver memory, setup registers,
++ * clear port bindings, invalidate all schemes
++ *
++ * keygen_regs: KeyGen registers base address
++ *
++ * Return: Handle to KeyGen driver
++ */
++struct fman_keygen *keygen_init(struct fman_kg_regs __iomem *keygen_regs)
++{
++	struct fman_keygen *keygen;
++	u32 ar;
++	int i;
++
++	/* Allocate memory for KeyGen driver */
++	keygen = kzalloc(sizeof(*keygen), GFP_KERNEL);
++	if (!keygen)
++		return NULL;
++
++	keygen->keygen_regs = keygen_regs;
++
++	/* KeyGen initialization (for Master partition):
++	 * Setup KeyGen registers
++	 */
++	iowrite32be(ENQUEUE_KG_DFLT_NIA, &keygen_regs->fmkg_gcr);
++
++	iowrite32be(FM_EX_KG_DOUBLE_ECC | FM_EX_KG_KEYSIZE_OVERFLOW,
++		    &keygen_regs->fmkg_eer);
++
++	iowrite32be(0, &keygen_regs->fmkg_fdor);
++	iowrite32be(0, &keygen_regs->fmkg_gdv0r);
++	iowrite32be(0, &keygen_regs->fmkg_gdv1r);
++
++	/* Clear binding between ports to schemes and classification plans
++	 * so that all ports are not bound to any scheme/classification plan
++	 */
++	for (i = 0; i < FMAN_MAX_NUM_OF_HW_PORTS; i++) {
++		/* Clear all pe sp schemes registers */
++		keygen_write_sp(keygen_regs, 0xffffffff, false);
++		ar = build_ar_bind_scheme(i, true);
++		keygen_write_ar_wait(keygen_regs, ar);
++
++		/* Clear all pe cpp classification plans registers */
++		keygen_write_cpp(keygen_regs, 0);
++		ar = build_ar_bind_cls_plan(i, true);
++		keygen_write_ar_wait(keygen_regs, ar);
++	}
++
++	/* Enable all scheme interrupts */
++	iowrite32be(0xFFFFFFFF, &keygen_regs->fmkg_seer);
++	iowrite32be(0xFFFFFFFF, &keygen_regs->fmkg_seeer);
++
++	/* Enable KyeGen */
++	iowrite32be(ioread32be(&keygen_regs->fmkg_gcr) | FM_KG_KGGCR_EN,
++		    &keygen_regs->fmkg_gcr);
++
++	return keygen;
++}
++EXPORT_SYMBOL(keygen_init);
++
++/* keygen_port_hashing_init
++ *
++ * Initializes a port for Rx Hashing with specified configuration parameters
++ *
++ * keygen: KeyGen handle
++ * hw_port_id: HW Port ID
++ * hash_base_fqid: Hashing Base FQID used for spreading
++ * hash_size: Hashing size
++ *
++ * Return: Zero for success or error code in case of failure
++ */
++int keygen_port_hashing_init(struct fman_keygen *keygen, u8 hw_port_id,
++			     u32 hash_base_fqid, u32 hash_size)
++{
++	struct keygen_scheme *scheme;
++	u8 scheme_id;
++	int err;
++
++	/* Validate Scheme configuration parameters */
++	if (hash_base_fqid == 0 || (hash_base_fqid & ~0x00FFFFFF)) {
++		pr_err("Base FQID must be between 1 and 2^24-1\n");
++		return -EINVAL;
++	}
++	if (hash_size == 0 || (hash_size & (hash_size - 1)) != 0) {
++		pr_err("Hash size must be power of two\n");
++		return -EINVAL;
++	}
++
++	/* Find a free scheme */
++	err = get_free_scheme_id(keygen, &scheme_id);
++	if (err) {
++		pr_err("The maximum number of available Schemes has been exceeded\n");
++		return -EINVAL;
++	}
++
++	/* Create and configure Hard-Coded Scheme: */
++
++	scheme = get_scheme(keygen, scheme_id);
++	if (!scheme) {
++		pr_err("Requested Scheme does not exist\n");
++		return -EINVAL;
++	}
++	if (scheme->used) {
++		pr_err("The requested Scheme is already used\n");
++		return -EINVAL;
++	}
++
++	/* Clear all scheme fields because the scheme may have been
++	 * previously used
++	 */
++	memset(scheme, 0, sizeof(struct keygen_scheme));
++
++	/* Setup scheme: */
++	scheme->hw_port_id = hw_port_id;
++	scheme->use_hashing = true;
++	scheme->base_fqid = hash_base_fqid;
++	scheme->hash_fqid_count = hash_size;
++	scheme->symmetric_hash = DEFAULT_SYMMETRIC_HASH;
++	scheme->hashShift = DEFAULT_HASH_SHIFT;
++
++	/* All Schemes in hard-coded configuration
++	 * are Indirect Schemes
++	 */
++	scheme->match_vector = 0;
++
++	err = keygen_scheme_setup(keygen, scheme_id, true);
++	if (err != 0) {
++		pr_err("Scheme setup failed\n");
++		return err;
++	}
++
++	/* Bind Rx port to Scheme */
++	err = keygen_bind_port_to_schemes(keygen, scheme_id, true);
++	if (err != 0) {
++		pr_err("Binding port to schemes failed\n");
++		return err;
++	}
++
++	return 0;
++}
++EXPORT_SYMBOL(keygen_port_hashing_init);
+diff --git a/drivers/net/ethernet/freescale/fman/fman_keygen.h b/drivers/net/ethernet/freescale/fman/fman_keygen.h
+new file mode 100644
+index 00000000..c4640de3
+--- /dev/null
++++ b/drivers/net/ethernet/freescale/fman/fman_keygen.h
+@@ -0,0 +1,46 @@
++/*
++ * Copyright 2017 NXP
++ *
++ * Redistribution and use in source and binary forms, with or without
++ * modification, are permitted provided that the following conditions are met:
++ *     * Redistributions of source code must retain the above copyright
++ *       notice, this list of conditions and the following disclaimer.
++ *     * Redistributions in binary form must reproduce the above copyright
++ *       notice, this list of conditions and the following disclaimer in the
++ *       documentation and/or other materials provided with the distribution.
++ *     * Neither the name of NXP nor the
++ *       names of its contributors may be used to endorse or promote products
++ *       derived from this software without specific prior written permission.
++ *
++ *
++ * ALTERNATIVELY, this software may be distributed under the terms of the
++ * GNU General Public License ("GPL") as published by the Free Software
++ * Foundation, either version 2 of that License or (at your option) any
++ * later version.
++ *
++ * THIS SOFTWARE IS PROVIDED BY NXP ``AS IS'' AND ANY
++ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
++ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
++ * DISCLAIMED. IN NO EVENT SHALL NXP BE LIABLE FOR ANY
++ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
++ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
++ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
++ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
++ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
++ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
++ */
++
++#ifndef __KEYGEN_H
++#define __KEYGEN_H
++
++#include <linux/io.h>
++
++struct fman_keygen;
++struct fman_kg_regs;
++
++struct fman_keygen *keygen_init(struct fman_kg_regs __iomem *keygen_regs);
++
++int keygen_port_hashing_init(struct fman_keygen *keygen, u8 hw_port_id,
++			     u32 hash_base_fqid, u32 hash_size);
++
++#endif /* __KEYGEN_H */
+diff --git a/drivers/net/ethernet/freescale/fman/fman_memac.c b/drivers/net/ethernet/freescale/fman/fman_memac.c
+index c0296880..446a97b7 100644
+--- a/drivers/net/ethernet/freescale/fman/fman_memac.c
++++ b/drivers/net/ethernet/freescale/fman/fman_memac.c
+@@ -350,6 +350,7 @@ struct fman_mac {
+ 	struct fman_rev_info fm_rev_info;
+ 	bool basex_if;
+ 	struct phy_device *pcsphy;
++	bool allmulti_enabled;
+ };
+ 
+ static void add_addr_in_paddr(struct memac_regs __iomem *regs, u8 *adr,
+@@ -940,6 +941,29 @@ int memac_add_hash_mac_address(struct fman_mac *memac, enet_addr_t *eth_addr)
+ 	return 0;
+ }
+ 
++int memac_set_allmulti(struct fman_mac *memac, bool enable)
++{
++	u32 entry;
++	struct memac_regs __iomem *regs = memac->regs;
++
++	if (!is_init_done(memac->memac_drv_param))
++		return -EINVAL;
++
++	if (enable) {
++		for (entry = 0; entry < HASH_TABLE_SIZE; entry++)
++			iowrite32be(entry | HASH_CTRL_MCAST_EN,
++				    &regs->hashtable_ctrl);
++	} else {
++		for (entry = 0; entry < HASH_TABLE_SIZE; entry++)
++			iowrite32be(entry & ~HASH_CTRL_MCAST_EN,
++				    &regs->hashtable_ctrl);
++	}
++
++	memac->allmulti_enabled = enable;
++
++	return 0;
++}
++
+ int memac_del_hash_mac_address(struct fman_mac *memac, enet_addr_t *eth_addr)
+ {
+ 	struct memac_regs __iomem *regs = memac->regs;
+@@ -963,8 +987,12 @@ int memac_del_hash_mac_address(struct fman_mac *memac, enet_addr_t *eth_addr)
+ 			break;
+ 		}
+ 	}
+-	if (list_empty(&memac->multicast_addr_hash->lsts[hash]))
+-		iowrite32be(hash & ~HASH_CTRL_MCAST_EN, &regs->hashtable_ctrl);
++
++	if (!memac->allmulti_enabled) {
++		if (list_empty(&memac->multicast_addr_hash->lsts[hash]))
++			iowrite32be(hash & ~HASH_CTRL_MCAST_EN,
++				    &regs->hashtable_ctrl);
++	}
+ 
+ 	return 0;
+ }
+diff --git a/drivers/net/ethernet/freescale/fman/fman_memac.h b/drivers/net/ethernet/freescale/fman/fman_memac.h
+index 173d8e0f..fca0ddcd 100644
+--- a/drivers/net/ethernet/freescale/fman/fman_memac.h
++++ b/drivers/net/ethernet/freescale/fman/fman_memac.h
+@@ -56,5 +56,6 @@ int memac_set_exception(struct fman_mac *memac,
+ 			enum fman_mac_exceptions exception, bool enable);
+ int memac_add_hash_mac_address(struct fman_mac *memac, enet_addr_t *eth_addr);
+ int memac_del_hash_mac_address(struct fman_mac *memac, enet_addr_t *eth_addr);
++int memac_set_allmulti(struct fman_mac *memac, bool enable);
+ 
+ #endif /* __MEMAC_H */
+diff --git a/drivers/net/ethernet/freescale/fman/fman_port.c b/drivers/net/ethernet/freescale/fman/fman_port.c
+index 57bf44fa..0f72b056 100644
+--- a/drivers/net/ethernet/freescale/fman/fman_port.c
++++ b/drivers/net/ethernet/freescale/fman/fman_port.c
+@@ -32,10 +32,6 @@
+ 
+ #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+ 
+-#include "fman_port.h"
+-#include "fman.h"
+-#include "fman_sp.h"
+-
+ #include <linux/io.h>
+ #include <linux/slab.h>
+ #include <linux/module.h>
+@@ -45,6 +41,11 @@
+ #include <linux/delay.h>
+ #include <linux/libfdt_env.h>
+ 
++#include "fman.h"
++#include "fman_port.h"
++#include "fman_sp.h"
++#include "fman_keygen.h"
++
+ /* Queue ID */
+ #define DFLT_FQ_ID		0x00FFFFFF
+ 
+@@ -184,6 +185,7 @@
+ #define NIA_ENG_QMI_ENQ					0x00540000
+ #define NIA_ENG_QMI_DEQ					0x00580000
+ #define NIA_ENG_HWP					0x00440000
++#define NIA_ENG_HWK					0x00480000
+ #define NIA_BMI_AC_ENQ_FRAME				0x00000002
+ #define NIA_BMI_AC_TX_RELEASE				0x000002C0
+ #define NIA_BMI_AC_RELEASE				0x000000C0
+@@ -322,6 +324,10 @@ struct fman_port_qmi_regs {
+ #define HWP_HXS_PHE_REPORT 0x00000800
+ #define HWP_HXS_PCAC_PSTAT 0x00000100
+ #define HWP_HXS_PCAC_PSTOP 0x00000001
++#define HWP_HXS_TCP_OFFSET 0xA
++#define HWP_HXS_UDP_OFFSET 0xB
++#define HWP_HXS_SH_PAD_REM 0x80000000
++
+ struct fman_port_hwp_regs {
+ 	struct {
+ 		u32 ssa; /* Soft Sequence Attachment */
+@@ -394,6 +400,8 @@ struct fman_port_bpools {
+ struct fman_port_cfg {
+ 	u32 dflt_fqid;
+ 	u32 err_fqid;
++	u32 pcd_base_fqid;
++	u32 pcd_fqs_count;
+ 	u8 deq_sp;
+ 	bool deq_high_priority;
+ 	enum fman_port_deq_type deq_type;
+@@ -724,6 +732,10 @@ static void init_hwp(struct fman_port *port)
+ 		iowrite32be(0xffffffff, &regs->pmda[i].lcv);
+ 	}
+ 
++	/* Short packet padding removal from checksum calculation */
++	iowrite32be(HWP_HXS_SH_PAD_REM, &regs->pmda[HWP_HXS_TCP_OFFSET].ssa);
++	iowrite32be(HWP_HXS_SH_PAD_REM, &regs->pmda[HWP_HXS_UDP_OFFSET].ssa);
++
+ 	start_port_hwp(port);
+ }
+ 
+@@ -1271,6 +1283,10 @@ static void set_rx_dflt_cfg(struct fman_port *port,
+ 		port_params->specific_params.rx_params.err_fqid;
+ 	port->cfg->dflt_fqid =
+ 		port_params->specific_params.rx_params.dflt_fqid;
++	port->cfg->pcd_base_fqid =
++		port_params->specific_params.rx_params.pcd_base_fqid;
++	port->cfg->pcd_fqs_count =
++		port_params->specific_params.rx_params.pcd_fqs_count;
+ }
+ 
+ static void set_tx_dflt_cfg(struct fman_port *port,
+@@ -1331,8 +1347,10 @@ int fman_port_config(struct fman_port *port, struct fman_port_params *params)
+ 	switch (port->port_type) {
+ 	case FMAN_PORT_TYPE_RX:
+ 		set_rx_dflt_cfg(port, params);
++		/* fall through */
+ 	case FMAN_PORT_TYPE_TX:
+ 		set_tx_dflt_cfg(port, params, &port->dts_params);
++		/* fall through */
+ 	default:
+ 		set_dflt_cfg(port, params);
+ 	}
+@@ -1397,6 +1415,24 @@ int fman_port_config(struct fman_port *port, struct fman_port_params *params)
+ }
+ EXPORT_SYMBOL(fman_port_config);
+ 
++/**
++ * fman_port_use_kg_hash
++ * port:        A pointer to a FM Port module.
++ * Sets the HW KeyGen or the BMI as HW Parser next engine, enabling
++ * or bypassing the KeyGen hashing of Rx traffic
++ */
++void fman_port_use_kg_hash(struct fman_port *port, bool enable)
++{
++	if (enable)
++		/* After the Parser frames go to KeyGen */
++		iowrite32be(NIA_ENG_HWK, &port->bmi_regs->rx.fmbm_rfpne);
++	else
++		/* After the Parser frames go to BMI */
++		iowrite32be(NIA_ENG_BMI | NIA_BMI_AC_ENQ_FRAME,
++			    &port->bmi_regs->rx.fmbm_rfpne);
++}
++EXPORT_SYMBOL(fman_port_use_kg_hash);
++
+ /**
+  * fman_port_init
+  * port:	A pointer to a FM Port module.
+@@ -1407,9 +1443,10 @@ EXPORT_SYMBOL(fman_port_config);
+  */
+ int fman_port_init(struct fman_port *port)
+ {
++	struct fman_port_init_params params;
++	struct fman_keygen *keygen;
+ 	struct fman_port_cfg *cfg;
+ 	int err;
+-	struct fman_port_init_params params;
+ 
+ 	if (is_init_done(port->cfg))
+ 		return -EINVAL;
+@@ -1472,6 +1509,17 @@ int fman_port_init(struct fman_port *port)
+ 	if (err)
+ 		return err;
+ 
++	if (port->cfg->pcd_fqs_count) {
++		keygen = port->dts_params.fman->keygen;
++		err = keygen_port_hashing_init(keygen, port->port_id,
++					       port->cfg->pcd_base_fqid,
++					       port->cfg->pcd_fqs_count);
++		if (err)
++			return err;
++
++		fman_port_use_kg_hash(port, true);
++	}
++
+ 	kfree(port->cfg);
+ 	port->cfg = NULL;
+ 
+@@ -1682,6 +1730,17 @@ u32 fman_port_get_qman_channel_id(struct fman_port *port)
+ }
+ EXPORT_SYMBOL(fman_port_get_qman_channel_id);
+ 
++int fman_port_get_hash_result_offset(struct fman_port *port, u32 *offset)
++{
++	if (port->buffer_offsets.hash_result_offset == ILLEGAL_BASE)
++		return -EINVAL;
++
++	*offset = port->buffer_offsets.hash_result_offset;
++
++	return 0;
++}
++EXPORT_SYMBOL(fman_port_get_hash_result_offset);
++
+ static int fman_port_probe(struct platform_device *of_dev)
+ {
+ 	struct fman_port *port;
+diff --git a/drivers/net/ethernet/freescale/fman/fman_port.h b/drivers/net/ethernet/freescale/fman/fman_port.h
+index 8ba90173..e86ca6a3 100644
+--- a/drivers/net/ethernet/freescale/fman/fman_port.h
++++ b/drivers/net/ethernet/freescale/fman/fman_port.h
+@@ -100,6 +100,9 @@ struct fman_port;
+ struct fman_port_rx_params {
+ 	u32 err_fqid;			/* Error Queue Id. */
+ 	u32 dflt_fqid;			/* Default Queue Id. */
++	u32 pcd_base_fqid;		/* PCD base Queue Id. */
++	u32 pcd_fqs_count;		/* Number of PCD FQs. */
++
+ 	/* Which external buffer pools are used
+ 	 * (up to FMAN_PORT_MAX_EXT_POOLS_NUM), and their sizes.
+ 	 */
+@@ -134,6 +137,8 @@ struct fman_port_params {
+ 
+ int fman_port_config(struct fman_port *port, struct fman_port_params *params);
+ 
++void fman_port_use_kg_hash(struct fman_port *port, bool enable);
++
+ int fman_port_init(struct fman_port *port);
+ 
+ int fman_port_cfg_buf_prefix_content(struct fman_port *port,
+@@ -146,6 +151,8 @@ int fman_port_enable(struct fman_port *port);
+ 
+ u32 fman_port_get_qman_channel_id(struct fman_port *port);
+ 
++int fman_port_get_hash_result_offset(struct fman_port *port, u32 *offset);
++
+ struct fman_port *fman_port_bind(struct device *dev);
+ 
+ #endif /* __FMAN_PORT_H */
+diff --git a/drivers/net/ethernet/freescale/fman/fman_tgec.c b/drivers/net/ethernet/freescale/fman/fman_tgec.c
+index 4b0f3a50..284735d4 100644
+--- a/drivers/net/ethernet/freescale/fman/fman_tgec.c
++++ b/drivers/net/ethernet/freescale/fman/fman_tgec.c
+@@ -217,6 +217,7 @@ struct fman_mac {
+ 	struct tgec_cfg *cfg;
+ 	void *fm;
+ 	struct fman_rev_info fm_rev_info;
++	bool allmulti_enabled;
+ };
+ 
+ static void set_mac_address(struct tgec_regs __iomem *regs, u8 *adr)
+@@ -564,6 +565,29 @@ int tgec_add_hash_mac_address(struct fman_mac *tgec, enet_addr_t *eth_addr)
+ 	return 0;
+ }
+ 
++int tgec_set_allmulti(struct fman_mac *tgec, bool enable)
++{
++	u32 entry;
++	struct tgec_regs __iomem *regs = tgec->regs;
++
++	if (!is_init_done(tgec->cfg))
++		return -EINVAL;
++
++	if (enable) {
++		for (entry = 0; entry < TGEC_HASH_TABLE_SIZE; entry++)
++			iowrite32be(entry | TGEC_HASH_MCAST_EN,
++				    &regs->hashtable_ctrl);
++	} else {
++		for (entry = 0; entry < TGEC_HASH_TABLE_SIZE; entry++)
++			iowrite32be(entry & ~TGEC_HASH_MCAST_EN,
++				    &regs->hashtable_ctrl);
++	}
++
++	tgec->allmulti_enabled = enable;
++
++	return 0;
++}
++
+ int tgec_del_hash_mac_address(struct fman_mac *tgec, enet_addr_t *eth_addr)
+ {
+ 	struct tgec_regs __iomem *regs = tgec->regs;
+@@ -591,9 +615,12 @@ int tgec_del_hash_mac_address(struct fman_mac *tgec, enet_addr_t *eth_addr)
+ 			break;
+ 		}
+ 	}
+-	if (list_empty(&tgec->multicast_addr_hash->lsts[hash]))
+-		iowrite32be((hash & ~TGEC_HASH_MCAST_EN),
+-			    &regs->hashtable_ctrl);
++
++	if (!tgec->allmulti_enabled) {
++		if (list_empty(&tgec->multicast_addr_hash->lsts[hash]))
++			iowrite32be((hash & ~TGEC_HASH_MCAST_EN),
++				    &regs->hashtable_ctrl);
++	}
+ 
+ 	return 0;
+ }
+diff --git a/drivers/net/ethernet/freescale/fman/fman_tgec.h b/drivers/net/ethernet/freescale/fman/fman_tgec.h
+index 514bba9f..cbbd3b42 100644
+--- a/drivers/net/ethernet/freescale/fman/fman_tgec.h
++++ b/drivers/net/ethernet/freescale/fman/fman_tgec.h
+@@ -51,5 +51,6 @@ int tgec_set_exception(struct fman_mac *tgec,
+ int tgec_add_hash_mac_address(struct fman_mac *tgec, enet_addr_t *eth_addr);
+ int tgec_del_hash_mac_address(struct fman_mac *tgec, enet_addr_t *eth_addr);
+ int tgec_get_version(struct fman_mac *tgec, u32 *mac_version);
++int tgec_set_allmulti(struct fman_mac *tgec, bool enable);
+ 
+ #endif /* __TGEC_H */
+diff --git a/drivers/net/ethernet/freescale/fman/mac.c b/drivers/net/ethernet/freescale/fman/mac.c
+index 66d73d00..c98de9f1 100644
+--- a/drivers/net/ethernet/freescale/fman/mac.c
++++ b/drivers/net/ethernet/freescale/fman/mac.c
+@@ -57,9 +57,7 @@ struct mac_priv_s {
+ 	struct device			*dev;
+ 	void __iomem			*vaddr;
+ 	u8				cell_index;
+-	phy_interface_t			phy_if;
+ 	struct fman			*fman;
+-	struct device_node		*phy_node;
+ 	struct device_node		*internal_phy_node;
+ 	/* List of multicast addresses */
+ 	struct list_head		mc_addr_list;
+@@ -106,7 +104,7 @@ static void set_fman_mac_params(struct mac_device *mac_dev,
+ 			     resource_size(mac_dev->res));
+ 	memcpy(&params->addr, mac_dev->addr, sizeof(mac_dev->addr));
+ 	params->max_speed	= priv->max_speed;
+-	params->phy_if		= priv->phy_if;
++	params->phy_if		= mac_dev->phy_if;
+ 	params->basex_if	= false;
+ 	params->mac_id		= priv->cell_index;
+ 	params->fm		= (void *)priv->fman;
+@@ -419,15 +417,12 @@ void fman_get_pause_cfg(struct mac_device *mac_dev, bool *rx_pause,
+ }
+ EXPORT_SYMBOL(fman_get_pause_cfg);
+ 
+-static void adjust_link_void(struct net_device *net_dev)
++static void adjust_link_void(struct mac_device *mac_dev)
+ {
+ }
+ 
+-static void adjust_link_dtsec(struct net_device *net_dev)
++static void adjust_link_dtsec(struct mac_device *mac_dev)
+ {
+-	struct device *dev = net_dev->dev.parent;
+-	struct dpaa_eth_data *eth_data = dev->platform_data;
+-	struct mac_device *mac_dev = eth_data->mac_dev;
+ 	struct phy_device *phy_dev = mac_dev->phy_dev;
+ 	struct fman_mac *fman_mac;
+ 	bool rx_pause, tx_pause;
+@@ -444,14 +439,12 @@ static void adjust_link_dtsec(struct net_device *net_dev)
+ 	fman_get_pause_cfg(mac_dev, &rx_pause, &tx_pause);
+ 	err = fman_set_mac_active_pause(mac_dev, rx_pause, tx_pause);
+ 	if (err < 0)
+-		netdev_err(net_dev, "fman_set_mac_active_pause() = %d\n", err);
++		dev_err(mac_dev->priv->dev, "fman_set_mac_active_pause() = %d\n",
++			err);
+ }
+ 
+-static void adjust_link_memac(struct net_device *net_dev)
++static void adjust_link_memac(struct mac_device *mac_dev)
+ {
+-	struct device *dev = net_dev->dev.parent;
+-	struct dpaa_eth_data *eth_data = dev->platform_data;
+-	struct mac_device *mac_dev = eth_data->mac_dev;
+ 	struct phy_device *phy_dev = mac_dev->phy_dev;
+ 	struct fman_mac *fman_mac;
+ 	bool rx_pause, tx_pause;
+@@ -463,60 +456,12 @@ static void adjust_link_memac(struct net_device *net_dev)
+ 	fman_get_pause_cfg(mac_dev, &rx_pause, &tx_pause);
+ 	err = fman_set_mac_active_pause(mac_dev, rx_pause, tx_pause);
+ 	if (err < 0)
+-		netdev_err(net_dev, "fman_set_mac_active_pause() = %d\n", err);
+-}
+-
+-/* Initializes driver's PHY state, and attaches to the PHY.
+- * Returns 0 on success.
+- */
+-static struct phy_device *init_phy(struct net_device *net_dev,
+-				   struct mac_device *mac_dev,
+-				   void (*adj_lnk)(struct net_device *))
+-{
+-	struct phy_device	*phy_dev;
+-	struct mac_priv_s	*priv = mac_dev->priv;
+-
+-	phy_dev = of_phy_connect(net_dev, priv->phy_node, adj_lnk, 0,
+-				 priv->phy_if);
+-	if (!phy_dev) {
+-		netdev_err(net_dev, "Could not connect to PHY\n");
+-		return NULL;
+-	}
+-
+-	/* Remove any features not supported by the controller */
+-	phy_dev->supported &= mac_dev->if_support;
+-	/* Enable the symmetric and asymmetric PAUSE frame advertisements,
+-	 * as most of the PHY drivers do not enable them by default.
+-	 */
+-	phy_dev->supported |= (SUPPORTED_Pause | SUPPORTED_Asym_Pause);
+-	phy_dev->advertising = phy_dev->supported;
+-
+-	mac_dev->phy_dev = phy_dev;
+-
+-	return phy_dev;
+-}
+-
+-static struct phy_device *dtsec_init_phy(struct net_device *net_dev,
+-					 struct mac_device *mac_dev)
+-{
+-	return init_phy(net_dev, mac_dev, &adjust_link_dtsec);
+-}
+-
+-static struct phy_device *tgec_init_phy(struct net_device *net_dev,
+-					struct mac_device *mac_dev)
+-{
+-	return init_phy(net_dev, mac_dev, adjust_link_void);
+-}
+-
+-static struct phy_device *memac_init_phy(struct net_device *net_dev,
+-					 struct mac_device *mac_dev)
+-{
+-	return init_phy(net_dev, mac_dev, &adjust_link_memac);
++		dev_err(mac_dev->priv->dev, "fman_set_mac_active_pause() = %d\n",
++			err);
+ }
+ 
+ static void setup_dtsec(struct mac_device *mac_dev)
+ {
+-	mac_dev->init_phy		= dtsec_init_phy;
+ 	mac_dev->init			= dtsec_initialization;
+ 	mac_dev->set_promisc		= dtsec_set_promiscuous;
+ 	mac_dev->change_addr		= dtsec_modify_mac_address;
+@@ -525,17 +470,17 @@ static void setup_dtsec(struct mac_device *mac_dev)
+ 	mac_dev->set_tx_pause		= dtsec_set_tx_pause_frames;
+ 	mac_dev->set_rx_pause		= dtsec_accept_rx_pause_frames;
+ 	mac_dev->set_exception		= dtsec_set_exception;
++	mac_dev->set_allmulti		= dtsec_set_allmulti;
+ 	mac_dev->set_multi		= set_multi;
+ 	mac_dev->start			= start;
+ 	mac_dev->stop			= stop;
+-
++	mac_dev->adjust_link            = adjust_link_dtsec;
+ 	mac_dev->priv->enable		= dtsec_enable;
+ 	mac_dev->priv->disable		= dtsec_disable;
+ }
+ 
+ static void setup_tgec(struct mac_device *mac_dev)
+ {
+-	mac_dev->init_phy		= tgec_init_phy;
+ 	mac_dev->init			= tgec_initialization;
+ 	mac_dev->set_promisc		= tgec_set_promiscuous;
+ 	mac_dev->change_addr		= tgec_modify_mac_address;
+@@ -544,17 +489,17 @@ static void setup_tgec(struct mac_device *mac_dev)
+ 	mac_dev->set_tx_pause		= tgec_set_tx_pause_frames;
+ 	mac_dev->set_rx_pause		= tgec_accept_rx_pause_frames;
+ 	mac_dev->set_exception		= tgec_set_exception;
++	mac_dev->set_allmulti		= tgec_set_allmulti;
+ 	mac_dev->set_multi		= set_multi;
+ 	mac_dev->start			= start;
+ 	mac_dev->stop			= stop;
+-
++	mac_dev->adjust_link            = adjust_link_void;
+ 	mac_dev->priv->enable		= tgec_enable;
+ 	mac_dev->priv->disable		= tgec_disable;
+ }
+ 
+ static void setup_memac(struct mac_device *mac_dev)
+ {
+-	mac_dev->init_phy		= memac_init_phy;
+ 	mac_dev->init			= memac_initialization;
+ 	mac_dev->set_promisc		= memac_set_promiscuous;
+ 	mac_dev->change_addr		= memac_modify_mac_address;
+@@ -563,10 +508,11 @@ static void setup_memac(struct mac_device *mac_dev)
+ 	mac_dev->set_tx_pause		= memac_set_tx_pause_frames;
+ 	mac_dev->set_rx_pause		= memac_accept_rx_pause_frames;
+ 	mac_dev->set_exception		= memac_set_exception;
++	mac_dev->set_allmulti		= memac_set_allmulti;
+ 	mac_dev->set_multi		= set_multi;
+ 	mac_dev->start			= start;
+ 	mac_dev->stop			= stop;
+-
++	mac_dev->adjust_link            = adjust_link_memac;
+ 	mac_dev->priv->enable		= memac_enable;
+ 	mac_dev->priv->disable		= memac_disable;
+ }
+@@ -599,8 +545,7 @@ static const u16 phy2speed[] = {
+ };
+ 
+ static struct platform_device *dpaa_eth_add_device(int fman_id,
+-						   struct mac_device *mac_dev,
+-						   struct device_node *node)
++						   struct mac_device *mac_dev)
+ {
+ 	struct platform_device *pdev;
+ 	struct dpaa_eth_data data;
+@@ -613,17 +558,14 @@ static struct platform_device *dpaa_eth_add_device(int fman_id,
+ 	data.mac_dev = mac_dev;
+ 	data.mac_hw_id = priv->cell_index;
+ 	data.fman_hw_id = fman_id;
+-	data.mac_node = node;
+ 
+ 	mutex_lock(&eth_lock);
+-
+ 	pdev = platform_device_alloc("dpaa-ethernet", dpaa_eth_dev_cnt);
+ 	if (!pdev) {
+ 		ret = -ENOMEM;
+ 		goto no_mem;
+ 	}
+ 
+-	pdev->dev.of_node = node;
+ 	pdev->dev.parent = priv->dev;
+ 
+ 	ret = platform_device_add_data(pdev, &data, sizeof(data));
+@@ -675,7 +617,6 @@ static int mac_probe(struct platform_device *_of_dev)
+ 	mac_dev = devm_kzalloc(dev, sizeof(*mac_dev), GFP_KERNEL);
+ 	if (!mac_dev) {
+ 		err = -ENOMEM;
+-		dev_err(dev, "devm_kzalloc() = %d\n", err);
+ 		goto _return;
+ 	}
+ 	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+@@ -705,9 +646,6 @@ static int mac_probe(struct platform_device *_of_dev)
+ 		goto _return;
+ 	}
+ 
+-	/* Register mac_dev */
+-	dev_set_drvdata(dev, mac_dev);
+-
+ 	INIT_LIST_HEAD(&priv->mc_addr_list);
+ 
+ 	/* Get the FM node */
+@@ -716,7 +654,7 @@ static int mac_probe(struct platform_device *_of_dev)
+ 		dev_err(dev, "of_get_parent(%s) failed\n",
+ 			mac_node->full_name);
+ 		err = -EINVAL;
+-		goto _return_dev_set_drvdata;
++		goto _return_of_get_parent;
+ 	}
+ 
+ 	of_dev = of_find_device_by_node(dev_node);
+@@ -752,7 +690,7 @@ static int mac_probe(struct platform_device *_of_dev)
+ 	if (err < 0) {
+ 		dev_err(dev, "of_address_to_resource(%s) = %d\n",
+ 			mac_node->full_name, err);
+-		goto _return_dev_set_drvdata;
++		goto _return_of_get_parent;
+ 	}
+ 
+ 	mac_dev->res = __devm_request_region(dev,
+@@ -762,7 +700,7 @@ static int mac_probe(struct platform_device *_of_dev)
+ 	if (!mac_dev->res) {
+ 		dev_err(dev, "__devm_request_mem_region(mac) failed\n");
+ 		err = -EBUSY;
+-		goto _return_dev_set_drvdata;
++		goto _return_of_get_parent;
+ 	}
+ 
+ 	priv->vaddr = devm_ioremap(dev, mac_dev->res->start,
+@@ -770,16 +708,12 @@ static int mac_probe(struct platform_device *_of_dev)
+ 	if (!priv->vaddr) {
+ 		dev_err(dev, "devm_ioremap() failed\n");
+ 		err = -EIO;
+-		goto _return_dev_set_drvdata;
++		goto _return_of_get_parent;
+ 	}
+ 
+ 	if (!of_device_is_available(mac_node)) {
+-		devm_iounmap(dev, priv->vaddr);
+-		__devm_release_region(dev, fman_get_mem_region(priv->fman),
+-				      res.start, res.end + 1 - res.start);
+-		devm_kfree(dev, mac_dev);
+-		dev_set_drvdata(dev, NULL);
+-		return -ENODEV;
++		err = -ENODEV;
++		goto _return_of_get_parent;
+ 	}
+ 
+ 	/* Get the cell-index */
+@@ -788,7 +722,7 @@ static int mac_probe(struct platform_device *_of_dev)
+ 		dev_err(dev, "failed to read cell-index for %s\n",
+ 			mac_node->full_name);
+ 		err = -EINVAL;
+-		goto _return_dev_set_drvdata;
++		goto _return_of_get_parent;
+ 	}
+ 	priv->cell_index = (u8)val;
+ 
+@@ -798,7 +732,7 @@ static int mac_probe(struct platform_device *_of_dev)
+ 		dev_err(dev, "of_get_mac_address(%s) failed\n",
+ 			mac_node->full_name);
+ 		err = -EINVAL;
+-		goto _return_dev_set_drvdata;
++		goto _return_of_get_parent;
+ 	}
+ 	memcpy(mac_dev->addr, mac_addr, sizeof(mac_dev->addr));
+ 
+@@ -808,14 +742,14 @@ static int mac_probe(struct platform_device *_of_dev)
+ 		dev_err(dev, "of_count_phandle_with_args(%s, fsl,fman-ports) failed\n",
+ 			mac_node->full_name);
+ 		err = nph;
+-		goto _return_dev_set_drvdata;
++		goto _return_of_get_parent;
+ 	}
+ 
+ 	if (nph != ARRAY_SIZE(mac_dev->port)) {
+ 		dev_err(dev, "Not supported number of fman-ports handles of mac node %s from device tree\n",
+ 			mac_node->full_name);
+ 		err = -EINVAL;
+-		goto _return_dev_set_drvdata;
++		goto _return_of_get_parent;
+ 	}
+ 
+ 	for (i = 0; i < ARRAY_SIZE(mac_dev->port); i++) {
+@@ -854,13 +788,13 @@ static int mac_probe(struct platform_device *_of_dev)
+ 			 mac_node->full_name);
+ 		phy_if = PHY_INTERFACE_MODE_SGMII;
+ 	}
+-	priv->phy_if = phy_if;
++	mac_dev->phy_if = phy_if;
+ 
+-	priv->speed		= phy2speed[priv->phy_if];
++	priv->speed		= phy2speed[mac_dev->phy_if];
+ 	priv->max_speed		= priv->speed;
+ 	mac_dev->if_support	= DTSEC_SUPPORTED;
+ 	/* We don't support half-duplex in SGMII mode */
+-	if (priv->phy_if == PHY_INTERFACE_MODE_SGMII)
++	if (mac_dev->phy_if == PHY_INTERFACE_MODE_SGMII)
+ 		mac_dev->if_support &= ~(SUPPORTED_10baseT_Half |
+ 					SUPPORTED_100baseT_Half);
+ 
+@@ -869,27 +803,29 @@ static int mac_probe(struct platform_device *_of_dev)
+ 		mac_dev->if_support |= SUPPORTED_1000baseT_Full;
+ 
+ 	/* The 10G interface only supports one mode */
+-	if (priv->phy_if == PHY_INTERFACE_MODE_XGMII)
++	if (mac_dev->phy_if == PHY_INTERFACE_MODE_XGMII)
+ 		mac_dev->if_support = SUPPORTED_10000baseT_Full;
+ 
+ 	/* Get the rest of the PHY information */
+-	priv->phy_node = of_parse_phandle(mac_node, "phy-handle", 0);
+-	if (!priv->phy_node && of_phy_is_fixed_link(mac_node)) {
++	mac_dev->phy_node = of_parse_phandle(mac_node, "phy-handle", 0);
++	if (!mac_dev->phy_node && of_phy_is_fixed_link(mac_node)) {
+ 		struct phy_device *phy;
+ 
+ 		err = of_phy_register_fixed_link(mac_node);
+ 		if (err)
+-			goto _return_dev_set_drvdata;
++			goto _return_of_get_parent;
+ 
+ 		priv->fixed_link = kzalloc(sizeof(*priv->fixed_link),
+ 					   GFP_KERNEL);
+ 		if (!priv->fixed_link)
+-			goto _return_dev_set_drvdata;
++			goto _return_of_get_parent;
+ 
+-		priv->phy_node = of_node_get(mac_node);
+-		phy = of_phy_find_device(priv->phy_node);
+-		if (!phy)
+-			goto _return_dev_set_drvdata;
++		mac_dev->phy_node = of_node_get(mac_node);
++		phy = of_phy_find_device(mac_dev->phy_node);
++		if (!phy) {
++			of_node_put(mac_dev->phy_node);
++			goto _return_of_get_parent;
++		}
+ 
+ 		priv->fixed_link->link = phy->link;
+ 		priv->fixed_link->speed = phy->speed;
+@@ -903,8 +839,8 @@ static int mac_probe(struct platform_device *_of_dev)
+ 	err = mac_dev->init(mac_dev);
+ 	if (err < 0) {
+ 		dev_err(dev, "mac_dev->init() = %d\n", err);
+-		of_node_put(priv->phy_node);
+-		goto _return_dev_set_drvdata;
++		of_node_put(mac_dev->phy_node);
++		goto _return_of_get_parent;
+ 	}
+ 
+ 	/* pause frame autonegotiation enabled */
+@@ -925,7 +861,7 @@ static int mac_probe(struct platform_device *_of_dev)
+ 		 mac_dev->addr[0], mac_dev->addr[1], mac_dev->addr[2],
+ 		 mac_dev->addr[3], mac_dev->addr[4], mac_dev->addr[5]);
+ 
+-	priv->eth_dev = dpaa_eth_add_device(fman_id, mac_dev, mac_node);
++	priv->eth_dev = dpaa_eth_add_device(fman_id, mac_dev);
+ 	if (IS_ERR(priv->eth_dev)) {
+ 		dev_err(dev, "failed to add Ethernet platform device for MAC %d\n",
+ 			priv->cell_index);
+@@ -936,9 +872,8 @@ static int mac_probe(struct platform_device *_of_dev)
+ 
+ _return_of_node_put:
+ 	of_node_put(dev_node);
+-_return_dev_set_drvdata:
++_return_of_get_parent:
+ 	kfree(priv->fixed_link);
+-	dev_set_drvdata(dev, NULL);
+ _return:
+ 	return err;
+ }
+diff --git a/drivers/net/ethernet/freescale/fman/mac.h b/drivers/net/ethernet/freescale/fman/mac.h
+index d7313f0c..b520cec1 100644
+--- a/drivers/net/ethernet/freescale/fman/mac.h
++++ b/drivers/net/ethernet/freescale/fman/mac.h
+@@ -50,6 +50,8 @@ struct mac_device {
+ 	struct fman_port	*port[2];
+ 	u32			 if_support;
+ 	struct phy_device	*phy_dev;
++	phy_interface_t		phy_if;
++	struct device_node	*phy_node;
+ 
+ 	bool autoneg_pause;
+ 	bool rx_pause_req;
+@@ -57,14 +59,15 @@ struct mac_device {
+ 	bool rx_pause_active;
+ 	bool tx_pause_active;
+ 	bool promisc;
++	bool allmulti;
+ 
+-	struct phy_device *(*init_phy)(struct net_device *net_dev,
+-				       struct mac_device *mac_dev);
+ 	int (*init)(struct mac_device *mac_dev);
+ 	int (*start)(struct mac_device *mac_dev);
+ 	int (*stop)(struct mac_device *mac_dev);
++	void (*adjust_link)(struct mac_device *mac_dev);
+ 	int (*set_promisc)(struct fman_mac *mac_dev, bool enable);
+ 	int (*change_addr)(struct fman_mac *mac_dev, enet_addr_t *enet_addr);
++	int (*set_allmulti)(struct fman_mac *mac_dev, bool enable);
+ 	int (*set_multi)(struct net_device *net_dev,
+ 			 struct mac_device *mac_dev);
+ 	int (*set_rx_pause)(struct fman_mac *mac_dev, bool en);
+@@ -82,7 +85,6 @@ struct mac_device {
+ };
+ 
+ struct dpaa_eth_data {
+-	struct device_node *mac_node;
+ 	struct mac_device *mac_dev;
+ 	int mac_hw_id;
+ 	int fman_hw_id;
+diff --git a/drivers/net/ethernet/freescale/fsl_pq_mdio.c b/drivers/net/ethernet/freescale/fsl_pq_mdio.c
+index 446c7b37..a10de1e9 100644
+--- a/drivers/net/ethernet/freescale/fsl_pq_mdio.c
++++ b/drivers/net/ethernet/freescale/fsl_pq_mdio.c
+@@ -381,7 +381,7 @@ static int fsl_pq_mdio_probe(struct platform_device *pdev)
+ {
+ 	const struct of_device_id *id =
+ 		of_match_device(fsl_pq_mdio_match, &pdev->dev);
+-	const struct fsl_pq_mdio_data *data = id->data;
++	const struct fsl_pq_mdio_data *data;
+ 	struct device_node *np = pdev->dev.of_node;
+ 	struct resource res;
+ 	struct device_node *tbi;
+@@ -389,6 +389,13 @@ static int fsl_pq_mdio_probe(struct platform_device *pdev)
+ 	struct mii_bus *new_bus;
+ 	int err;
+ 
++	if (!id) {
++		dev_err(&pdev->dev, "Failed to match device\n");
++		return -ENODEV;
++	}
++
++	data = id->data;
++
+ 	dev_dbg(&pdev->dev, "found %s compatible node\n", id->compatible);
+ 
+ 	new_bus = mdiobus_alloc_size(sizeof(*priv));
+diff --git a/drivers/net/ethernet/freescale/gianfar.c b/drivers/net/ethernet/freescale/gianfar.c
+index 3a2b08b1..60bd1b36 100644
+--- a/drivers/net/ethernet/freescale/gianfar.c
++++ b/drivers/net/ethernet/freescale/gianfar.c
+@@ -1375,9 +1375,11 @@ static int gfar_probe(struct platform_device *ofdev)
+ 
+ 	gfar_init_addr_hash_table(priv);
+ 
+-	/* Insert receive time stamps into padding alignment bytes */
++	/* Insert receive time stamps into padding alignment bytes, and
++	 * plus 2 bytes padding to ensure the cpu alignment.
++	 */
+ 	if (priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER)
+-		priv->padding = 8;
++		priv->padding = 8 + DEFAULT_PADDING;
+ 
+ 	if (dev->features & NETIF_F_IP_CSUM ||
+ 	    priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER)
+@@ -2933,7 +2935,7 @@ static irqreturn_t gfar_transmit(int irq, void *grp_id)
+ static bool gfar_add_rx_frag(struct gfar_rx_buff *rxb, u32 lstatus,
+ 			     struct sk_buff *skb, bool first)
+ {
+-	unsigned int size = lstatus & BD_LENGTH_MASK;
++	int size = lstatus & BD_LENGTH_MASK;
+ 	struct page *page = rxb->page;
+ 	bool last = !!(lstatus & BD_LFLAG(RXBD_LAST));
+ 
+@@ -2948,11 +2950,16 @@ static bool gfar_add_rx_frag(struct gfar_rx_buff *rxb, u32 lstatus,
+ 		if (last)
+ 			size -= skb->len;
+ 
+-		/* in case the last fragment consisted only of the FCS */
++		/* Add the last fragment if it contains something other than
++		 * the FCS, otherwise drop it and trim off any part of the FCS
++		 * that was already received.
++		 */
+ 		if (size > 0)
+ 			skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
+ 					rxb->page_offset + RXBUF_ALIGNMENT,
+ 					size, GFAR_RXB_TRUESIZE);
++		else if (size < 0)
++			pskb_trim(skb, skb->len + size);
+ 	}
+ 
+ 	/* try reuse page */
+@@ -3068,9 +3075,6 @@ static void gfar_process_frame(struct net_device *ndev, struct sk_buff *skb)
+ 	if (ndev->features & NETIF_F_RXCSUM)
+ 		gfar_rx_checksum(skb, fcb);
+ 
+-	/* Tell the skb what kind of packet this is */
+-	skb->protocol = eth_type_trans(skb, ndev);
+-
+ 	/* There's need to check for NETIF_F_HW_VLAN_CTAG_RX here.
+ 	 * Even if vlan rx accel is disabled, on some chips
+ 	 * RXFCB_VLN is pseudo randomly set.
+@@ -3141,13 +3145,15 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
+ 			continue;
+ 		}
+ 
++		gfar_process_frame(ndev, skb);
++
+ 		/* Increment the number of packets */
+ 		total_pkts++;
+ 		total_bytes += skb->len;
+ 
+ 		skb_record_rx_queue(skb, rx_queue->qindex);
+ 
+-		gfar_process_frame(ndev, skb);
++		skb->protocol = eth_type_trans(skb, ndev);
+ 
+ 		/* Send the packet up the stack */
+ 		napi_gro_receive(&rx_queue->grp->napi_rx, skb);
+diff --git a/drivers/net/ethernet/freescale/gianfar_ptp.c b/drivers/net/ethernet/freescale/gianfar_ptp.c
+index 57798814..ec4b6997 100644
+--- a/drivers/net/ethernet/freescale/gianfar_ptp.c
++++ b/drivers/net/ethernet/freescale/gianfar_ptp.c
+@@ -314,11 +314,10 @@ static int ptp_gianfar_adjtime(struct ptp_clock_info *ptp, s64 delta)
+ 	now = tmr_cnt_read(etsects);
+ 	now += delta;
+ 	tmr_cnt_write(etsects, now);
++	set_fipers(etsects);
+ 
+ 	spin_unlock_irqrestore(&etsects->lock, flags);
+ 
+-	set_fipers(etsects);
+-
+ 	return 0;
+ }
+ 
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/Kconfig b/drivers/net/ethernet/freescale/sdk_dpaa/Kconfig
+index b21d2362..b37b2a00 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/Kconfig
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/Kconfig
+@@ -80,7 +80,6 @@ config FSL_DPAA_ETH_JUMBO_FRAME
+ 	  significantly the driver's memory footprint and may even deplete
+ 	  the system memory. Also, the skb truesize is altered and messages
+ 	  from the stack that warn against this are bypassed.
+-	  This option is not available on LS1043.
+ 
+ config FSL_DPAA_TS
+ 	bool "Linux compliant timestamping"
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c
+index 1f76aa39..33c80b6e 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c
+@@ -776,9 +776,8 @@ static int dpa_private_netdev_init(struct net_device *net_dev)
+ 	net_dev->features |= NETIF_F_HW_ACCEL_MQ;
+ 
+ #ifndef CONFIG_PPC
+-	/* Due to the A010022 FMan errata, we can not use contig frames larger
+-	 * than 4K, nor S/G frames. We need to stop advertising S/G and GSO
+-	 * support.
++	/* Due to the A010022 FMan errata, we can not use S/G frames. We need
++	 * to stop advertising S/G and GSO support.
+ 	 */
+ 	if (unlikely(dpaa_errata_a010022)) {
+ 		net_dev->hw_features &= ~NETIF_F_SG;
+@@ -981,9 +980,6 @@ dpaa_eth_priv_probe(struct platform_device *_of_dev)
+ 	/* We only want to use jumbo frame optimization if we actually have
+ 	 * L2 MAX FRM set for jumbo frames as well.
+ 	 */
+-#ifndef CONFIG_PPC
+-	if (likely(!dpaa_errata_a010022))
+-#endif
+ 	if(fm_get_max_frm() < 9600)
+ 		dev_warn(dev,
+ 			"Invalid configuration: if jumbo frames support is on, FSL_FM_MAX_FRAME_SIZE should be set to 9600\n");
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h
+index 57c9bef4..2e2d14f9 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h
+@@ -672,8 +672,8 @@ static inline void _dpa_bp_free_pf(void *addr)
+ /* LS1043A SoC has a HW issue regarding FMan DMA transactions; The issue
+  * manifests itself at high traffic rates when frames cross 4K memory
+  * boundaries or when they are not aligned to 16 bytes; For the moment, we
+- * use a SW workaround to avoid frames larger than 4K or that exceed 4K
+- * alignments and to realign the frames to 16 bytes.
++ * use a SW workaround that realigns frames to 256 bytes. Scatter/Gather
++ * frames aren't supported on egress.
+  */
+ 
+ #ifndef CONFIG_PPC
+@@ -682,6 +682,10 @@ extern bool dpaa_errata_a010022; /* SoC affected by A010022 errata */
+ #define HAS_DMA_ISSUE(start, size) \
+ 	(((uintptr_t)(start) + (size)) > \
+ 	 (((uintptr_t)(start) + 0x1000) & ~0xFFF))
++/* The headroom needs to accommodate our private data (64 bytes) but
++ * we reserve 256 bytes instead to guarantee 256 data alignment.
++ */
++#define DPAA_A010022_HEADROOM	256
+ #endif  /* !CONFIG_PPC */
+ 
+ #endif	/* __DPA_H */
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_ceetm.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_ceetm.c
+index b090db1f..d57b248e 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_ceetm.c
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_ceetm.c
+@@ -65,6 +65,76 @@ static void get_dcp_and_sp(struct net_device *dev, enum qm_dc_portal *dcp_id,
+ 	}
+ }
+ 
++/* Wait for the DPAA Eth driver WQ TX FQs to empty */
++static void dpaa_drain_fqs(struct net_device *dev)
++{
++	const struct dpa_priv_s *priv = netdev_priv(dev);
++	struct qm_mcr_queryfq_np np;
++	struct qman_fq *fq;
++	int ret, i;
++
++	for (i = 0; i < DPAA_ETH_TX_QUEUES; i ++) {
++		fq = priv->egress_fqs[i];
++		while (true) {
++			ret = qman_query_fq_np(fq, &np);
++			if (unlikely(ret)) {
++				pr_err(KBUILD_BASENAME
++				       " : %s : unable to query FQ %x: %d\n",
++				       __func__, fq->fqid, ret);
++				break;
++			}
++
++			if (np.frm_cnt == 0)
++				break;
++		}
++	}
++}
++
++/* Wait for the DPAA CEETM TX CQs to empty */
++static void ceetm_drain_class(struct ceetm_class *cl)
++{
++	struct qm_mcr_ceetm_cq_query cq_query;
++	struct qm_ceetm_cq *cq;
++	unsigned int idx;
++	int ret;
++
++	if (!cl)
++		return;
++
++	switch (cl->type) {
++	case CEETM_ROOT:
++		/* The ROOT classes aren't directly linked to CEETM CQs */
++		return;
++	case CEETM_PRIO:
++		cq = (struct qm_ceetm_cq*)cl->prio.cq;
++		break;
++	case CEETM_WBFS:
++		cq = (struct qm_ceetm_cq*)cl->wbfs.cq;
++		break;
++	}
++
++	if (!cq || !cl->ch)
++		return;
++
++	/* Build the query CQID by merging the channel and the CQ IDs */
++	idx = (cq->parent->idx << 4) | cq->idx;
++
++	while (true) {
++		ret = qman_ceetm_query_cq(idx,
++					  cl->ch->dcp_idx,
++					  &cq_query);
++		if (unlikely(ret)) {
++			pr_err(KBUILD_BASENAME
++			       " : %s : unable to query CQ %x: %d\n",
++			       __func__, idx, ret);
++			break;
++		}
++
++		if (cq_query.frm_cnt == 0)
++			break;
++	}
++}
++
+ /* Enqueue Rejection Notification callback */
+ static void ceetm_ern(struct qman_portal *portal, struct qman_fq *fq,
+ 		      const struct qm_mr_entry *msg)
+@@ -242,7 +312,6 @@ static int ceetm_config_lfq(struct qm_ceetm_cq *cq, struct ceetm_fq *fq,
+ /* Configure a prio ceetm class */
+ static int ceetm_config_prio_cls(struct ceetm_class *cls,
+ 				 struct net_device *dev,
+-				 struct qm_ceetm_channel *channel,
+ 				 unsigned int id)
+ {
+ 	int err;
+@@ -253,22 +322,22 @@ static int ceetm_config_prio_cls(struct ceetm_class *cls,
+ 		return err;
+ 
+ 	/* Claim and configure the CCG */
+-	err = ceetm_config_ccg(&cls->prio.ccg, channel, id, cls->prio.fq,
++	err = ceetm_config_ccg(&cls->prio.ccg, cls->ch, id, cls->prio.fq,
+ 			       dpa_priv);
+ 	if (err)
+ 		return err;
+ 
+ 	/* Claim and configure the CQ */
+-	err = qman_ceetm_cq_claim(&cls->prio.cq, channel, id, cls->prio.ccg);
++	err = qman_ceetm_cq_claim(&cls->prio.cq, cls->ch, id, cls->prio.ccg);
+ 	if (err)
+ 		return err;
+ 
+ 	if (cls->shaped) {
+-		err = qman_ceetm_channel_set_cq_cr_eligibility(channel, id, 1);
++		err = qman_ceetm_channel_set_cq_cr_eligibility(cls->ch, id, 1);
+ 		if (err)
+ 			return err;
+ 
+-		err = qman_ceetm_channel_set_cq_er_eligibility(channel, id, 1);
++		err = qman_ceetm_channel_set_cq_er_eligibility(cls->ch, id, 1);
+ 		if (err)
+ 			return err;
+ 	}
+@@ -284,7 +353,6 @@ static int ceetm_config_prio_cls(struct ceetm_class *cls,
+ /* Configure a wbfs ceetm class */
+ static int ceetm_config_wbfs_cls(struct ceetm_class *cls,
+ 				 struct net_device *dev,
+-				 struct qm_ceetm_channel *channel,
+ 				 unsigned int id, int type)
+ {
+ 	int err;
+@@ -295,17 +363,17 @@ static int ceetm_config_wbfs_cls(struct ceetm_class *cls,
+ 		return err;
+ 
+ 	/* Claim and configure the CCG */
+-	err = ceetm_config_ccg(&cls->wbfs.ccg, channel, id, cls->wbfs.fq,
++	err = ceetm_config_ccg(&cls->wbfs.ccg, cls->ch, id, cls->wbfs.fq,
+ 			       dpa_priv);
+ 	if (err)
+ 		return err;
+ 
+ 	/* Claim and configure the CQ */
+ 	if (type == WBFS_GRP_B)
+-		err = qman_ceetm_cq_claim_B(&cls->wbfs.cq, channel, id,
++		err = qman_ceetm_cq_claim_B(&cls->wbfs.cq, cls->ch, id,
+ 					    cls->wbfs.ccg);
+ 	else
+-		err = qman_ceetm_cq_claim_A(&cls->wbfs.cq, channel, id,
++		err = qman_ceetm_cq_claim_A(&cls->wbfs.cq, cls->ch, id,
+ 					    cls->wbfs.ccg);
+ 	if (err)
+ 		return err;
+@@ -353,6 +421,8 @@ static void ceetm_link_class(struct Qdisc *sch,
+ /* Destroy a ceetm class */
+ static void ceetm_cls_destroy(struct Qdisc *sch, struct ceetm_class *cl)
+ {
++	struct net_device *dev = qdisc_dev(sch);
++
+ 	if (!cl)
+ 		return;
+ 
+@@ -366,10 +436,10 @@ static void ceetm_cls_destroy(struct Qdisc *sch, struct ceetm_class *cl)
+ 			cl->root.child = NULL;
+ 		}
+ 
+-		if (cl->root.ch && qman_ceetm_channel_release(cl->root.ch))
++		if (cl->ch && qman_ceetm_channel_release(cl->ch))
+ 			pr_err(KBUILD_BASENAME
+ 			       " : %s : error releasing the channel %d\n",
+-			       __func__, cl->root.ch->idx);
++			       __func__, cl->ch->idx);
+ 
+ 		break;
+ 
+@@ -379,6 +449,12 @@ static void ceetm_cls_destroy(struct Qdisc *sch, struct ceetm_class *cl)
+ 			cl->prio.child = NULL;
+ 		}
+ 
++		/* We must make sure the CQ is empty before releasing it.
++		 * Pause all transmissions while we wait for it to drain.
++		 */
++		netif_tx_stop_all_queues(dev);
++		ceetm_drain_class(cl);
++
+ 		if (cl->prio.lfq && qman_ceetm_lfq_release(cl->prio.lfq))
+ 			pr_err(KBUILD_BASENAME
+ 			       " : %s : error releasing the LFQ %d\n",
+@@ -399,9 +475,16 @@ static void ceetm_cls_destroy(struct Qdisc *sch, struct ceetm_class *cl)
+ 		if (cl->prio.cstats)
+ 			free_percpu(cl->prio.cstats);
+ 
++		netif_tx_wake_all_queues(dev);
+ 		break;
+ 
+ 	case CEETM_WBFS:
++		/* We must make sure the CQ is empty before releasing it.
++		 * Pause all transmissions while we wait for it to drain.
++		 */
++		netif_tx_stop_all_queues(dev);
++		ceetm_drain_class(cl);
++
+ 		if (cl->wbfs.lfq && qman_ceetm_lfq_release(cl->wbfs.lfq))
+ 			pr_err(KBUILD_BASENAME
+ 			       " : %s : error releasing the LFQ %d\n",
+@@ -421,6 +504,8 @@ static void ceetm_cls_destroy(struct Qdisc *sch, struct ceetm_class *cl)
+ 
+ 		if (cl->wbfs.cstats)
+ 			free_percpu(cl->wbfs.cstats);
++
++		netif_tx_wake_all_queues(dev);
+ 	}
+ 
+ 	tcf_destroy_chain(&cl->filter_list);
+@@ -475,7 +560,9 @@ static void ceetm_destroy(struct Qdisc *sch)
+ 		if (!priv->root.qdiscs)
+ 			break;
+ 
+-		/* Remove the pfifo qdiscs */
++		/* Destroy the pfifo qdiscs in case they haven't been attached
++		 * to the netdev queues yet.
++		 */
+ 		for (ntx = 0; ntx < dev->num_tx_queues; ntx++)
+ 			if (priv->root.qdiscs[ntx])
+ 				qdisc_destroy(priv->root.qdiscs[ntx]);
+@@ -489,6 +576,10 @@ static void ceetm_destroy(struct Qdisc *sch)
+ 		break;
+ 
+ 	case CEETM_WBFS:
++		/* Reset the WBFS groups and priorities */
++		if (priv->wbfs.ch)
++			qman_ceetm_channel_set_group(priv->wbfs.ch, 1, 0, 0);
++
+ 		if (priv->wbfs.parent)
+ 			priv->wbfs.parent->prio.child = NULL;
+ 		break;
+@@ -605,7 +696,16 @@ static int ceetm_init_root(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 		goto err_init_root;
+ 	}
+ 
+-	/* pre-allocate underlying pfifo qdiscs */
++	/* Pre-allocate underlying pfifo qdiscs.
++	 *
++	 * We want to offload shaping and scheduling decisions to the hardware.
++	 * The pfifo qdiscs will be attached to the netdev queues and will
++	 * guide the traffic from the IP stack down to the driver with minimum
++	 * interference.
++	 *
++	 * The CEETM qdiscs and classes will be crossed when the traffic
++	 * reaches the driver.
++	 */
+ 	priv->root.qdiscs = kcalloc(dev->num_tx_queues,
+ 				    sizeof(priv->root.qdiscs[0]),
+ 				    GFP_KERNEL);
+@@ -752,6 +852,7 @@ static int ceetm_init_prio(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 
+ 	priv->shaped = parent_cl->shaped;
+ 	priv->prio.qcount = qopt->qcount;
++	priv->prio.ch = parent_cl->ch;
+ 
+ 	/* Create and configure qcount child classes */
+ 	for (i = 0; i < priv->prio.qcount; i++) {
+@@ -777,6 +878,7 @@ static int ceetm_init_prio(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 		child_cl->type = CEETM_PRIO;
+ 		child_cl->shaped = priv->shaped;
+ 		child_cl->prio.child = NULL;
++		child_cl->ch = priv->prio.ch;
+ 
+ 		/* All shaped CQs have CR and ER enabled by default */
+ 		child_cl->prio.cr = child_cl->shaped;
+@@ -785,8 +887,7 @@ static int ceetm_init_prio(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 		child_cl->prio.cq = NULL;
+ 
+ 		/* Configure the corresponding hardware CQ */
+-		err = ceetm_config_prio_cls(child_cl, dev,
+-					    parent_cl->root.ch, i);
++		err = ceetm_config_prio_cls(child_cl, dev, i);
+ 		if (err) {
+ 			pr_err(KBUILD_BASENAME " : %s : failed to configure the ceetm prio class %X\n",
+ 			       __func__, child_cl->common.classid);
+@@ -818,7 +919,6 @@ static int ceetm_init_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 	struct ceetm_class *parent_cl, *child_cl, *root_cl;
+ 	struct Qdisc *parent_qdisc;
+ 	struct ceetm_qdisc *parent_priv;
+-	struct qm_ceetm_channel *channel;
+ 	struct net_device *dev = qdisc_dev(sch);
+ 
+ 	pr_debug(KBUILD_BASENAME " : %s : qdisc %X\n", __func__, sch->handle);
+@@ -891,8 +991,7 @@ static int ceetm_init_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 	priv->wbfs.qcount = qopt->qcount;
+ 	priv->wbfs.cr = qopt->cr;
+ 	priv->wbfs.er = qopt->er;
+-
+-	channel = root_cl->root.ch;
++	priv->wbfs.ch = parent_cl->ch;
+ 
+ 	/* Configure the hardware wbfs channel groups */
+ 	if (priv->wbfs.qcount == CEETM_MAX_WBFS_QCOUNT) {
+@@ -907,7 +1006,7 @@ static int ceetm_init_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 		/* Configure the group B */
+ 		priv->wbfs.group_type = WBFS_GRP_B;
+ 
+-		err = qman_ceetm_channel_get_group(channel, &small_group,
++		err = qman_ceetm_channel_get_group(priv->wbfs.ch, &small_group,
+ 						   &prio_a, &prio_b);
+ 		if (err) {
+ 			pr_err(KBUILD_BASENAME " : %s : failed to get group details\n",
+@@ -925,7 +1024,7 @@ static int ceetm_init_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 		/* Configure the small group A */
+ 		priv->wbfs.group_type = WBFS_GRP_A;
+ 
+-		err = qman_ceetm_channel_get_group(channel, &small_group,
++		err = qman_ceetm_channel_get_group(priv->wbfs.ch, &small_group,
+ 						   &prio_a, &prio_b);
+ 		if (err) {
+ 			pr_err(KBUILD_BASENAME " : %s : failed to get group details\n",
+@@ -940,13 +1039,13 @@ static int ceetm_init_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 		prio_b = prio_b ? : prio_a;
+ 	}
+ 
+-	err = qman_ceetm_channel_set_group(channel, small_group, prio_a,
++	err = qman_ceetm_channel_set_group(priv->wbfs.ch, small_group, prio_a,
+ 					   prio_b);
+ 	if (err)
+ 		goto err_init_wbfs;
+ 
+ 	if (priv->shaped) {
+-		err = qman_ceetm_channel_set_group_cr_eligibility(channel,
++		err = qman_ceetm_channel_set_group_cr_eligibility(priv->wbfs.ch,
+ 								  group_b,
+ 								priv->wbfs.cr);
+ 		if (err) {
+@@ -955,7 +1054,7 @@ static int ceetm_init_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 			goto err_init_wbfs;
+ 		}
+ 
+-		err = qman_ceetm_channel_set_group_er_eligibility(channel,
++		err = qman_ceetm_channel_set_group_er_eligibility(priv->wbfs.ch,
+ 								  group_b,
+ 								priv->wbfs.er);
+ 		if (err) {
+@@ -991,13 +1090,14 @@ static int ceetm_init_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 		child_cl->wbfs.fq = NULL;
+ 		child_cl->wbfs.cq = NULL;
+ 		child_cl->wbfs.weight = qopt->qweight[i];
++		child_cl->ch = priv->wbfs.ch;
+ 
+ 		if (priv->wbfs.group_type == WBFS_GRP_B)
+ 			id = WBFS_GRP_B_OFFSET + i;
+ 		else
+ 			id = WBFS_GRP_A_OFFSET + i;
+ 
+-		err = ceetm_config_wbfs_cls(child_cl, dev, channel, id,
++		err = ceetm_config_wbfs_cls(child_cl, dev, id,
+ 					    priv->wbfs.group_type);
+ 		if (err) {
+ 			pr_err(KBUILD_BASENAME " : %s : failed to configure the ceetm wbfs class %X\n",
+@@ -1083,7 +1183,10 @@ static int ceetm_init(struct Qdisc *sch, struct nlattr *opt)
+ 
+ 	switch (priv->type) {
+ 	case CEETM_ROOT:
++		netif_tx_stop_all_queues(dev);
++		dpaa_drain_fqs(dev);
+ 		ret = ceetm_init_root(sch, priv, qopt);
++		netif_tx_wake_all_queues(dev);
+ 		break;
+ 	case CEETM_PRIO:
+ 		ret = ceetm_init_prio(sch, priv, qopt);
+@@ -1159,9 +1262,6 @@ static int ceetm_change_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ {
+ 	int err;
+ 	bool group_b;
+-	struct qm_ceetm_channel *channel;
+-	struct ceetm_class *prio_class, *root_class;
+-	struct ceetm_qdisc *prio_qdisc;
+ 
+ 	if (qopt->qcount) {
+ 		pr_err("CEETM: the qcount can not be modified\n");
+@@ -1187,14 +1287,10 @@ static int ceetm_change_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 	if (!priv->shaped)
+ 		return 0;
+ 
+-	prio_class = priv->wbfs.parent;
+-	prio_qdisc = qdisc_priv(prio_class->parent);
+-	root_class = prio_qdisc->prio.parent;
+-	channel = root_class->root.ch;
+ 	group_b = priv->wbfs.group_type == WBFS_GRP_B;
+ 
+ 	if (qopt->cr != priv->wbfs.cr) {
+-		err = qman_ceetm_channel_set_group_cr_eligibility(channel,
++		err = qman_ceetm_channel_set_group_cr_eligibility(priv->wbfs.ch,
+ 								  group_b,
+ 								  qopt->cr);
+ 		if (err)
+@@ -1203,7 +1299,7 @@ static int ceetm_change_wbfs(struct Qdisc *sch, struct ceetm_qdisc *priv,
+ 	}
+ 
+ 	if (qopt->er != priv->wbfs.er) {
+-		err = qman_ceetm_channel_set_group_er_eligibility(channel,
++		err = qman_ceetm_channel_set_group_er_eligibility(priv->wbfs.ch,
+ 								  group_b,
+ 								  qopt->er);
+ 		if (err)
+@@ -1273,7 +1369,10 @@ static int ceetm_change(struct Qdisc *sch, struct nlattr *opt)
+ 	return ret;
+ }
+ 
+-/* Attach the underlying pfifo qdiscs */
++/* Graft the underlying pfifo qdiscs to the netdev queues.
++ * It's safe to remove our references at this point, since the kernel will
++ * destroy the qdiscs on its own and no cleanup from our part is required.
++ */
+ static void ceetm_attach(struct Qdisc *sch)
+ {
+ 	struct net_device *dev = qdisc_dev(sch);
+@@ -1289,6 +1388,9 @@ static void ceetm_attach(struct Qdisc *sch)
+ 		if (old_qdisc)
+ 			qdisc_destroy(old_qdisc);
+ 	}
++
++	kfree(priv->root.qdiscs);
++	priv->root.qdiscs = NULL;
+ }
+ 
+ static unsigned long ceetm_cls_get(struct Qdisc *sch, u32 classid)
+@@ -1331,7 +1433,7 @@ static int ceetm_cls_change_root(struct ceetm_class *cl,
+ 
+ 	if (cl->shaped && cl->root.rate != copt->rate) {
+ 		bps = copt->rate << 3; /* Bps -> bps */
+-		err = qman_ceetm_channel_set_commit_rate_bps(cl->root.ch, bps,
++		err = qman_ceetm_channel_set_commit_rate_bps(cl->ch, bps,
+ 							     dev->mtu);
+ 		if (err)
+ 			goto change_cls_err;
+@@ -1340,7 +1442,7 @@ static int ceetm_cls_change_root(struct ceetm_class *cl,
+ 
+ 	if (cl->shaped && cl->root.ceil != copt->ceil) {
+ 		bps = copt->ceil << 3; /* Bps -> bps */
+-		err = qman_ceetm_channel_set_excess_rate_bps(cl->root.ch, bps,
++		err = qman_ceetm_channel_set_excess_rate_bps(cl->ch, bps,
+ 							     dev->mtu);
+ 		if (err)
+ 			goto change_cls_err;
+@@ -1348,7 +1450,7 @@ static int ceetm_cls_change_root(struct ceetm_class *cl,
+ 	}
+ 
+ 	if (!cl->shaped && cl->root.tbl != copt->tbl) {
+-		err = qman_ceetm_channel_set_weight(cl->root.ch, copt->tbl);
++		err = qman_ceetm_channel_set_weight(cl->ch, copt->tbl);
+ 		if (err)
+ 			goto change_cls_err;
+ 		cl->root.tbl = copt->tbl;
+@@ -1543,7 +1645,7 @@ static int ceetm_cls_change(struct Qdisc *sch, u32 classid, u32 parentid,
+ 		goto claim_err;
+ 	}
+ 
+-	cl->root.ch = channel;
++	cl->ch = channel;
+ 
+ 	if (cl->shaped) {
+ 		/* Configure the channel shaper */
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_ceetm.h b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_ceetm.h
+index 0ff3b9de..431703c8 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_ceetm.h
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_ceetm.h
+@@ -121,12 +121,14 @@ struct root_q {
+ struct prio_q {
+ 	__u16 qcount;
+ 	struct ceetm_class *parent;
++	struct qm_ceetm_channel *ch;
+ };
+ 
+ struct wbfs_q {
+ 	__u16 qcount;
+ 	int group_type;
+ 	struct ceetm_class *parent;
++	struct qm_ceetm_channel *ch;
+ 	__u16 cr;
+ 	__u16 er;
+ };
+@@ -164,7 +166,6 @@ struct root_c {
+ 	bool wbfs_grp_b;
+ 	bool wbfs_grp_large;
+ 	struct Qdisc *child;
+-	struct qm_ceetm_channel *ch;
+ };
+ 
+ struct prio_c {
+@@ -193,6 +194,7 @@ struct ceetm_class {
+ 	int refcnt; /* usage count of this class */
+ 	struct tcf_proto *filter_list; /* class attached filters */
+ 	struct Qdisc *parent;
++	struct qm_ceetm_channel *ch;
+ 	bool shaped;
+ 	int type; /* ROOT/PRIO/WBFS */
+ 	union {
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.c
+index bbaf29c7..3917c206 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.c
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.c
+@@ -265,15 +265,6 @@ int dpa_change_mtu(struct net_device *net_dev, int new_mtu)
+ {
+ 	int max_mtu = dpa_get_max_mtu();
+ 
+-#ifndef CONFIG_PPC
+-	/* Due to the A010022 FMan errata, we can not use contig frames larger
+-	 * than 4K, nor S/G frames. We need to prevent the user from setting a
+-	 * large MTU.
+-	 */
+-	if (unlikely(dpaa_errata_a010022))
+-		max_mtu = DPA_BP_RAW_SIZE;
+-#endif
+-
+ 	/* Make sure we don't exceed the Ethernet controller's MAXFRM */
+ 	if (new_mtu < 68 || new_mtu > max_mtu) {
+ 		netdev_err(net_dev, "Invalid L3 mtu %d (must be between %d and %d).\n",
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.h b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.h
+index b0393282..abada8c4 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.h
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.h
+@@ -55,10 +55,12 @@
+ 	fm_set_##type##_port_params(port, &param); \
+ }
+ 
++/* The SGT needs to be 256 bytes long. Even if the table has only one entry,
++ * the FMan will read 256 bytes from its start.
++ */
++#define DPA_SGT_SIZE 256
+ #define DPA_SGT_MAX_ENTRIES 16 /* maximum number of entries in SG Table */
+ 
+-#define DPA_SGT_ENTRIES_THRESHOLD	DPA_SGT_MAX_ENTRIES
+-
+ #define DPA_BUFF_RELEASE_MAX 8 /* maximum number of buffers released at once */
+ 
+ #define DPA_RX_PCD_HI_PRIO_FQ_INIT_FAIL(dpa_fq, _errno) \
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
+index 138919aa..0e30d5c9 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
+@@ -82,8 +82,8 @@ static void dpa_bp_recycle_frag(struct dpa_bp *dpa_bp, unsigned long vaddr,
+ 
+ static int _dpa_bp_add_8_bufs(const struct dpa_bp *dpa_bp)
+ {
++	void *new_buf, *fman_buf;
+ 	struct bm_buffer bmb[8];
+-	void *new_buf;
+ 	dma_addr_t addr;
+ 	uint8_t i;
+ 	struct device *dev = dpa_bp->dev;
+@@ -113,21 +113,37 @@ static int _dpa_bp_add_8_bufs(const struct dpa_bp *dpa_bp)
+ 
+ 		if (unlikely(!new_buf))
+ 			goto netdev_alloc_failed;
+-		new_buf = PTR_ALIGN(new_buf + SMP_CACHE_BYTES, SMP_CACHE_BYTES);
++		new_buf = PTR_ALIGN(new_buf, SMP_CACHE_BYTES);
+ 
+-		skb = build_skb(new_buf, DPA_SKB_SIZE(dpa_bp->size) +
+-			SKB_DATA_ALIGN(sizeof(struct skb_shared_info)));
++		/* Apart from the buffer that will be used by the FMan, the
++		 * skb also guarantees enough space to hold the backpointer
++		 * in the headroom and the shared info at the end.
++		 */
++		skb = build_skb(new_buf,
++				SMP_CACHE_BYTES + DPA_SKB_SIZE(dpa_bp->size) +
++				SKB_DATA_ALIGN(sizeof(struct skb_shared_info)));
+ 		if (unlikely(!skb)) {
+ 			put_page(virt_to_head_page(new_buf));
+ 			goto build_skb_failed;
+ 		}
+ 
+-		/* Store the skb back-pointer before the start of the buffer.
+-		 * Otherwise it will be overwritten by the FMan.
++		/* Reserve SMP_CACHE_BYTES in the skb's headroom to store the
++		 * backpointer. This area will not be synced to, or
++		 * overwritten by, the FMan.
+ 		 */
+-		DPA_WRITE_SKB_PTR(skb, skbh, new_buf, -1);
++		skb_reserve(skb, SMP_CACHE_BYTES);
++
++		/* We don't sync the first SMP_CACHE_BYTES of the buffer to
++		 * the FMan. The skb backpointer is stored at the end of the
++		 * reserved headroom. Otherwise it will be overwritten by the
++		 * FMan.
++		 * The buffer synced with the FMan starts right after the
++		 * reserved headroom.
++		 */
++		fman_buf = new_buf + SMP_CACHE_BYTES;
++		DPA_WRITE_SKB_PTR(skb, skbh, fman_buf, -1);
+ 
+-		addr = dma_map_single(dev, new_buf,
++		addr = dma_map_single(dev, fman_buf,
+ 				dpa_bp->size, DMA_BIDIRECTIONAL);
+ 		if (unlikely(dma_mapping_error(dev, addr)))
+ 			goto dma_map_failed;
+@@ -246,8 +262,8 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
+ 
+ 	if (unlikely(fd->format == qm_fd_sg)) {
+ 		nr_frags = skb_shinfo(skb)->nr_frags;
+-		dma_unmap_single(dpa_bp->dev, addr, dpa_fd_offset(fd) +
+-				 sizeof(struct qm_sg_entry) * (1 + nr_frags),
++		dma_unmap_single(dpa_bp->dev, addr,
++				 dpa_fd_offset(fd) + DPA_SGT_SIZE,
+ 				 dma_dir);
+ 
+ 		/* The sgt buffer has been allocated with netdev_alloc_frag(),
+@@ -407,12 +423,6 @@ static struct sk_buff *__hot contig_fd_to_skb(const struct dpa_priv_s *priv,
+ 	 * warn us that the frame length is larger than the truesize. We
+ 	 * bypass the warning.
+ 	 */
+-#ifndef CONFIG_PPC
+-	/* We do not support Jumbo frames on LS1043 and thus we edit
+-	 * the skb truesize only when the 4k errata is not present.
+-	 */
+-	if (likely(!dpaa_errata_a010022))
+-#endif
+ 	skb->truesize = SKB_TRUESIZE(dpa_fd_length(fd));
+ #endif
+ 
+@@ -483,7 +493,6 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
+ 				 DMA_BIDIRECTIONAL);
+ 		if (i == 0) {
+ 			DPA_READ_SKB_PTR(skb, skbh, sg_vaddr, -1);
+-			DPA_BUG_ON(skb->head != sg_vaddr);
+ #ifdef CONFIG_FSL_DPAA_1588
+ 			if (priv->tsu && priv->tsu->valid &&
+ 			    priv->tsu->hwts_rx_en_ioctl)
+@@ -809,35 +818,30 @@ static struct sk_buff *a010022_realign_skb(struct sk_buff *skb,
+ {
+ 	int trans_offset = skb_transport_offset(skb);
+ 	int net_offset = skb_network_offset(skb);
++	int nsize, headroom, npage_order;
+ 	struct sk_buff *nskb = NULL;
+-	int nsize, headroom;
+ 	struct page *npage;
+ 	void *npage_addr;
+ 
+-	/* Guarantee the minimum required headroom */
+-	if (skb_headroom(skb) >= priv->tx_headroom)
+-		headroom = skb_headroom(skb);
+-	else
+-		headroom = priv->tx_headroom;
++	headroom = DPAA_A010022_HEADROOM;
+ 
+-	npage = alloc_page(GFP_ATOMIC);
++	/* For the new skb we only need the old one's data (both non-paged and
++	 * paged). We can skip the old tailroom.
++	 *
++	 * Make sure the skb_shinfo is cache-line aligned.
++	 */
++	nsize = SMP_CACHE_BYTES + DPA_SKB_SIZE(headroom + skb->len) +
++		SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
++
++	/* Reserve enough memory to accommodate Jumbo frames */
++	npage_order = (nsize - 1) / PAGE_SIZE;
++	npage = alloc_pages(GFP_ATOMIC | __GFP_COMP, npage_order);
+ 	if (unlikely(!npage)) {
+ 		WARN_ONCE(1, "Memory allocation failure\n");
+ 		return NULL;
+ 	}
+ 	npage_addr = page_address(npage);
+ 
+-	/* For the new skb we only need the old one's data (both non-paged and
+-	 * paged) and a headroom large enough to fit our private info. We can
+-	 * skip the old tailroom.
+-	 *
+-	 * Make sure the new linearized buffer will not exceed a page's size.
+-	 */
+-	nsize = headroom + skb->len +
+-		SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+-	if (unlikely(nsize > 4096))
+-		goto err;
+-
+ 	nskb = build_skb(npage_addr, nsize);
+ 	if (unlikely(!nskb))
+ 		goto err;
+@@ -846,7 +850,7 @@ static struct sk_buff *a010022_realign_skb(struct sk_buff *skb,
+ 	 * alignment.
+ 	 * Code borrowed and adapted from skb_copy().
+ 	 */
+-	skb_reserve(nskb, priv->tx_headroom);
++	skb_reserve(nskb, headroom);
+ 	skb_put(nskb, skb->len);
+ 	if (skb_copy_bits(skb, 0, nskb->data, skb->len)) {
+ 		WARN_ONCE(1, "skb parsing failure\n");
+@@ -906,7 +910,11 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
+ 	nr_frags = skb_shinfo(skb)->nr_frags;
+ 	fd->format = qm_fd_sg;
+ 
+-	sgt_size = sizeof(struct qm_sg_entry) * (1 + nr_frags);
++	/* The FMan reads 256 bytes from the start of the SGT regardless of
++	 * its size. In accordance, we reserve the same amount of memory as
++	 * well.
++	 */
++	sgt_size = DPA_SGT_SIZE;
+ 
+ 	/* Get a page frag to store the SGTable, or a full page if the errata
+ 	 * is in place and we need to avoid crossing a 4k boundary.
+@@ -1113,6 +1121,11 @@ int __hot dpa_tx_extended(struct sk_buff *skb, struct net_device *net_dev,
+ 				percpu_stats->tx_errors++;
+ 				return NETDEV_TX_OK;
+ 			}
++
++			/* propagate the skb ownership information */
++			if (skb->sk)
++				skb_set_owner_w(skb_new, skb->sk);
++
+ 			dev_kfree_skb(skb);
+ 			skb = skb_new;
+ 		}
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/mac-api.c b/drivers/net/ethernet/freescale/sdk_dpaa/mac-api.c
+index 2c5652d9..28709234 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/mac-api.c
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/mac-api.c
+@@ -502,17 +502,41 @@ static int memac_init_phy(struct net_device *net_dev,
+ 			  struct mac_device *mac_dev)
+ {
+ 	struct phy_device       *phy_dev;
++	void (*adjust_link_handler)(struct net_device *);
+ 
+ 	if ((macdev2enetinterface(mac_dev) == e_ENET_MODE_XGMII_10000) ||
+-	    (macdev2enetinterface(mac_dev) == e_ENET_MODE_SGMII_2500) ||
+-	    of_phy_is_fixed_link(mac_dev->phy_node)) {
+-		phy_dev = of_phy_connect(net_dev, mac_dev->phy_node,
+-					 &adjust_link_void, 0,
+-					 mac_dev->phy_if);
++	    (macdev2enetinterface(mac_dev) == e_ENET_MODE_SGMII_2500)) {
++		/* Pass a void link state handler to the PHY state machine
++		 * for XGMII (10G) and SGMII 2.5G, as the hardware does not
++		 * permit dynamic link speed adjustments. */
++		adjust_link_handler = adjust_link_void;
++	} else if (macdev2enetinterface(mac_dev) & e_ENET_IF_RGMII) {
++		/* Regular RGMII ports connected to a PHY, as well as
++		 * ports that are marked as "fixed-link" in the DTS,
++		 * will have the adjust_link callback. This calls
++		 * fman_memac_adjust_link in order to configure the
++		 * IF_MODE register, which is needed in both cases.
++		 */
++		adjust_link_handler = adjust_link;
++	} else if (of_phy_is_fixed_link(mac_dev->phy_node)) {
++		/* Pass a void link state handler for fixed-link
++		 * interfaces that are not RGMII. Only RGMII has been
++		 * tested and confirmed to work with fixed-link. Other
++		 * MII interfaces may need further work.
++		 * TODO: Change this as needed.
++		 */
++		adjust_link_handler = adjust_link_void;
+ 	} else {
+-		phy_dev = of_phy_connect(net_dev, mac_dev->phy_node,
+-					 &adjust_link, 0, mac_dev->phy_if);
++		/* MII, RMII, SMII, GMII, SGMII, BASEX ports,
++		 * that are NOT fixed-link.
++		 * TODO: May not be needed for interfaces that
++		 * pass through the SerDes block (*SGMII, XFI).
++		 */
++		adjust_link_handler = adjust_link;
+ 	}
++	phy_dev = of_phy_connect(net_dev, mac_dev->phy_node,
++	                         adjust_link_handler, 0,
++	                         mac_dev->phy_if);
+ 
+ 	if (unlikely(phy_dev == NULL) || IS_ERR(phy_dev)) {
+ 		netdev_err(net_dev, "Could not connect to PHY %s\n",
+diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/mac.c b/drivers/net/ethernet/freescale/sdk_dpaa/mac.c
+index a6bfceaf..f0fb3a8c 100644
+--- a/drivers/net/ethernet/freescale/sdk_dpaa/mac.c
++++ b/drivers/net/ethernet/freescale/sdk_dpaa/mac.c
+@@ -133,10 +133,10 @@ static int __cold free_macdev(struct mac_device *mac_dev)
+ 
+ static const struct of_device_id mac_match[] = {
+ 	[DTSEC] = {
+-		.compatible	= "fsl,fman-1g-mac"
++		.compatible	= "fsl,fman-dtsec"
+ 	},
+ 	[XGMAC] = {
+-		.compatible	= "fsl,fman-10g-mac"
++		.compatible	= "fsl,fman-xgec"
+ 	},
+ 	[MEMAC] = {
+ 		.compatible	= "fsl,fman-memac"
+diff --git a/drivers/net/ethernet/freescale/sdk_fman/Peripherals/FM/MAC/dtsec.c b/drivers/net/ethernet/freescale/sdk_fman/Peripherals/FM/MAC/dtsec.c
+index 38948f97..071fa462 100644
+--- a/drivers/net/ethernet/freescale/sdk_fman/Peripherals/FM/MAC/dtsec.c
++++ b/drivers/net/ethernet/freescale/sdk_fman/Peripherals/FM/MAC/dtsec.c
+@@ -386,6 +386,7 @@ static void FreeInitResources(t_Dtsec *p_Dtsec)
+ static t_Error GracefulStop(t_Dtsec *p_Dtsec, e_CommMode mode)
+ {
+     struct dtsec_regs *p_MemMap;
++    int pollTimeout = 0;
+ 
+     ASSERT_COND(p_Dtsec);
+ 
+@@ -408,16 +409,32 @@ static t_Error GracefulStop(t_Dtsec *p_Dtsec, e_CommMode mode)
+     }
+ 
+     if (mode & e_COMM_MODE_TX)
+-#if defined(FM_GTS_ERRATA_DTSEC_A004) || defined(FM_GTS_AFTER_MAC_ABORTED_FRAME_ERRATA_DTSEC_A0012)
+-    if (p_Dtsec->fmMacControllerDriver.fmRevInfo.majorRev == 2)
+-        DBG(INFO, ("GTS not supported due to DTSEC_A004 errata."));
+-#else  /* not defined(FM_GTS_ERRATA_DTSEC_A004) ||... */
+-#ifdef FM_GTS_UNDERRUN_ERRATA_DTSEC_A0014
+-        DBG(INFO, ("GTS not supported due to DTSEC_A0014 errata."));
+-#else  /* FM_GTS_UNDERRUN_ERRATA_DTSEC_A0014 */
++    {
++#if defined(FM_GTS_ERRATA_DTSEC_A004)
++        if (p_Dtsec->fmMacControllerDriver.fmRevInfo.majorRev == 2)
++            DBG(INFO, ("GTS not supported due to DTSEC_A004 errata."));
++#else  /* not defined(FM_GTS_ERRATA_DTSEC_A004) */
++
+         fman_dtsec_stop_tx(p_MemMap);
+-#endif /* FM_GTS_UNDERRUN_ERRATA_DTSEC_A0014 */
+-#endif /* defined(FM_GTS_ERRATA_DTSEC_A004) ||...  */
++
++#if defined(FM_GTS_UNDERRUN_ERRATA_DTSEC_A0014) || defined(FM_GTS_AFTER_MAC_ABORTED_FRAME_ERRATA_DTSEC_A0012)
++        XX_UDelay(10);
++#endif /* FM_GTS_UNDERRUN_ERRATA_DTSEC_A0014 || FM_GTS_AFTER_MAC_ABORTED_FRAME_ERRATA_DTSEC_A0012 */
++#endif /* defined(FM_GTS_ERRATA_DTSEC_A004) */
++    }
++
++    /* Poll GRSC/GTSC bits in IEVENT register until both are set */
++#if defined(FM_GRS_ERRATA_DTSEC_A002) || defined(FM_GTS_ERRATA_DTSEC_A004) || defined(FM_GTS_AFTER_MAC_ABORTED_FRAME_ERRATA_DTSEC_A0012) || defined(FM_GTS_UNDERRUN_ERRATA_DTSEC_A0014) || defined(FM_GTS_AFTER_DROPPED_FRAME_ERRATA_DTSEC_A004839)
++    XX_UDelay(10);
++#else
++    while (fman_dtsec_get_event(p_MemMap, DTSEC_IMASK_GRSCEN | DTSEC_IMASK_GTSCEN) != (DTSEC_IMASK_GRSCEN | DTSEC_IMASK_GTSCEN))
++    {
++        if (pollTimeout == 100)
++            break;
++        XX_UDelay(1);
++        pollTimeout++;
++    }
++#endif
+ 
+     return E_OK;
+ }
+@@ -632,7 +649,12 @@ static t_Error DtsecSetTxPauseFrames(t_Handle h_Dtsec,
+                       " value should be greater than 320."));
+ #endif /* FM_BAD_TX_TS_IN_B_2_B_ERRATA_DTSEC_A003 */
+ 
++    GracefulStop(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     fman_dtsec_set_tx_pause_frames(p_Dtsec->p_MemMap, pauseTime);
++
++    GracefulRestart(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     return E_OK;
+ }
+ 
+@@ -653,8 +675,12 @@ static t_Error DtsecRxIgnoreMacPause(t_Handle h_Dtsec, bool en)
+     SANITY_CHECK_RETURN_ERROR(p_Dtsec, E_INVALID_STATE);
+     SANITY_CHECK_RETURN_ERROR(!p_Dtsec->p_DtsecDriverParam, E_INVALID_STATE);
+ 
++    GracefulStop(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     fman_dtsec_handle_rx_pause(p_Dtsec->p_MemMap, accept_pause);
+ 
++    GracefulRestart(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     return E_OK;
+ }
+ 
+@@ -787,8 +813,13 @@ static t_Error DtsecModifyMacAddress (t_Handle h_Dtsec, t_EnetAddr *p_EnetAddr)
+     /* Initialize MAC Station Address registers (1 & 2)    */
+     /* Station address have to be swapped (big endian to little endian */
+     p_Dtsec->addr = ENET_ADDR_TO_UINT64(*p_EnetAddr);
++
++    GracefulStop(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     fman_dtsec_set_mac_address(p_Dtsec->p_MemMap, (uint8_t *)(*p_EnetAddr));
+ 
++    GracefulRestart(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     return E_OK;
+ }
+ 
+@@ -1076,8 +1107,12 @@ static t_Error DtsecSetWakeOnLan(t_Handle h_Dtsec, bool en)
+     SANITY_CHECK_RETURN_ERROR(p_Dtsec, E_INVALID_STATE);
+     SANITY_CHECK_RETURN_ERROR(!p_Dtsec->p_DtsecDriverParam, E_INVALID_STATE);
+ 
++    GracefulStop(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     fman_dtsec_set_wol(p_Dtsec->p_MemMap, en);
+ 
++    GracefulRestart(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     return E_OK;
+ }
+ 
+@@ -1098,11 +1133,15 @@ static t_Error DtsecAdjustLink(t_Handle h_Dtsec, e_EnetSpeed speed, bool fullDup
+     enet_speed = (enum enet_speed) ENET_SPEED_FROM_MODE(p_Dtsec->enetMode);
+     p_Dtsec->halfDuplex = !fullDuplex;
+ 
++    GracefulStop(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     err = fman_dtsec_adjust_link(p_Dtsec->p_MemMap, enet_interface, enet_speed, fullDuplex);
+ 
+     if (err == -EINVAL)
+         RETURN_ERROR(MAJOR, E_CONFLICT, ("Ethernet interface does not support Half Duplex mode"));
+ 
++    GracefulRestart(p_Dtsec, e_COMM_MODE_RX_AND_TX);
++
+     return (t_Error)err;
+ }
+ 
+diff --git a/drivers/net/ethernet/freescale/sdk_fman/src/wrapper/lnxwrp_fm.c b/drivers/net/ethernet/freescale/sdk_fman/src/wrapper/lnxwrp_fm.c
+index 31f654b4..c5fc266e 100644
+--- a/drivers/net/ethernet/freescale/sdk_fman/src/wrapper/lnxwrp_fm.c
++++ b/drivers/net/ethernet/freescale/sdk_fman/src/wrapper/lnxwrp_fm.c
+@@ -687,9 +687,9 @@ static t_LnxWrpFmDev * ReadFmDevTreeNode (struct platform_device *of_dev)
+     if (WARN_ON(strlen("ptp-timer") >= sizeof(ids[0].name)))
+         return NULL;
+     strcpy(ids[0].name, "ptp-timer");
+-    if (WARN_ON(strlen("fsl,fman-rtc") >= sizeof(ids[0].compatible)))
++    if (WARN_ON(strlen("fsl,fman-ptp-timer") >= sizeof(ids[0].compatible)))
+         return NULL;
+-    strcpy(ids[0].compatible, "fsl,fman-rtc");
++    strcpy(ids[0].compatible, "fsl,fman-ptp-timer");
+     for_each_child_of_node(fm_node, dev_node) {
+         if (likely(of_match_node(ids, dev_node) != NULL)) {
+             _errno = of_address_to_resource(dev_node, 0, &res);
+@@ -920,7 +920,7 @@ static t_Error ConfigureFmDev(t_LnxWrpFmDev  *p_LnxWrpFmDev)
+ 
+     if (p_LnxWrpFmDev->fmRtcPhysBaseAddr)
+     {
+-        dev_res = __devm_request_region(p_LnxWrpFmDev->dev, p_LnxWrpFmDev->res, p_LnxWrpFmDev->fmRtcPhysBaseAddr, p_LnxWrpFmDev->fmRtcMemSize, "fman-rtc");
++        dev_res = __devm_request_region(p_LnxWrpFmDev->dev, p_LnxWrpFmDev->res, p_LnxWrpFmDev->fmRtcPhysBaseAddr, p_LnxWrpFmDev->fmRtcMemSize, "fman-ptp-timer");
+         if (unlikely(dev_res == NULL))
+             RETURN_ERROR(MAJOR, E_INVALID_STATE, ("__devm_request_region() failed"));
+ 
+diff --git a/drivers/net/ethernet/freescale/sdk_fman/src/wrapper/lnxwrp_fm_port.c b/drivers/net/ethernet/freescale/sdk_fman/src/wrapper/lnxwrp_fm_port.c
+index 00ab4bcb..c76b9c05 100644
+--- a/drivers/net/ethernet/freescale/sdk_fman/src/wrapper/lnxwrp_fm_port.c
++++ b/drivers/net/ethernet/freescale/sdk_fman/src/wrapper/lnxwrp_fm_port.c
+@@ -303,7 +303,21 @@ static t_LnxWrpFmPortDev *ReadFmPortDevTreeNode(struct platform_device
+ 	tmp_prop = be32_to_cpu(*uint32_prop);
+ 	if (WARN_ON(lenp != sizeof(uint32_t)))
+ 		return NULL;
+-	if (of_device_is_compatible(port_node, "fsl,fman-port-oh")) {
++	if (of_device_is_compatible(port_node, "fsl,fman-port-oh") ||
++	    of_device_is_compatible(port_node, "fsl,fman-v2-port-oh") ||
++	    of_device_is_compatible(port_node, "fsl,fman-v3-port-oh")) {
++#ifndef CONFIG_FMAN_ARM
++#ifdef CONFIG_FMAN_P3040_P4080_P5020
++		/* On PPC FMan v2, OH ports start from cell-index 0x1 */
++		tmp_prop -= 0x1;
++#else
++		/* On PPC FMan v3 (Low and High), OH ports start from
++		 * cell-index 0x2
++		 */
++		tmp_prop -= 0x2;
++#endif // CONFIG_FMAN_P3040_P4080_P5020
++#endif // CONFIG_FMAN_ARM
++
+ 		if (unlikely(tmp_prop >= FM_MAX_NUM_OF_OH_PORTS)) {
+ 			REPORT_ERROR(MAJOR, E_INVALID_VALUE,
+ 				     ("of_get_property(%s, cell-index) failed",
+@@ -388,7 +402,14 @@ static t_LnxWrpFmPortDev *ReadFmPortDevTreeNode(struct platform_device
+ 			settings.param.specificParams.nonRxParams.qmChannel =
+ 			p_LnxWrpFmPortDev->txCh;
+ 	} else if (of_device_is_compatible(port_node, "fsl,fman-port-10g-tx")) {
++#ifndef CONFIG_FMAN_ARM
++		/* On T102x, the 10G TX port IDs start from 0x28 */
++		if (IS_T1023_T1024)
++			tmp_prop -= 0x28;
++		else
++#endif
+ 		tmp_prop -= 0x30;
++
+ 		if (unlikely(tmp_prop>= FM_MAX_NUM_OF_10G_TX_PORTS)) {
+ 			REPORT_ERROR(MAJOR, E_INVALID_VALUE,
+ 					("of_get_property(%s, cell-index) failed",
+@@ -399,7 +420,7 @@ static t_LnxWrpFmPortDev *ReadFmPortDevTreeNode(struct platform_device
+ 			FM_MAX_NUM_OF_1G_TX_PORTS];
+ #ifndef CONFIG_FMAN_ARM
+ 		if (IS_T1023_T1024)
+-			p_LnxWrpFmPortDev = &p_LnxWrpFmDev->txPorts[*uint32_prop];
++			p_LnxWrpFmPortDev = &p_LnxWrpFmDev->txPorts[tmp_prop];
+ #endif
+ 
+ 		p_LnxWrpFmPortDev->id = tmp_prop;
+@@ -437,7 +458,14 @@ static t_LnxWrpFmPortDev *ReadFmPortDevTreeNode(struct platform_device
+ 		if (p_LnxWrpFmDev->pcdActive)
+ 			p_LnxWrpFmPortDev->defPcd = p_LnxWrpFmDev->defPcd;
+ 	} else if (of_device_is_compatible(port_node, "fsl,fman-port-10g-rx")) {
++#ifndef CONFIG_FMAN_ARM
++		/* On T102x, the 10G RX port IDs start from 0x08 */
++		if (IS_T1023_T1024)
++			tmp_prop -= 0x8;
++		else
++#endif
+ 		tmp_prop -= 0x10;
++
+ 		if (unlikely(tmp_prop >= FM_MAX_NUM_OF_10G_RX_PORTS)) {
+ 			REPORT_ERROR(MAJOR, E_INVALID_VALUE,
+ 					("of_get_property(%s, cell-index) failed",
+@@ -449,7 +477,7 @@ static t_LnxWrpFmPortDev *ReadFmPortDevTreeNode(struct platform_device
+ 
+ #ifndef CONFIG_FMAN_ARM
+ 		if (IS_T1023_T1024)
+-			p_LnxWrpFmPortDev = &p_LnxWrpFmDev->rxPorts[*uint32_prop];
++			p_LnxWrpFmPortDev = &p_LnxWrpFmDev->rxPorts[tmp_prop];
+ #endif
+ 
+ 		p_LnxWrpFmPortDev->id = tmp_prop;
+@@ -637,7 +665,7 @@ static t_Error CheckNConfigFmPortAdvArgs (t_LnxWrpFmPortDev *p_LnxWrpFmPortDev)
+     uint32_prop = (uint32_t *)of_get_property(port_node, "ar-tables-sizes",
+ 	&lenp);
+     if (uint32_prop) {
+-    
++
+     	if (WARN_ON(lenp != sizeof(uint32_t)*8))
+             RETURN_ERROR(MINOR, E_INVALID_VALUE, NO_MSG);
+     	if (WARN_ON(p_LnxWrpFmPortDev->settings.param.portType !=
+@@ -671,7 +699,7 @@ static t_Error CheckNConfigFmPortAdvArgs (t_LnxWrpFmPortDev *p_LnxWrpFmPortDev)
+         if (uint32_prop) {
+         	if (WARN_ON(lenp != sizeof(uint32_t)*3))
+                 RETURN_ERROR(MINOR, E_INVALID_VALUE, NO_MSG);
+-         
++
+             p_LnxWrpFmPortDev->dsar_table_sizes.max_num_of_ip_prot_filtering  =
+ 		(uint16_t)be32_to_cpu(uint32_prop[0]);
+             p_LnxWrpFmPortDev->dsar_table_sizes.max_num_of_tcp_port_filtering =
+@@ -679,7 +707,7 @@ static t_Error CheckNConfigFmPortAdvArgs (t_LnxWrpFmPortDev *p_LnxWrpFmPortDev)
+             p_LnxWrpFmPortDev->dsar_table_sizes.max_num_of_udp_port_filtering =
+ 		(uint16_t)be32_to_cpu(uint32_prop[2]);
+         }
+-        
++
+         if ((err = FM_PORT_ConfigDsarSupport(p_LnxWrpFmPortDev->h_Dev,
+ 		(t_FmPortDsarTablesSizes*)&p_LnxWrpFmPortDev->dsar_table_sizes)) != E_OK)
+ 		RETURN_ERROR(MINOR, err, NO_MSG);
+@@ -1418,6 +1446,10 @@ static int fm_port_remove(struct platform_device *of_dev)
+ static const struct of_device_id fm_port_match[] = {
+ 	{
+ 	 .compatible = "fsl,fman-port-oh"},
++	{
++	 .compatible = "fsl,fman-v2-port-oh"},
++	{
++	 .compatible = "fsl,fman-v3-port-oh"},
+ 	{
+ 	 .compatible = "fsl,fman-port-1g-rx"},
+ 	{
+diff --git a/drivers/net/ethernet/freescale/ucc_geth.c b/drivers/net/ethernet/freescale/ucc_geth.c
+index f76d3327..ef9bc26e 100644
+--- a/drivers/net/ethernet/freescale/ucc_geth.c
++++ b/drivers/net/ethernet/freescale/ucc_geth.c
+@@ -2594,11 +2594,10 @@ static int ucc_geth_startup(struct ucc_geth_private *ugeth)
+ 		} else if (ugeth->ug_info->uf_info.bd_mem_part ==
+ 			   MEM_PART_MURAM) {
+ 			out_be32(&ugeth->p_send_q_mem_reg->sqqd[i].bd_ring_base,
+-				 (u32) immrbar_virt_to_phys(ugeth->
+-							    p_tx_bd_ring[i]));
++				 (u32)qe_muram_dma(ugeth->p_tx_bd_ring[i]));
+ 			out_be32(&ugeth->p_send_q_mem_reg->sqqd[i].
+ 				 last_bd_completed_address,
+-				 (u32) immrbar_virt_to_phys(endOfRing));
++				 (u32)qe_muram_dma(endOfRing));
+ 		}
+ 	}
+ 
+@@ -2844,8 +2843,7 @@ static int ucc_geth_startup(struct ucc_geth_private *ugeth)
+ 		} else if (ugeth->ug_info->uf_info.bd_mem_part ==
+ 			   MEM_PART_MURAM) {
+ 			out_be32(&ugeth->p_rx_bd_qs_tbl[i].externalbdbaseptr,
+-				 (u32) immrbar_virt_to_phys(ugeth->
+-							    p_rx_bd_ring[i]));
++				 (u32)qe_muram_dma(ugeth->p_rx_bd_ring[i]));
+ 		}
+ 		/* rest of fields handled by QE */
+ 	}
+diff --git a/drivers/pci/quirks.c b/drivers/pci/quirks.c
+index 5109aaa3..617c196d 100644
+--- a/drivers/pci/quirks.c
++++ b/drivers/pci/quirks.c
+@@ -3323,6 +3323,13 @@ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_ATHEROS, 0x0032, quirk_no_bus_reset);
+ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_ATHEROS, 0x003c, quirk_no_bus_reset);
+ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_ATHEROS, 0x0033, quirk_no_bus_reset);
+ 
++/*
++ * NXP (Freescale Vendor ID) LS1088 chips do not behave correctly after
++ * bus reset. Link state of device does not comes UP and so config space
++ * never accessible again.
++ */
++DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_FREESCALE, 0x80c0, quirk_no_bus_reset);
++
+ static void quirk_no_pm_reset(struct pci_dev *dev)
+ {
+ 	/*
+@@ -3369,22 +3376,29 @@ DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_PORT_RIDGE,
+ 
+ static void quirk_chelsio_extend_vpd(struct pci_dev *dev)
+ {
+-	pci_set_vpd_size(dev, 8192);
++	int chip = (dev->device & 0xf000) >> 12;
++	int func = (dev->device & 0x0f00) >>  8;
++	int prod = (dev->device & 0x00ff) >>  0;
++
++	/*
++	 * If this is a T3-based adapter, there's a 1KB VPD area at offset
++	 * 0xc00 which contains the preferred VPD values.  If this is a T4 or
++	 * later based adapter, the special VPD is at offset 0x400 for the
++	 * Physical Functions (the SR-IOV Virtual Functions have no VPD
++	 * Capabilities).  The PCI VPD Access core routines will normally
++	 * compute the size of the VPD by parsing the VPD Data Structure at
++	 * offset 0x000.  This will result in silent failures when attempting
++	 * to accesses these other VPD areas which are beyond those computed
++	 * limits.
++	 */
++	if (chip == 0x0 && prod >= 0x20)
++		pci_set_vpd_size(dev, 8192);
++	else if (chip >= 0x4 && func < 0x8)
++		pci_set_vpd_size(dev, 2048);
+ }
+ 
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x20, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x21, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x22, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x23, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x24, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x25, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x26, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x30, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x31, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x32, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x35, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x36, quirk_chelsio_extend_vpd);
+-DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, 0x37, quirk_chelsio_extend_vpd);
++DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_CHELSIO, PCI_ANY_ID,
++			quirk_chelsio_extend_vpd);
+ 
+ #ifdef CONFIG_ACPI
+ /*
+@@ -3850,6 +3864,8 @@ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_MARVELL_EXT, 0x9120,
+ 			 quirk_dma_func1_alias);
+ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_MARVELL_EXT, 0x9123,
+ 			 quirk_dma_func1_alias);
++DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_MARVELL_EXT, 0x9128,
++			 quirk_dma_func1_alias);
+ /* https://bugzilla.kernel.org/show_bug.cgi?id=42679#c14 */
+ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_MARVELL_EXT, 0x9130,
+ 			 quirk_dma_func1_alias);
+@@ -3865,11 +3881,16 @@ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_MARVELL_EXT, 0x9182,
+ /* https://bugzilla.kernel.org/show_bug.cgi?id=42679#c46 */
+ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_MARVELL_EXT, 0x91a0,
+ 			 quirk_dma_func1_alias);
++/* https://bugzilla.kernel.org/show_bug.cgi?id=42679#c127 */
++DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_MARVELL_EXT, 0x9220,
++			 quirk_dma_func1_alias);
+ /* https://bugzilla.kernel.org/show_bug.cgi?id=42679#c49 */
+ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_MARVELL_EXT, 0x9230,
+ 			 quirk_dma_func1_alias);
+ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_TTI, 0x0642,
+ 			 quirk_dma_func1_alias);
++DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_TTI, 0x0645,
++			 quirk_dma_func1_alias);
+ /* https://bugs.gentoo.org/show_bug.cgi?id=497630 */
+ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_JMICRON,
+ 			 PCI_DEVICE_ID_JMICRON_JMB388_ESD,
+@@ -4097,6 +4118,9 @@ static int pci_quirk_cavium_acs(struct pci_dev *dev, u16 acs_flags)
+ 	 */
+ 	acs_flags &= ~(PCI_ACS_RR | PCI_ACS_CR | PCI_ACS_SV | PCI_ACS_UF);
+ 
++	if (!((dev->device >= 0xa000) && (dev->device <= 0xa0ff)))
++		return -ENOTTY;
++
+ 	return acs_flags ? 0 : 1;
+ }
+ 
+@@ -4208,11 +4232,29 @@ static int pci_quirk_qcom_rp_acs(struct pci_dev *dev, u16 acs_flags)
+  * 0xa290-0xa29f PCI Express Root port #{0-16}
+  * 0xa2e7-0xa2ee PCI Express Root port #{17-24}
+  *
++ * Mobile chipsets are also affected, 7th & 8th Generation
++ * Specification update confirms ACS errata 22, status no fix: (7th Generation
++ * Intel Processor Family I/O for U/Y Platforms and 8th Generation Intel
++ * Processor Family I/O for U Quad Core Platforms Specification Update,
++ * August 2017, Revision 002, Document#: 334660-002)[6]
++ * Device IDs from I/O datasheet: (7th Generation Intel Processor Family I/O
++ * for U/Y Platforms and 8th Generation Intel  Processor Family I/O for U
++ * Quad Core Platforms, Vol 1 of 2, August 2017, Document#: 334658-003)[7]
++ *
++ * 0x9d10-0x9d1b PCI Express Root port #{1-12}
++ *
++ * The 300 series chipset suffers from the same bug so include those root
++ * ports here as well.
++ *
++ * 0xa32c-0xa343 PCI Express Root port #{0-24}
++ *
+  * [1] http://www.intel.com/content/www/us/en/chipsets/100-series-chipset-datasheet-vol-2.html
+  * [2] http://www.intel.com/content/www/us/en/chipsets/100-series-chipset-datasheet-vol-1.html
+  * [3] http://www.intel.com/content/www/us/en/chipsets/100-series-chipset-spec-update.html
+  * [4] http://www.intel.com/content/www/us/en/chipsets/200-series-chipset-pch-spec-update.html
+  * [5] http://www.intel.com/content/www/us/en/chipsets/200-series-chipset-pch-datasheet-vol-1.html
++ * [6] https://www.intel.com/content/www/us/en/processors/core/7th-gen-core-family-mobile-u-y-processor-lines-i-o-spec-update.html
++ * [7] https://www.intel.com/content/www/us/en/processors/core/7th-gen-core-family-mobile-u-y-processor-lines-i-o-datasheet-vol-1.html
+  */
+ static bool pci_quirk_intel_spt_pch_acs_match(struct pci_dev *dev)
+ {
+@@ -4222,6 +4264,8 @@ static bool pci_quirk_intel_spt_pch_acs_match(struct pci_dev *dev)
+ 	switch (dev->device) {
+ 	case 0xa110 ... 0xa11f: case 0xa167 ... 0xa16a: /* Sunrise Point */
+ 	case 0xa290 ... 0xa29f: case 0xa2e7 ... 0xa2ee: /* Union Point */
++	case 0x9d10 ... 0x9d1b: /* 7th & 8th Gen Mobile */
++	case 0xa32c ... 0xa343:				/* 300 series */
+ 		return true;
+ 	}
+ 
+diff --git a/drivers/soc/fsl/layerscape/ftm_alarm.c b/drivers/soc/fsl/layerscape/ftm_alarm.c
+index 49865b0b..d0e2c418 100644
+--- a/drivers/soc/fsl/layerscape/ftm_alarm.c
++++ b/drivers/soc/fsl/layerscape/ftm_alarm.c
+@@ -41,6 +41,7 @@
+ 
+ static void __iomem *ftm1_base;
+ static void __iomem *rcpm_ftm_addr;
++static void __iomem *scfg_scrachpad_addr;
+ static u32 alarm_freq;
+ static bool big_endian;
+ 
+@@ -61,7 +62,7 @@ static struct rcpm_cfg ls1012a_rcpm_cfg = {
+ 
+ static struct rcpm_cfg ls1021a_rcpm_cfg = {
+ 	.big_endian = BIG_ENDIAN,
+-	.flextimer_set_bit = 0x20000,
++	.flextimer_set_bit = 0x30000000,
+ };
+ 
+ static struct rcpm_cfg ls1043a_rcpm_cfg = {
+@@ -308,6 +309,12 @@ static int ftm_alarm_probe(struct platform_device *pdev)
+ 			iowrite32be(ippdexpcr, rcpm_ftm_addr);
+ 		else
+ 			iowrite32(ippdexpcr, rcpm_ftm_addr);
++
++		r = platform_get_resource_byname(pdev, IORESOURCE_MEM, "scrachpad");
++		if (r) {
++			scfg_scrachpad_addr = devm_ioremap_resource(&pdev->dev, r);
++			iowrite32(ippdexpcr, scfg_scrachpad_addr);
++		}
+ 	}
+ 
+ 	irq = irq_of_parse_and_map(np, 0);
+diff --git a/drivers/staging/fsl-dpaa2/Kconfig b/drivers/staging/fsl-dpaa2/Kconfig
+index 8d56ec98..cab0d032 100644
+--- a/drivers/staging/fsl-dpaa2/Kconfig
++++ b/drivers/staging/fsl-dpaa2/Kconfig
+@@ -29,9 +29,9 @@ config FSL_DPAA2_ETH_USE_ERR_QUEUE
+ 
+ # QBMAN_DEBUG requires some additional DPIO APIs
+ config FSL_DPAA2_ETH_DEBUGFS
+-	depends on DEBUG_FS && FSL_QBMAN_DEBUG
++	depends on DEBUG_FS
+ 	bool "Enable debugfs support"
+-	default n
++	default y
+ 	---help---
+ 	  Enable advanced statistics through debugfs interface.
+ 
+@@ -44,6 +44,13 @@ config FSL_DPAA2_ETH_DCB
+           (PFC) in the driver.
+ 
+           If unsure, say N.
++
++config FSL_DPAA2_ETH_CEETM
++	depends on NET_SCHED
++	bool "DPAA2 Ethernet CEETM QoS"
++	default n
++	---help---
++	  Enable QoS offloading support through the CEETM hardware block.
+ endif
+ 
+ source "drivers/staging/fsl-dpaa2/mac/Kconfig"
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/Makefile b/drivers/staging/fsl-dpaa2/ethernet/Makefile
+index e26911d5..75ad764a 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/Makefile
++++ b/drivers/staging/fsl-dpaa2/ethernet/Makefile
+@@ -6,6 +6,7 @@ obj-$(CONFIG_FSL_DPAA2_ETH) += fsl-dpaa2-eth.o
+ 
+ fsl-dpaa2-eth-objs    := dpaa2-eth.o dpaa2-ethtool.o dpni.o
+ fsl-dpaa2-eth-${CONFIG_FSL_DPAA2_ETH_DEBUGFS} += dpaa2-eth-debugfs.o
++fsl-dpaa2-eth-${CONFIG_FSL_DPAA2_ETH_CEETM} += dpaa2-eth-ceetm.o
+ 
+ # Needed by the tracing framework
+ CFLAGS_dpaa2-eth.o := -I$(src)
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/TODO b/drivers/staging/fsl-dpaa2/ethernet/TODO
+new file mode 100644
+index 00000000..e400a5e4
+--- /dev/null
++++ b/drivers/staging/fsl-dpaa2/ethernet/TODO
+@@ -0,0 +1,18 @@
++* Add a DPAA2 MAC kernel driver in order to allow PHY management; currently
++  the DPMAC objects and their link to DPNIs are handled by MC internally
++  and all PHYs are seen as fixed-link
++* add more debug support: decide how to expose detailed debug statistics,
++  add ingress error queue support
++* MC firmware uprev; the DPAA2 objects used by the Ethernet driver need to
++  be kept in sync with binary interface changes in MC
++* refine README file
++* cleanup
++
++NOTE: None of the above is must-have before getting the DPAA2 Ethernet driver
++out of staging. The main requirement for that is to have the drivers it
++depends on, fsl-mc bus and DPIO driver, moved to drivers/bus and drivers/soc
++respectively.
++
++ Please send any patches to Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
++ ruxandra.radulescu@nxp.com, devel@driverdev.osuosl.org,
++ linux-kernel@vger.kernel.org
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-ceetm.c b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-ceetm.c
+new file mode 100644
+index 00000000..f8bf0182
+--- /dev/null
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-ceetm.c
+@@ -0,0 +1,1253 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * Copyright 2017 NXP
++ *
++ */
++
++#include <linux/init.h>
++#include <linux/module.h>
++
++#include "dpaa2-eth-ceetm.h"
++#include "dpaa2-eth.h"
++
++#define DPAA2_CEETM_DESCRIPTION "FSL DPAA2 CEETM qdisc"
++/* Conversion formula from userspace passed Bps to expected Mbit */
++#define dpaa2_eth_bps_to_mbit(rate) (rate >> 17)
++
++static const struct nla_policy dpaa2_ceetm_policy[DPAA2_CEETM_TCA_MAX] = {
++	[DPAA2_CEETM_TCA_COPT] = { .len = sizeof(struct dpaa2_ceetm_tc_copt) },
++	[DPAA2_CEETM_TCA_QOPS] = { .len = sizeof(struct dpaa2_ceetm_tc_qopt) },
++};
++
++struct Qdisc_ops dpaa2_ceetm_qdisc_ops;
++
++static inline int dpaa2_eth_set_ch_shaping(struct dpaa2_eth_priv *priv,
++					   struct dpni_tx_shaping_cfg *scfg,
++					   struct dpni_tx_shaping_cfg *ecfg,
++					   int coupled, int ch_id)
++{
++	int err = 0;
++
++	netdev_dbg(priv->net_dev, "%s: ch_id %d rate %d mbps\n", __func__,
++		   ch_id, scfg->rate_limit);
++	err = dpni_set_tx_shaping(priv->mc_io, 0, priv->mc_token, scfg,
++				  ecfg, coupled);
++	if (err)
++		netdev_err(priv->net_dev, "dpni_set_tx_shaping err\n");
++
++	return err;
++}
++
++static inline int dpaa2_eth_reset_ch_shaping(struct dpaa2_eth_priv *priv,
++					     int ch_id)
++{
++	struct dpni_tx_shaping_cfg cfg = { 0 };
++
++	return dpaa2_eth_set_ch_shaping(priv, &cfg, &cfg, 0, ch_id);
++}
++
++static inline int
++dpaa2_eth_update_shaping_cfg(struct net_device *dev,
++			     struct dpaa2_ceetm_shaping_cfg cfg,
++			     struct dpni_tx_shaping_cfg *scfg,
++			     struct dpni_tx_shaping_cfg *ecfg)
++{
++	scfg->rate_limit = dpaa2_eth_bps_to_mbit(cfg.cir);
++	ecfg->rate_limit = dpaa2_eth_bps_to_mbit(cfg.eir);
++
++	if (cfg.cbs > DPAA2_ETH_MAX_BURST_SIZE) {
++		netdev_err(dev, "Committed burst size must be under %d\n",
++			   DPAA2_ETH_MAX_BURST_SIZE);
++		return -EINVAL;
++	}
++
++	scfg->max_burst_size = cfg.cbs;
++
++	if (cfg.ebs > DPAA2_ETH_MAX_BURST_SIZE) {
++		netdev_err(dev, "Excess burst size must be under %d\n",
++			   DPAA2_ETH_MAX_BURST_SIZE);
++		return -EINVAL;
++	}
++
++	ecfg->max_burst_size = cfg.ebs;
++
++	if ((!cfg.cir || !cfg.eir) && cfg.coupled) {
++		netdev_err(dev, "Coupling can be set when both CIR and EIR are finite\n");
++		return -EINVAL;
++	}
++
++	return 0;
++}
++
++enum update_tx_prio {
++	DPAA2_ETH_ADD_CQ,
++	DPAA2_ETH_DEL_CQ,
++};
++
++/* Normalize weights based on max passed value */
++static inline int dpaa2_eth_normalize_tx_prio(struct dpaa2_ceetm_qdisc *priv)
++{
++	struct dpni_tx_schedule_cfg *sched_cfg;
++	struct dpaa2_ceetm_class *cl;
++	u32 qpri;
++	u16 weight_max = 0, increment;
++	int i;
++
++	/* Check the boundaries of the provided values */
++	for (i = 0; i < priv->clhash.hashsize; i++)
++		hlist_for_each_entry(cl, &priv->clhash.hash[i], common.hnode)
++			weight_max = (weight_max == 0 ? cl->prio.weight :
++				     (weight_max < cl->prio.weight ?
++				      cl->prio.weight : weight_max));
++
++	/* If there are no elements, there's nothing to do */
++	if (weight_max == 0)
++		return 0;
++
++	increment = (DPAA2_CEETM_MAX_WEIGHT - DPAA2_CEETM_MIN_WEIGHT) /
++		    weight_max;
++
++	for (i = 0; i < priv->clhash.hashsize; i++) {
++		hlist_for_each_entry(cl, &priv->clhash.hash[i], common.hnode) {
++			if (cl->prio.mode == STRICT_PRIORITY)
++				continue;
++
++			qpri = cl->prio.qpri;
++			sched_cfg = &priv->prio.tx_prio_cfg.tc_sched[qpri];
++
++			sched_cfg->delta_bandwidth =
++				DPAA2_CEETM_MIN_WEIGHT +
++				(cl->prio.weight * increment);
++
++			pr_debug("%s: Normalized CQ qpri %d weight to %d\n",
++				 __func__, qpri, sched_cfg->delta_bandwidth);
++		}
++	}
++
++	return 0;
++}
++
++static inline int dpaa2_eth_update_tx_prio(struct dpaa2_eth_priv *priv,
++					   struct dpaa2_ceetm_class *cl,
++					   enum update_tx_prio type)
++{
++	struct dpaa2_ceetm_qdisc *sch = qdisc_priv(cl->parent);
++	struct dpni_congestion_notification_cfg notif_cfg = {0};
++	struct dpni_tx_schedule_cfg *sched_cfg;
++	struct dpni_taildrop td = {0};
++	u8 ch_id = 0, tc_id = 0;
++	u32 qpri = 0;
++	int err = 0;
++
++	qpri = cl->prio.qpri;
++	tc_id = DPNI_BUILD_CH_TC(ch_id, qpri);
++
++	switch (type) {
++	case DPAA2_ETH_ADD_CQ:
++		/* Disable congestion notifications */
++		notif_cfg.threshold_entry = 0;
++		notif_cfg.threshold_exit = 0;
++		err = dpni_set_congestion_notification(priv->mc_io, 0,
++						       priv->mc_token,
++						       DPNI_QUEUE_TX, tc_id,
++						       &notif_cfg);
++		if (err) {
++			netdev_err(priv->net_dev, "Error disabling congestion notifications %d\n",
++				   err);
++			return err;
++		}
++		/* Enable taildrop */
++		td.enable = 1;
++		td.units = DPNI_CONGESTION_UNIT_FRAMES;
++		td.threshold = DPAA2_CEETM_TD_THRESHOLD;
++		err = dpni_set_taildrop(priv->mc_io, 0, priv->mc_token,
++					DPNI_CP_GROUP, DPNI_QUEUE_TX, tc_id,
++					0, &td);
++		if (err) {
++			netdev_err(priv->net_dev, "Error enabling Tx taildrop %d\n",
++				   err);
++			return err;
++		}
++		break;
++	case DPAA2_ETH_DEL_CQ:
++		/* Disable taildrop */
++		td.enable = 0;
++		err = dpni_set_taildrop(priv->mc_io, 0, priv->mc_token,
++					DPNI_CP_GROUP, DPNI_QUEUE_TX, tc_id,
++					0, &td);
++		if (err) {
++			netdev_err(priv->net_dev, "Error disabling Tx taildrop %d\n",
++				   err);
++			return err;
++		}
++		/* Enable congestion notifications */
++		notif_cfg.units = DPNI_CONGESTION_UNIT_BYTES;
++		notif_cfg.threshold_entry = DPAA2_ETH_TX_CONG_ENTRY_THRESH;
++		notif_cfg.threshold_exit = DPAA2_ETH_TX_CONG_EXIT_THRESH;
++		notif_cfg.message_ctx = (u64)priv;
++		notif_cfg.message_iova = priv->cscn_dma;
++		notif_cfg.notification_mode = DPNI_CONG_OPT_WRITE_MEM_ON_ENTER |
++					      DPNI_CONG_OPT_WRITE_MEM_ON_EXIT |
++					      DPNI_CONG_OPT_COHERENT_WRITE;
++		err = dpni_set_congestion_notification(priv->mc_io, 0,
++						       priv->mc_token,
++						       DPNI_QUEUE_TX, tc_id,
++						       &notif_cfg);
++		if (err) {
++			netdev_err(priv->net_dev, "Error enabling congestion notifications %d\n",
++				   err);
++			return err;
++		}
++		break;
++	}
++
++	/* We can zero out the structure in the tx_prio_conf array */
++	if (type == DPAA2_ETH_DEL_CQ) {
++		sched_cfg = &sch->prio.tx_prio_cfg.tc_sched[qpri];
++		memset(sched_cfg, 0, sizeof(*sched_cfg));
++	}
++
++	/* Normalize priorities */
++	err = dpaa2_eth_normalize_tx_prio(sch);
++
++	/* Debug print goes here */
++	print_hex_dump_debug("tx_prio: ", DUMP_PREFIX_OFFSET, 16, 1,
++			     &sch->prio.tx_prio_cfg,
++			     sizeof(sch->prio.tx_prio_cfg), 0);
++
++	/* Call dpni_set_tx_priorities for the entire prio qdisc */
++	err = dpni_set_tx_priorities(priv->mc_io, 0, priv->mc_token,
++				     &sch->prio.tx_prio_cfg);
++	if (err)
++		netdev_err(priv->net_dev, "dpni_set_tx_priorities err %d\n",
++			   err);
++
++	return err;
++}
++
++static void dpaa2_eth_ceetm_enable(struct dpaa2_eth_priv *priv)
++{
++	priv->ceetm_en = true;
++}
++
++static void dpaa2_eth_ceetm_disable(struct dpaa2_eth_priv *priv)
++{
++	priv->ceetm_en = false;
++}
++
++/* Find class in qdisc hash table using given handle */
++static inline struct dpaa2_ceetm_class *dpaa2_ceetm_find(u32 handle,
++							 struct Qdisc *sch)
++{
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct Qdisc_class_common *clc;
++
++	pr_debug(KBUILD_BASENAME " : %s : find class %X in qdisc %X\n",
++		 __func__, handle, sch->handle);
++
++	clc = qdisc_class_find(&priv->clhash, handle);
++	return clc ? container_of(clc, struct dpaa2_ceetm_class, common) : NULL;
++}
++
++/* Insert a class in the qdisc's class hash */
++static void dpaa2_ceetm_link_class(struct Qdisc *sch,
++				   struct Qdisc_class_hash *clhash,
++				   struct Qdisc_class_common *common)
++{
++	sch_tree_lock(sch);
++	qdisc_class_hash_insert(clhash, common);
++	sch_tree_unlock(sch);
++	qdisc_class_hash_grow(sch, clhash);
++}
++
++/* Destroy a ceetm class */
++static void dpaa2_ceetm_cls_destroy(struct Qdisc *sch,
++				    struct dpaa2_ceetm_class *cl)
++{
++	struct net_device *dev = qdisc_dev(sch);
++	struct dpaa2_eth_priv *priv = netdev_priv(dev);
++
++	if (!cl)
++		return;
++
++	pr_debug(KBUILD_BASENAME " : %s : destroy class %X from under %X\n",
++		 __func__, cl->common.classid, sch->handle);
++
++	/* Recurse into child first */
++	if (cl->child) {
++		qdisc_destroy(cl->child);
++		cl->child = NULL;
++	}
++
++	switch (cl->type) {
++	case CEETM_ROOT:
++		if (dpaa2_eth_reset_ch_shaping(priv, cl->root.ch_id))
++			netdev_err(dev, "Error resetting channel shaping\n");
++
++		break;
++
++	case CEETM_PRIO:
++		if (dpaa2_eth_update_tx_prio(priv, cl, DPAA2_ETH_DEL_CQ))
++			netdev_err(dev, "Error resetting tx_priorities\n");
++
++		if (cl->prio.cstats)
++			free_percpu(cl->prio.cstats);
++
++		break;
++	}
++
++	tcf_destroy_chain(&cl->filter_list);
++	kfree(cl);
++}
++
++/* Destroy a ceetm qdisc */
++static void dpaa2_ceetm_destroy(struct Qdisc *sch)
++{
++	unsigned int i;
++	struct hlist_node *next;
++	struct dpaa2_ceetm_class *cl;
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct net_device *dev = qdisc_dev(sch);
++	struct dpaa2_eth_priv *priv_eth = netdev_priv(dev);
++
++	pr_debug(KBUILD_BASENAME " : %s : destroy qdisc %X\n",
++		 __func__, sch->handle);
++
++	/* All filters need to be removed before destroying the classes */
++	tcf_destroy_chain(&priv->filter_list);
++
++	for (i = 0; i < priv->clhash.hashsize; i++) {
++		hlist_for_each_entry(cl, &priv->clhash.hash[i], common.hnode)
++			tcf_destroy_chain(&cl->filter_list);
++	}
++
++	for (i = 0; i < priv->clhash.hashsize; i++) {
++		hlist_for_each_entry_safe(cl, next, &priv->clhash.hash[i],
++					  common.hnode)
++			dpaa2_ceetm_cls_destroy(sch, cl);
++	}
++
++	qdisc_class_hash_destroy(&priv->clhash);
++
++	switch (priv->type) {
++	case CEETM_ROOT:
++		dpaa2_eth_ceetm_disable(priv_eth);
++
++		if (priv->root.qstats)
++			free_percpu(priv->root.qstats);
++
++		if (!priv->root.qdiscs)
++			break;
++
++		/* Destroy the pfifo qdiscs in case they haven't been attached
++		 * to the netdev queues yet.
++		 */
++		for (i = 0; i < dev->num_tx_queues; i++)
++			if (priv->root.qdiscs[i])
++				qdisc_destroy(priv->root.qdiscs[i]);
++
++		kfree(priv->root.qdiscs);
++		break;
++
++	case CEETM_PRIO:
++		if (priv->prio.parent)
++			priv->prio.parent->child = NULL;
++		break;
++	}
++}
++
++static int dpaa2_ceetm_dump(struct Qdisc *sch, struct sk_buff *skb)
++{
++	struct Qdisc *qdisc;
++	unsigned int ntx, i;
++	struct nlattr *nest;
++	struct dpaa2_ceetm_tc_qopt qopt;
++	struct dpaa2_ceetm_qdisc_stats *qstats;
++	struct net_device *dev = qdisc_dev(sch);
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++
++	pr_debug(KBUILD_BASENAME " : %s : qdisc %X\n", __func__, sch->handle);
++
++	sch_tree_lock(sch);
++	memset(&qopt, 0, sizeof(qopt));
++	qopt.type = priv->type;
++	qopt.shaped = priv->shaped;
++
++	switch (priv->type) {
++	case CEETM_ROOT:
++		/* Gather statistics from the underlying pfifo qdiscs */
++		sch->q.qlen = 0;
++		memset(&sch->bstats, 0, sizeof(sch->bstats));
++		memset(&sch->qstats, 0, sizeof(sch->qstats));
++
++		for (ntx = 0; ntx < dev->num_tx_queues; ntx++) {
++			qdisc = netdev_get_tx_queue(dev, ntx)->qdisc_sleeping;
++			sch->q.qlen		+= qdisc->q.qlen;
++			sch->bstats.bytes	+= qdisc->bstats.bytes;
++			sch->bstats.packets	+= qdisc->bstats.packets;
++			sch->qstats.qlen	+= qdisc->qstats.qlen;
++			sch->qstats.backlog	+= qdisc->qstats.backlog;
++			sch->qstats.drops	+= qdisc->qstats.drops;
++			sch->qstats.requeues	+= qdisc->qstats.requeues;
++			sch->qstats.overlimits	+= qdisc->qstats.overlimits;
++		}
++
++		for_each_online_cpu(i) {
++			qstats = per_cpu_ptr(priv->root.qstats, i);
++			sch->qstats.drops += qstats->drops;
++		}
++
++		break;
++
++	case CEETM_PRIO:
++		qopt.prio_group_A = priv->prio.tx_prio_cfg.prio_group_A;
++		qopt.prio_group_B = priv->prio.tx_prio_cfg.prio_group_B;
++		qopt.separate_groups = priv->prio.tx_prio_cfg.separate_groups;
++		break;
++
++	default:
++		pr_err(KBUILD_BASENAME " : %s : invalid qdisc\n", __func__);
++		sch_tree_unlock(sch);
++		return -EINVAL;
++	}
++
++	nest = nla_nest_start(skb, TCA_OPTIONS);
++	if (!nest)
++		goto nla_put_failure;
++	if (nla_put(skb, DPAA2_CEETM_TCA_QOPS, sizeof(qopt), &qopt))
++		goto nla_put_failure;
++	nla_nest_end(skb, nest);
++
++	sch_tree_unlock(sch);
++	return skb->len;
++
++nla_put_failure:
++	sch_tree_unlock(sch);
++	nla_nest_cancel(skb, nest);
++	return -EMSGSIZE;
++}
++
++static int dpaa2_ceetm_change_prio(struct Qdisc *sch,
++				   struct dpaa2_ceetm_qdisc *priv,
++				   struct dpaa2_ceetm_tc_qopt *qopt)
++{
++	/* TODO: Once LX2 support is added */
++	/* priv->shaped = parent_cl->shaped; */
++	priv->prio.tx_prio_cfg.prio_group_A = qopt->prio_group_A;
++	priv->prio.tx_prio_cfg.prio_group_B = qopt->prio_group_B;
++	priv->prio.tx_prio_cfg.separate_groups = qopt->separate_groups;
++
++	return 0;
++}
++
++/* Edit a ceetm qdisc */
++static int dpaa2_ceetm_change(struct Qdisc *sch, struct nlattr *opt)
++{
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct nlattr *tb[DPAA2_CEETM_TCA_QOPS + 1];
++	struct dpaa2_ceetm_tc_qopt *qopt;
++	int err;
++
++	pr_debug(KBUILD_BASENAME " : %s : qdisc %X\n", __func__, sch->handle);
++
++	err = nla_parse_nested(tb, DPAA2_CEETM_TCA_QOPS, opt,
++			       dpaa2_ceetm_policy);
++	if (err < 0) {
++		pr_err(KBUILD_BASENAME " : %s : tc error in %s\n", __func__,
++		       "nla_parse_nested");
++		return err;
++	}
++
++	if (!tb[DPAA2_CEETM_TCA_QOPS]) {
++		pr_err(KBUILD_BASENAME " : %s : tc error in %s\n", __func__,
++		       "tb");
++		return -EINVAL;
++	}
++
++	if (TC_H_MIN(sch->handle)) {
++		pr_err("CEETM: a qdisc should not have a minor\n");
++		return -EINVAL;
++	}
++
++	qopt = nla_data(tb[DPAA2_CEETM_TCA_QOPS]);
++
++	if (priv->type != qopt->type) {
++		pr_err("CEETM: qdisc %X is not of the provided type\n",
++		       sch->handle);
++		return -EINVAL;
++	}
++
++	switch (priv->type) {
++	case CEETM_PRIO:
++		err = dpaa2_ceetm_change_prio(sch, priv, qopt);
++		break;
++	default:
++		pr_err(KBUILD_BASENAME " : %s : invalid qdisc\n", __func__);
++		err = -EINVAL;
++	}
++
++	return err;
++}
++
++/* Configure a root ceetm qdisc */
++static int dpaa2_ceetm_init_root(struct Qdisc *sch,
++				 struct dpaa2_ceetm_qdisc *priv,
++				 struct dpaa2_ceetm_tc_qopt *qopt)
++{
++	struct net_device *dev = qdisc_dev(sch);
++	struct dpaa2_eth_priv *priv_eth = netdev_priv(dev);
++	struct netdev_queue *dev_queue;
++	unsigned int i, parent_id;
++	struct Qdisc *qdisc;
++	int err;
++
++	pr_debug(KBUILD_BASENAME " : %s : qdisc %X\n", __func__, sch->handle);
++
++	/* Validate inputs */
++	if (sch->parent != TC_H_ROOT) {
++		pr_err("CEETM: a root ceetm qdisc can not be attached to a class\n");
++		tcf_destroy_chain(&priv->filter_list);
++		qdisc_class_hash_destroy(&priv->clhash);
++		return -EINVAL;
++	}
++
++	/* Pre-allocate underlying pfifo qdiscs.
++	 *
++	 * We want to offload shaping and scheduling decisions to the hardware.
++	 * The pfifo qdiscs will be attached to the netdev queues and will
++	 * guide the traffic from the IP stack down to the driver with minimum
++	 * interference.
++	 *
++	 * The CEETM qdiscs and classes will be crossed when the traffic
++	 * reaches the driver.
++	 */
++	priv->root.qdiscs = kcalloc(dev->num_tx_queues,
++				    sizeof(priv->root.qdiscs[0]),
++				    GFP_KERNEL);
++	if (!priv->root.qdiscs) {
++		err = -ENOMEM;
++		goto err_init_root;
++	}
++
++	for (i = 0; i < dev->num_tx_queues; i++) {
++		dev_queue = netdev_get_tx_queue(dev, i);
++		parent_id = TC_H_MAKE(TC_H_MAJ(sch->handle),
++				      TC_H_MIN(i + PFIFO_MIN_OFFSET));
++
++		qdisc = qdisc_create_dflt(dev_queue, &pfifo_qdisc_ops,
++					  parent_id);
++		if (!qdisc) {
++			err = -ENOMEM;
++			goto err_init_root;
++		}
++
++		priv->root.qdiscs[i] = qdisc;
++		qdisc->flags |= TCQ_F_ONETXQUEUE;
++	}
++
++	sch->flags |= TCQ_F_MQROOT;
++
++	priv->root.qstats = alloc_percpu(struct dpaa2_ceetm_qdisc_stats);
++	if (!priv->root.qstats) {
++		pr_err(KBUILD_BASENAME " : %s : alloc_percpu() failed\n",
++		       __func__);
++		err = -ENOMEM;
++		goto err_init_root;
++	}
++
++	dpaa2_eth_ceetm_enable(priv_eth);
++	return 0;
++
++err_init_root:
++	dpaa2_ceetm_destroy(sch);
++	return err;
++}
++
++/* Configure a prio ceetm qdisc */
++static int dpaa2_ceetm_init_prio(struct Qdisc *sch,
++				 struct dpaa2_ceetm_qdisc *priv,
++				 struct dpaa2_ceetm_tc_qopt *qopt)
++{
++	struct net_device *dev = qdisc_dev(sch);
++	struct dpaa2_ceetm_class *parent_cl;
++	struct Qdisc *parent_qdisc;
++	int err;
++
++	pr_debug(KBUILD_BASENAME " : %s : qdisc %X\n", __func__, sch->handle);
++
++	if (sch->parent == TC_H_ROOT) {
++		pr_err("CEETM: a prio ceetm qdisc can not be root\n");
++		err = -EINVAL;
++		goto err_init_prio;
++	}
++
++	parent_qdisc = qdisc_lookup(dev, TC_H_MAJ(sch->parent));
++	if (strcmp(parent_qdisc->ops->id, dpaa2_ceetm_qdisc_ops.id)) {
++		pr_err("CEETM: a ceetm qdisc can not be attached to other qdisc/class types\n");
++		err = -EINVAL;
++		goto err_init_prio;
++	}
++
++	/* Obtain the parent root ceetm_class */
++	parent_cl = dpaa2_ceetm_find(sch->parent, parent_qdisc);
++
++	if (!parent_cl || parent_cl->type != CEETM_ROOT) {
++		pr_err("CEETM: a prio ceetm qdiscs can be added only under a root ceetm class\n");
++		err = -EINVAL;
++		goto err_init_prio;
++	}
++
++	priv->prio.parent = parent_cl;
++	parent_cl->child = sch;
++
++	err = dpaa2_ceetm_change_prio(sch, priv, qopt);
++
++	return 0;
++
++err_init_prio:
++	dpaa2_ceetm_destroy(sch);
++	return err;
++}
++
++/* Configure a generic ceetm qdisc */
++static int dpaa2_ceetm_init(struct Qdisc *sch, struct nlattr *opt)
++{
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct net_device *dev = qdisc_dev(sch);
++	struct nlattr *tb[DPAA2_CEETM_TCA_QOPS + 1];
++	struct dpaa2_ceetm_tc_qopt *qopt;
++	int err;
++
++	pr_debug(KBUILD_BASENAME " : %s : qdisc %X\n", __func__, sch->handle);
++
++	if (!netif_is_multiqueue(dev))
++		return -EOPNOTSUPP;
++
++	RCU_INIT_POINTER(priv->filter_list, NULL);
++
++	if (!opt) {
++		pr_err(KBUILD_BASENAME " : %s : tc error - opt = NULL\n",
++		       __func__);
++		return -EINVAL;
++	}
++
++	err = nla_parse_nested(tb, DPAA2_CEETM_TCA_QOPS, opt,
++			       dpaa2_ceetm_policy);
++	if (err < 0) {
++		pr_err(KBUILD_BASENAME " : %s : tc error in %s\n", __func__,
++		       "nla_parse_nested");
++		return err;
++	}
++
++	if (!tb[DPAA2_CEETM_TCA_QOPS]) {
++		pr_err(KBUILD_BASENAME " : %s : tc error in %s\n", __func__,
++		       "tb");
++		return -EINVAL;
++	}
++
++	if (TC_H_MIN(sch->handle)) {
++		pr_err("CEETM: a qdisc should not have a minor\n");
++		return -EINVAL;
++	}
++
++	qopt = nla_data(tb[DPAA2_CEETM_TCA_QOPS]);
++
++	/* Initialize the class hash list. Each qdisc has its own class hash */
++	err = qdisc_class_hash_init(&priv->clhash);
++	if (err < 0) {
++		pr_err(KBUILD_BASENAME " : %s : qdisc_class_hash_init failed\n",
++		       __func__);
++		return err;
++	}
++
++	priv->type = qopt->type;
++	priv->shaped = qopt->shaped;
++
++	switch (priv->type) {
++	case CEETM_ROOT:
++		err = dpaa2_ceetm_init_root(sch, priv, qopt);
++		break;
++	case CEETM_PRIO:
++		err = dpaa2_ceetm_init_prio(sch, priv, qopt);
++		break;
++	default:
++		pr_err(KBUILD_BASENAME " : %s : invalid qdisc\n", __func__);
++		dpaa2_ceetm_destroy(sch);
++		err = -EINVAL;
++	}
++
++	return err;
++}
++
++/* Attach the underlying pfifo qdiscs */
++static void dpaa2_ceetm_attach(struct Qdisc *sch)
++{
++	struct net_device *dev = qdisc_dev(sch);
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct Qdisc *qdisc, *old_qdisc;
++	unsigned int i;
++
++	pr_debug(KBUILD_BASENAME " : %s : qdisc %X\n", __func__, sch->handle);
++
++	for (i = 0; i < dev->num_tx_queues; i++) {
++		qdisc = priv->root.qdiscs[i];
++		old_qdisc = dev_graft_qdisc(qdisc->dev_queue, qdisc);
++		if (old_qdisc)
++			qdisc_destroy(old_qdisc);
++	}
++
++	/* Remove the references to the pfifo qdiscs since the kernel will
++	 * destroy them when needed. No cleanup from our part is required from
++	 * this point on.
++	 */
++	kfree(priv->root.qdiscs);
++	priv->root.qdiscs = NULL;
++}
++
++static unsigned long dpaa2_ceetm_cls_get(struct Qdisc *sch, u32 classid)
++{
++	struct dpaa2_ceetm_class *cl;
++
++	pr_debug(KBUILD_BASENAME " : %s : classid %X from qdisc %X\n",
++		 __func__, classid, sch->handle);
++	cl = dpaa2_ceetm_find(classid, sch);
++
++	if (cl)
++		cl->refcnt++;
++
++	return (unsigned long)cl;
++}
++
++static void dpaa2_ceetm_cls_put(struct Qdisc *sch, unsigned long arg)
++{
++	struct dpaa2_ceetm_class *cl = (struct dpaa2_ceetm_class *)arg;
++	pr_debug(KBUILD_BASENAME " : %s : classid %X from qdisc %X\n",
++		 __func__, cl->common.classid, sch->handle);
++	cl->refcnt--;
++
++	if (cl->refcnt == 0)
++		dpaa2_ceetm_cls_destroy(sch, cl);
++}
++
++static int dpaa2_ceetm_cls_change_root(struct dpaa2_ceetm_class *cl,
++				       struct dpaa2_ceetm_tc_copt *copt,
++				       struct net_device *dev)
++{
++	struct dpaa2_eth_priv *priv = netdev_priv(dev);
++	struct dpni_tx_shaping_cfg scfg = { 0 }, ecfg = { 0 };
++	int err = 0;
++
++	pr_debug(KBUILD_BASENAME " : %s : class %X\n", __func__,
++		 cl->common.classid);
++
++	if (!cl->shaped)
++		return 0;
++
++	if (dpaa2_eth_update_shaping_cfg(dev, copt->shaping_cfg,
++					 &scfg, &ecfg))
++		return -EINVAL;
++
++	err = dpaa2_eth_set_ch_shaping(priv, &scfg, &ecfg,
++				       copt->shaping_cfg.coupled,
++				       cl->root.ch_id);
++	if (err)
++		return err;
++
++	memcpy(&cl->root.shaping_cfg, &copt->shaping_cfg,
++	       sizeof(struct dpaa2_ceetm_shaping_cfg));
++
++	return err;
++}
++
++static int dpaa2_ceetm_cls_change_prio(struct dpaa2_ceetm_class *cl,
++				       struct dpaa2_ceetm_tc_copt *copt,
++				       struct net_device *dev)
++{
++	struct dpaa2_ceetm_qdisc *sch = qdisc_priv(cl->parent);
++	struct dpni_tx_schedule_cfg *sched_cfg;
++	struct dpaa2_eth_priv *priv = netdev_priv(dev);
++	int err;
++
++	pr_debug(KBUILD_BASENAME " : %s : class %X mode %d weight %d\n",
++		 __func__, cl->common.classid, copt->mode, copt->weight);
++
++	if (!cl->prio.cstats) {
++		cl->prio.cstats = alloc_percpu(struct dpaa2_ceetm_class_stats);
++		if (!cl->prio.cstats) {
++			pr_err(KBUILD_BASENAME " : %s : alloc_percpu() failed\n",
++			       __func__);
++			return -ENOMEM;
++		}
++	}
++
++	cl->prio.mode = copt->mode;
++	cl->prio.weight = copt->weight;
++
++	sched_cfg = &sch->prio.tx_prio_cfg.tc_sched[cl->prio.qpri];
++
++	switch (copt->mode) {
++	case STRICT_PRIORITY:
++		sched_cfg->mode = DPNI_TX_SCHED_STRICT_PRIORITY;
++		break;
++	case WEIGHTED_A:
++		sched_cfg->mode = DPNI_TX_SCHED_WEIGHTED_A;
++		break;
++	case WEIGHTED_B:
++		sched_cfg->mode = DPNI_TX_SCHED_WEIGHTED_B;
++		break;
++	}
++
++	err = dpaa2_eth_update_tx_prio(priv, cl, DPAA2_ETH_ADD_CQ);
++
++	return err;
++}
++
++/* Add a new ceetm class */
++static int dpaa2_ceetm_cls_add(struct Qdisc *sch, u32 classid,
++			       struct dpaa2_ceetm_tc_copt *copt,
++			       unsigned long *arg)
++{
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct net_device *dev = qdisc_dev(sch);
++	struct dpaa2_eth_priv *priv_eth = netdev_priv(dev);
++	struct dpaa2_ceetm_class *cl;
++	int err;
++
++	if (copt->type == CEETM_ROOT &&
++	    priv->clhash.hashelems == dpaa2_eth_ch_count(priv_eth)) {
++		pr_err("CEETM: only %d channel%s per DPNI allowed, sorry\n",
++		       dpaa2_eth_ch_count(priv_eth),
++		       dpaa2_eth_ch_count(priv_eth) == 1 ? "" : "s");
++		return -EINVAL;
++	}
++
++	if (copt->type == CEETM_PRIO &&
++	    priv->clhash.hashelems == dpaa2_eth_tc_count(priv_eth)) {
++		pr_err("CEETM: only %d queue%s per channel allowed, sorry\n",
++		       dpaa2_eth_tc_count(priv_eth),
++		       dpaa2_eth_tc_count(priv_eth) == 1 ? "" : "s");
++		return -EINVAL;
++	}
++
++	cl = kzalloc(sizeof(*cl), GFP_KERNEL);
++	if (!cl)
++		return -ENOMEM;
++
++	RCU_INIT_POINTER(cl->filter_list, NULL);
++
++	cl->common.classid = classid;
++	cl->refcnt = 1;
++	cl->parent = sch;
++	cl->child = NULL;
++
++	/* Add class handle in Qdisc */
++	dpaa2_ceetm_link_class(sch, &priv->clhash, &cl->common);
++
++	cl->shaped = copt->shaped;
++	cl->type = copt->type;
++
++	/* Claim a CEETM channel / tc - DPAA2. will assume transition from
++	 * classid to qdid/qpri, starting from qdid / qpri 0
++	 */
++	switch (copt->type) {
++	case CEETM_ROOT:
++		cl->root.ch_id = classid - sch->handle - 1;
++		err = dpaa2_ceetm_cls_change_root(cl, copt, dev);
++		break;
++	case CEETM_PRIO:
++		cl->prio.qpri = classid - sch->handle - 1;
++		err = dpaa2_ceetm_cls_change_prio(cl, copt, dev);
++		break;
++	default:
++		err = -EINVAL;
++		break;
++	}
++
++	if (err) {
++		pr_err("%s: Unable to set new %s class\n", __func__,
++		       (copt->type == CEETM_ROOT ? "root" : "prio"));
++		goto out_free;
++	}
++
++	switch (copt->type) {
++	case CEETM_ROOT:
++		pr_debug(KBUILD_BASENAME " : %s : configured root class %X associated with channel qdid %d\n",
++			 __func__, classid, cl->root.ch_id);
++		break;
++	case CEETM_PRIO:
++		pr_debug(KBUILD_BASENAME " : %s : configured prio class %X associated with queue qpri %d\n",
++			 __func__, classid, cl->prio.qpri);
++		break;
++	}
++
++	*arg = (unsigned long)cl;
++	return 0;
++
++out_free:
++	kfree(cl);
++	return err;
++}
++
++/* Add or configure a ceetm class */
++static int dpaa2_ceetm_cls_change(struct Qdisc *sch, u32 classid, u32 parentid,
++				  struct nlattr **tca, unsigned long *arg)
++{
++	struct dpaa2_ceetm_qdisc *priv;
++	struct dpaa2_ceetm_class *cl = (struct dpaa2_ceetm_class *)*arg;
++	struct nlattr *opt = tca[TCA_OPTIONS];
++	struct nlattr *tb[DPAA2_CEETM_TCA_MAX];
++	struct dpaa2_ceetm_tc_copt *copt;
++	struct net_device *dev = qdisc_dev(sch);
++	int err;
++
++	pr_debug(KBUILD_BASENAME " : %s : classid %X under qdisc %X\n",
++		 __func__, classid, sch->handle);
++
++	if (strcmp(sch->ops->id, dpaa2_ceetm_qdisc_ops.id)) {
++		pr_err("CEETM: a ceetm class can not be attached to other qdisc/class types\n");
++		return -EINVAL;
++	}
++
++	priv = qdisc_priv(sch);
++
++	if (!opt) {
++		pr_err(KBUILD_BASENAME " : %s : tc error NULL opt\n", __func__);
++		return -EINVAL;
++	}
++
++	err = nla_parse_nested(tb, DPAA2_CEETM_TCA_COPT, opt,
++			       dpaa2_ceetm_policy);
++	if (err < 0) {
++		pr_err(KBUILD_BASENAME " : %s : tc error in %s\n", __func__,
++		       "nla_parse_nested");
++		return -EINVAL;
++	}
++
++	if (!tb[DPAA2_CEETM_TCA_COPT]) {
++		pr_err(KBUILD_BASENAME " : %s : tc error in %s\n", __func__,
++		       "tb");
++		return -EINVAL;
++	}
++
++	copt = nla_data(tb[DPAA2_CEETM_TCA_COPT]);
++
++	/* Configure an existing ceetm class */
++	if (cl) {
++		if (copt->type != cl->type) {
++			pr_err("CEETM: class %X is not of the provided type\n",
++			       cl->common.classid);
++			return -EINVAL;
++		}
++
++		switch (copt->type) {
++		case CEETM_ROOT:
++			return dpaa2_ceetm_cls_change_root(cl, copt, dev);
++		case CEETM_PRIO:
++			return dpaa2_ceetm_cls_change_prio(cl, copt, dev);
++
++		default:
++			pr_err(KBUILD_BASENAME " : %s : invalid class\n",
++			       __func__);
++			return -EINVAL;
++		}
++	}
++
++	return dpaa2_ceetm_cls_add(sch, classid, copt, arg);
++}
++
++static void dpaa2_ceetm_cls_walk(struct Qdisc *sch, struct qdisc_walker *arg)
++{
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct dpaa2_ceetm_class *cl;
++	unsigned int i;
++
++	pr_debug(KBUILD_BASENAME " : %s : qdisc %X\n", __func__, sch->handle);
++
++	if (arg->stop)
++		return;
++
++	for (i = 0; i < priv->clhash.hashsize; i++) {
++		hlist_for_each_entry(cl, &priv->clhash.hash[i], common.hnode) {
++			if (arg->count < arg->skip) {
++				arg->count++;
++				continue;
++			}
++			if (arg->fn(sch, (unsigned long)cl, arg) < 0) {
++				arg->stop = 1;
++				return;
++			}
++			arg->count++;
++		}
++	}
++}
++
++static int dpaa2_ceetm_cls_dump(struct Qdisc *sch, unsigned long arg,
++				struct sk_buff *skb, struct tcmsg *tcm)
++{
++	struct dpaa2_ceetm_class *cl = (struct dpaa2_ceetm_class *)arg;
++	struct nlattr *nest;
++	struct dpaa2_ceetm_tc_copt copt;
++
++	pr_debug(KBUILD_BASENAME " : %s : class %X under qdisc %X\n",
++		 __func__, cl->common.classid, sch->handle);
++
++	sch_tree_lock(sch);
++
++	tcm->tcm_parent = ((struct Qdisc *)cl->parent)->handle;
++	tcm->tcm_handle = cl->common.classid;
++
++	memset(&copt, 0, sizeof(copt));
++
++	copt.shaped = cl->shaped;
++	copt.type = cl->type;
++
++	switch (cl->type) {
++	case CEETM_ROOT:
++		if (cl->child)
++			tcm->tcm_info = cl->child->handle;
++
++		memcpy(&copt.shaping_cfg, &cl->root.shaping_cfg,
++		       sizeof(struct dpaa2_ceetm_shaping_cfg));
++
++		break;
++
++	case CEETM_PRIO:
++		if (cl->child)
++			tcm->tcm_info = cl->child->handle;
++
++		copt.mode = cl->prio.mode;
++		copt.weight = cl->prio.weight;
++
++		break;
++	}
++
++	nest = nla_nest_start(skb, TCA_OPTIONS);
++	if (!nest)
++		goto nla_put_failure;
++	if (nla_put(skb, DPAA2_CEETM_TCA_COPT, sizeof(copt), &copt))
++		goto nla_put_failure;
++	nla_nest_end(skb, nest);
++	sch_tree_unlock(sch);
++	return skb->len;
++
++nla_put_failure:
++	sch_tree_unlock(sch);
++	nla_nest_cancel(skb, nest);
++	return -EMSGSIZE;
++}
++
++static int dpaa2_ceetm_cls_delete(struct Qdisc *sch, unsigned long arg)
++{
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct dpaa2_ceetm_class *cl = (struct dpaa2_ceetm_class *)arg;
++
++	pr_debug(KBUILD_BASENAME " : %s : class %X under qdisc %X\n",
++		 __func__, cl->common.classid, sch->handle);
++
++	sch_tree_lock(sch);
++	qdisc_class_hash_remove(&priv->clhash, &cl->common);
++	cl->refcnt--;
++	WARN_ON(cl->refcnt == 0);
++	sch_tree_unlock(sch);
++	return 0;
++}
++
++/* Get the class' child qdisc, if any */
++static struct Qdisc *dpaa2_ceetm_cls_leaf(struct Qdisc *sch, unsigned long arg)
++{
++	struct dpaa2_ceetm_class *cl = (struct dpaa2_ceetm_class *)arg;
++
++	pr_debug(KBUILD_BASENAME " : %s : class %X under qdisc %X\n",
++		 __func__, cl->common.classid, sch->handle);
++
++	switch (cl->type) {
++	case CEETM_ROOT:
++	case CEETM_PRIO:
++		return cl->child;
++	}
++
++	return NULL;
++}
++
++static int dpaa2_ceetm_cls_graft(struct Qdisc *sch, unsigned long arg,
++				 struct Qdisc *new, struct Qdisc **old)
++{
++	if (new && strcmp(new->ops->id, dpaa2_ceetm_qdisc_ops.id)) {
++		pr_err("CEETM: only ceetm qdiscs can be attached to ceetm classes\n");
++		return -EOPNOTSUPP;
++	}
++
++	return 0;
++}
++
++static int dpaa2_ceetm_cls_dump_stats(struct Qdisc *sch, unsigned long arg,
++				      struct gnet_dump *d)
++{
++	struct dpaa2_ceetm_class *cl = (struct dpaa2_ceetm_class *)arg;
++	struct gnet_stats_basic_packed tmp_bstats;
++	struct dpaa2_ceetm_tc_xstats xstats;
++	union dpni_statistics dpni_stats;
++	struct net_device *dev = qdisc_dev(sch);
++	struct dpaa2_eth_priv *priv_eth = netdev_priv(dev);
++	u8 ch_id = 0;
++	int err;
++
++	memset(&xstats, 0, sizeof(xstats));
++	memset(&tmp_bstats, 0, sizeof(tmp_bstats));
++
++	if (cl->type == CEETM_ROOT)
++		return 0;
++
++	err = dpni_get_statistics(priv_eth->mc_io, 0, priv_eth->mc_token, 3,
++				  DPNI_BUILD_CH_TC(ch_id, cl->prio.qpri),
++				  &dpni_stats);
++	if (err)
++		netdev_warn(dev, "dpni_get_stats(%d) failed - %d\n", 3, err);
++
++	xstats.ceetm_dequeue_bytes = dpni_stats.page_3.ceetm_dequeue_bytes;
++	xstats.ceetm_dequeue_frames = dpni_stats.page_3.ceetm_dequeue_frames;
++	xstats.ceetm_reject_bytes = dpni_stats.page_3.ceetm_reject_bytes;
++	xstats.ceetm_reject_frames = dpni_stats.page_3.ceetm_reject_frames;
++
++	return gnet_stats_copy_app(d, &xstats, sizeof(xstats));
++}
++
++static struct tcf_proto __rcu **dpaa2_ceetm_tcf_chain(struct Qdisc *sch,
++						      unsigned long arg)
++{
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct dpaa2_ceetm_class *cl = (struct dpaa2_ceetm_class *)arg;
++
++	pr_debug(KBUILD_BASENAME " : %s : class %X under qdisc %X\n", __func__,
++		 cl ? cl->common.classid : 0, sch->handle);
++	return cl ? &cl->filter_list : &priv->filter_list;
++}
++
++static unsigned long dpaa2_ceetm_tcf_bind(struct Qdisc *sch,
++					  unsigned long parent,
++					  u32 classid)
++{
++	struct dpaa2_ceetm_class *cl = dpaa2_ceetm_find(classid, sch);
++
++	pr_debug(KBUILD_BASENAME " : %s : class %X under qdisc %X\n", __func__,
++		 cl ? cl->common.classid : 0, sch->handle);
++	return (unsigned long)cl;
++}
++
++static void dpaa2_ceetm_tcf_unbind(struct Qdisc *sch, unsigned long arg)
++{
++	struct dpaa2_ceetm_class *cl = (struct dpaa2_ceetm_class *)arg;
++
++	pr_debug(KBUILD_BASENAME " : %s : class %X under qdisc %X\n", __func__,
++		 cl ? cl->common.classid : 0, sch->handle);
++}
++
++const struct Qdisc_class_ops dpaa2_ceetm_cls_ops = {
++	.graft		=	dpaa2_ceetm_cls_graft,
++	.leaf		=	dpaa2_ceetm_cls_leaf,
++	.get		=	dpaa2_ceetm_cls_get,
++	.put		=	dpaa2_ceetm_cls_put,
++	.change		=	dpaa2_ceetm_cls_change,
++	.delete		=	dpaa2_ceetm_cls_delete,
++	.walk		=	dpaa2_ceetm_cls_walk,
++	.tcf_chain	=	dpaa2_ceetm_tcf_chain,
++	.bind_tcf	=	dpaa2_ceetm_tcf_bind,
++	.unbind_tcf	=	dpaa2_ceetm_tcf_unbind,
++	.dump		=	dpaa2_ceetm_cls_dump,
++	.dump_stats	=	dpaa2_ceetm_cls_dump_stats,
++};
++
++struct Qdisc_ops dpaa2_ceetm_qdisc_ops __read_mostly = {
++	.id		=	"ceetm",
++	.priv_size	=	sizeof(struct dpaa2_ceetm_qdisc),
++	.cl_ops		=	&dpaa2_ceetm_cls_ops,
++	.init		=	dpaa2_ceetm_init,
++	.destroy	=	dpaa2_ceetm_destroy,
++	.change		=	dpaa2_ceetm_change,
++	.dump		=	dpaa2_ceetm_dump,
++	.attach		=	dpaa2_ceetm_attach,
++	.owner		=	THIS_MODULE,
++};
++
++/* Run the filters and classifiers attached to the qdisc on the provided skb */
++int dpaa2_ceetm_classify(struct sk_buff *skb, struct Qdisc *sch,
++			 int *qdid, int *qpri)
++{
++	struct dpaa2_ceetm_qdisc *priv = qdisc_priv(sch);
++	struct dpaa2_ceetm_class *cl = NULL;
++	struct tcf_result res;
++	struct tcf_proto *tcf;
++	int result;
++
++	tcf = rcu_dereference_bh(priv->filter_list);
++	while (tcf && (result = tc_classify(skb, tcf, &res, false)) >= 0) {
++#ifdef CONFIG_NET_CLS_ACT
++		switch (result) {
++		case TC_ACT_QUEUED:
++		case TC_ACT_STOLEN:
++		case TC_ACT_SHOT:
++			/* No valid class found due to action */
++			return -1;
++		}
++#endif
++		cl = (void *)res.class;
++		if (!cl) {
++			/* The filter leads to the qdisc */
++			if (res.classid == sch->handle)
++				return 0;
++
++			cl = dpaa2_ceetm_find(res.classid, sch);
++			/* The filter leads to an invalid class */
++			if (!cl)
++				break;
++		}
++
++		/* The class might have its own filters attached */
++		tcf = rcu_dereference_bh(cl->filter_list);
++	}
++
++	/* No valid class found */
++	if (!cl)
++		return 0;
++
++	switch (cl->type) {
++	case CEETM_ROOT:
++		*qdid = cl->root.ch_id;
++
++		/* The root class does not have a child prio qdisc */
++		if (!cl->child)
++			return 0;
++
++		/* Run the prio qdisc classifiers */
++		return dpaa2_ceetm_classify(skb, cl->child, qdid, qpri);
++
++	case CEETM_PRIO:
++		*qpri = cl->prio.qpri;
++		break;
++	}
++
++	return 0;
++}
++
++int __init dpaa2_ceetm_register(void)
++{
++	int err = 0;
++
++	pr_debug(KBUILD_MODNAME ": " DPAA2_CEETM_DESCRIPTION "\n");
++
++	err = register_qdisc(&dpaa2_ceetm_qdisc_ops);
++	if (unlikely(err))
++		pr_err(KBUILD_MODNAME
++		       ": %s:%hu:%s(): register_qdisc() = %d\n",
++		       KBUILD_BASENAME ".c", __LINE__, __func__, err);
++
++	return err;
++}
++
++void __exit dpaa2_ceetm_unregister(void)
++{
++	pr_debug(KBUILD_MODNAME ": %s:%s() ->\n",
++		 KBUILD_BASENAME ".c", __func__);
++
++	unregister_qdisc(&dpaa2_ceetm_qdisc_ops);
++}
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-ceetm.h b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-ceetm.h
+new file mode 100644
+index 00000000..48003b63
+--- /dev/null
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-ceetm.h
+@@ -0,0 +1,182 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
++/*
++ * Copyright 2017 NXP
++ *
++ */
++
++#ifndef __DPAA2_ETH_CEETM_H
++#define __DPAA2_ETH_CEETM_H
++
++#include <net/pkt_sched.h>
++#include <net/pkt_cls.h>
++#include <net/netlink.h>
++
++#include "dpaa2-eth.h"
++
++/* For functional purposes, there are num_tx_queues pfifo qdiscs through which
++ * frames reach the driver. Their handles start from 1:21. Handles 1:1 to 1:20
++ * are reserved for the maximum 32 CEETM channels (majors and minors are in
++ * hex).
++ */
++#define PFIFO_MIN_OFFSET		0x21
++
++#define DPAA2_CEETM_MIN_WEIGHT		100
++#define DPAA2_CEETM_MAX_WEIGHT		24800
++
++#define DPAA2_CEETM_TD_THRESHOLD	1000
++
++enum wbfs_group_type {
++	WBFS_GRP_A,
++	WBFS_GRP_B,
++	WBFS_GRP_LARGE
++};
++
++enum {
++	DPAA2_CEETM_TCA_UNSPEC,
++	DPAA2_CEETM_TCA_COPT,
++	DPAA2_CEETM_TCA_QOPS,
++	DPAA2_CEETM_TCA_MAX,
++};
++
++/* CEETM configuration types */
++enum dpaa2_ceetm_type {
++	CEETM_ROOT = 1,
++	CEETM_PRIO,
++};
++
++enum {
++	STRICT_PRIORITY = 0,
++	WEIGHTED_A,
++	WEIGHTED_B,
++};
++
++struct dpaa2_ceetm_shaping_cfg {
++	__u64 cir; /* committed information rate */
++	__u64 eir; /* excess information rate */
++	__u16 cbs; /* committed burst size */
++	__u16 ebs; /* excess burst size */
++	__u8 coupled; /* shaper coupling */
++};
++
++extern const struct nla_policy ceetm_policy[DPAA2_CEETM_TCA_MAX];
++
++struct dpaa2_ceetm_class;
++struct dpaa2_ceetm_qdisc_stats;
++struct dpaa2_ceetm_class_stats;
++
++/* corresponds to CEETM shaping at LNI level */
++struct dpaa2_root_q {
++	struct Qdisc **qdiscs;
++	struct dpaa2_ceetm_qdisc_stats __percpu *qstats;
++};
++
++/* corresponds to the number of priorities a channel serves */
++struct dpaa2_prio_q {
++	struct dpaa2_ceetm_class *parent;
++	struct dpni_tx_priorities_cfg tx_prio_cfg;
++};
++
++struct dpaa2_ceetm_qdisc {
++	struct Qdisc_class_hash clhash;
++	struct tcf_proto *filter_list; /* qdisc attached filters */
++
++	enum dpaa2_ceetm_type type; /* ROOT/PRIO */
++	bool shaped;
++	union {
++		struct dpaa2_root_q root;
++		struct dpaa2_prio_q prio;
++	};
++};
++
++/* CEETM Qdisc configuration parameters */
++struct dpaa2_ceetm_tc_qopt {
++	enum dpaa2_ceetm_type type;
++	__u16 shaped;
++	__u8 prio_group_A;
++	__u8 prio_group_B;
++	__u8 separate_groups;
++};
++
++/* root class - corresponds to a channel */
++struct dpaa2_root_c {
++	struct dpaa2_ceetm_shaping_cfg shaping_cfg;
++	u32 ch_id;
++};
++
++/* prio class - corresponds to a strict priority queue (group) */
++struct dpaa2_prio_c {
++	struct dpaa2_ceetm_class_stats __percpu *cstats;
++	u32 qpri;
++	u8 mode;
++	u16 weight;
++};
++
++struct dpaa2_ceetm_class {
++	struct Qdisc_class_common common;
++	int refcnt;
++	struct tcf_proto *filter_list; /* class attached filters */
++	struct Qdisc *parent;
++	struct Qdisc *child;
++
++	enum dpaa2_ceetm_type type; /* ROOT/PRIO */
++	bool shaped;
++	union {
++		struct dpaa2_root_c root;
++		struct dpaa2_prio_c prio;
++	};
++};
++
++/* CEETM Class configuration parameters */
++struct dpaa2_ceetm_tc_copt {
++	enum dpaa2_ceetm_type type;
++	struct dpaa2_ceetm_shaping_cfg shaping_cfg;
++	__u16 shaped;
++	__u8 mode;
++	__u16 weight;
++};
++
++/* CEETM stats */
++struct dpaa2_ceetm_qdisc_stats {
++	__u32 drops;
++};
++
++struct dpaa2_ceetm_class_stats {
++	/* Software counters */
++	struct gnet_stats_basic_packed bstats;
++	__u32 ern_drop_count;
++	__u32 congested_count;
++};
++
++struct dpaa2_ceetm_tc_xstats {
++	__u64 ceetm_dequeue_bytes;
++	__u64 ceetm_dequeue_frames;
++	__u64 ceetm_reject_bytes;
++	__u64 ceetm_reject_frames;
++};
++
++#ifdef CONFIG_FSL_DPAA2_ETH_CEETM
++int __init dpaa2_ceetm_register(void);
++void __exit dpaa2_ceetm_unregister(void);
++int dpaa2_ceetm_classify(struct sk_buff *skb, struct Qdisc *sch,
++			 int *qdid, int *qpri);
++#else
++static inline int dpaa2_ceetm_register(void)
++{
++	return 0;
++}
++
++static inline void dpaa2_ceetm_unregister(void) {}
++
++static inline int dpaa2_ceetm_classify(struct sk_buff *skb, struct Qdisc *sch,
++				       int *qdid, int *qpri)
++{
++	return 0;
++}
++#endif
++
++static inline bool dpaa2_eth_ceetm_is_enabled(struct dpaa2_eth_priv *priv)
++{
++	return priv->ceetm_en;
++}
++
++#endif
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c
+index c916c02c..599b8132 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c
+@@ -110,7 +110,7 @@ static int dpaa2_dbg_fqs_show(struct seq_file *file, void *offset)
+ 	u32 fcnt, bcnt;
+ 	int i, err;
+ 
+-	seq_printf(file, "FQ stats for %s:\n", priv->net_dev->name);
++	seq_printf(file, "non-zero FQ stats for %s:\n", priv->net_dev->name);
+ 	seq_printf(file, "%s%16s%16s%16s%16s%16s%16s\n",
+ 		   "VFQID", "CPU", "Traffic Class", "Type", "Frames",
+ 		   "Pending frames", "Congestion");
+@@ -121,6 +121,10 @@ static int dpaa2_dbg_fqs_show(struct seq_file *file, void *offset)
+ 		if (err)
+ 			fcnt = 0;
+ 
++		/* A lot of queues, no use displaying zero traffic ones */
++		if (!fq->stats.frames && !fcnt)
++			continue;
++
+ 		seq_printf(file, "%5d%16d%16d%16s%16llu%16u%16llu\n",
+ 			   fq->fqid,
+ 			   fq->target_cpu,
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-trace.h b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-trace.h
+index e8e6522a..3b040e8d 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-trace.h
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-trace.h
+@@ -37,6 +37,7 @@
+ 
+ #include <linux/skbuff.h>
+ #include <linux/netdevice.h>
++#include "dpaa2-eth.h"
+ #include <linux/tracepoint.h>
+ 
+ #define TR_FMT "[%s] fd: addr=0x%llx, len=%u, off=%u"
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
+index a8bea3c0..fab1163b 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
+@@ -1,4 +1,5 @@
+-/* Copyright 2014-2015 Freescale Semiconductor Inc.
++/* Copyright 2014-2016 Freescale Semiconductor Inc.
++ * Copyright 2016-2017 NXP
+  *
+  * Redistribution and use in source and binary forms, with or without
+  * modification, are permitted provided that the following conditions are met:
+@@ -34,21 +35,17 @@
+ #include <linux/etherdevice.h>
+ #include <linux/of_net.h>
+ #include <linux/interrupt.h>
+-#include <linux/debugfs.h>
+-#include <linux/kthread.h>
+ #include <linux/msi.h>
+-#include <linux/net_tstamp.h>
++#include <linux/kthread.h>
+ #include <linux/iommu.h>
++#include <linux/net_tstamp.h>
+ #include <linux/bpf.h>
+ #include <linux/filter.h>
+ #include <linux/atomic.h>
+-
+-#include "../../fsl-mc/include/dpbp.h"
+-#include "../../fsl-mc/include/dpcon.h"
+-#include "../../fsl-mc/include/mc.h"
+-#include "../../fsl-mc/include/mc-sys.h"
++#include <net/sock.h>
++#include <linux/fsl/mc.h>
+ #include "dpaa2-eth.h"
+-#include "dpkg.h"
++#include "dpaa2-eth-ceetm.h"
+ 
+ /* CREATE_TRACE_POINTS only needs to be defined once. Other dpa files
+  * using trace events only need to #include <trace/events/sched.h>
+@@ -62,7 +59,8 @@ MODULE_DESCRIPTION("Freescale DPAA2 Ethernet Driver");
+ 
+ const char dpaa2_eth_drv_version[] = "0.1";
+ 
+-void *dpaa2_eth_iova_to_virt(struct iommu_domain *domain, dma_addr_t iova_addr)
++static void *dpaa2_iova_to_virt(struct iommu_domain *domain,
++				dma_addr_t iova_addr)
+ {
+ 	phys_addr_t phys_addr;
+ 
+@@ -108,26 +106,26 @@ static void free_rx_fd(struct dpaa2_eth_priv *priv,
+ 	if (fd_format == dpaa2_fd_single)
+ 		goto free_buf;
+ 	else if (fd_format != dpaa2_fd_sg)
+-		/* we don't support any other format */
++		/* We don't support any other format */
+ 		return;
+ 
+-	/* For S/G frames, we first need to free all SG entries */
++	/* For S/G frames, we first need to free all SG entries
++	 * except the first one, which was taken care of already
++	 */
+ 	sgt = vaddr + dpaa2_fd_get_offset(fd);
+-	for (i = 0; i < DPAA2_ETH_MAX_SG_ENTRIES; i++) {
++	for (i = 1; i < DPAA2_ETH_MAX_SG_ENTRIES; i++) {
+ 		addr = dpaa2_sg_get_addr(&sgt[i]);
+-		sg_vaddr = dpaa2_eth_iova_to_virt(priv->iommu_domain, addr);
+-
++		sg_vaddr = dpaa2_iova_to_virt(priv->iommu_domain, addr);
+ 		dma_unmap_single(dev, addr, DPAA2_ETH_RX_BUF_SIZE,
+ 				 DMA_BIDIRECTIONAL);
+ 
+-		put_page(virt_to_head_page(sg_vaddr));
+-
++		skb_free_frag(sg_vaddr);
+ 		if (dpaa2_sg_is_final(&sgt[i]))
+ 			break;
+ 	}
+ 
+ free_buf:
+-	put_page(virt_to_head_page(vaddr));
++	skb_free_frag(vaddr);
+ }
+ 
+ /* Build a linear skb based on a single-buffer frame descriptor */
+@@ -176,7 +174,7 @@ static struct sk_buff *build_frag_skb(struct dpaa2_eth_priv *priv,
+ 
+ 		/* Get the address and length from the S/G entry */
+ 		sg_addr = dpaa2_sg_get_addr(sge);
+-		sg_vaddr = dpaa2_eth_iova_to_virt(priv->iommu_domain, sg_addr);
++		sg_vaddr = dpaa2_iova_to_virt(priv->iommu_domain, sg_addr);
+ 		dma_unmap_single(dev, sg_addr, DPAA2_ETH_RX_BUF_SIZE,
+ 				 DMA_BIDIRECTIONAL);
+ 
+@@ -185,8 +183,20 @@ static struct sk_buff *build_frag_skb(struct dpaa2_eth_priv *priv,
+ 		if (i == 0) {
+ 			/* We build the skb around the first data buffer */
+ 			skb = build_skb(sg_vaddr, DPAA2_ETH_SKB_SIZE);
+-			if (unlikely(!skb))
+-				goto err_build;
++			if (unlikely(!skb)) {
++				/* Free the first SG entry now, since we already
++				 * unmapped it and obtained the virtual address
++				 */
++				skb_free_frag(sg_vaddr);
++
++				/* We still need to subtract the buffers used
++				 * by this FD from our software counter
++				 */
++				while (!dpaa2_sg_is_final(&sgt[i]) &&
++				       i < DPAA2_ETH_MAX_SG_ENTRIES)
++					i++;
++				break;
++			}
+ 
+ 			sg_offset = dpaa2_sg_get_offset(sge);
+ 			skb_reserve(skb, sg_offset);
+@@ -213,21 +223,12 @@ static struct sk_buff *build_frag_skb(struct dpaa2_eth_priv *priv,
+ 			break;
+ 	}
+ 
++	WARN_ONCE(i == DPAA2_ETH_MAX_SG_ENTRIES, "Final bit not set in SGT");
++
+ 	/* Count all data buffers + SG table buffer */
+ 	ch->buf_count -= i + 2;
+ 
+ 	return skb;
+-
+-err_build:
+-	/* We still need to subtract the buffers used by this FD from our
+-	 * software counter
+-	 */
+-	for (i = 0; i < DPAA2_ETH_MAX_SG_ENTRIES; i++)
+-		if (dpaa2_sg_is_final(&sgt[i]))
+-			break;
+-	ch->buf_count -= i + 2;
+-
+-	return NULL;
+ }
+ 
+ static int dpaa2_eth_xdp_tx(struct dpaa2_eth_priv *priv,
+@@ -248,7 +249,7 @@ static int dpaa2_eth_xdp_tx(struct dpaa2_eth_priv *priv,
+ 	dpaa2_fd_set_ctrl(fd, DPAA2_FD_CTRL_ASAL);
+ 
+ 	ctrl = DPAA2_FAEAD_A4V | DPAA2_FAEAD_A2V | DPAA2_FAEAD_EBDDV;
+-	faead = dpaa2_eth_get_faead(buf_start, false);
++	faead = dpaa2_get_faead(buf_start, false);
+ 	faead->ctrl = cpu_to_le32(ctrl);
+ 	faead->conf_fqid = 0;
+ 
+@@ -257,7 +258,8 @@ static int dpaa2_eth_xdp_tx(struct dpaa2_eth_priv *priv,
+ 
+ 	fq = &priv->fq[queue_id];
+ 	for (i = 0; i < DPAA2_ETH_ENQUEUE_RETRIES; i++) {
+-		err = dpaa2_io_service_enqueue_qd(NULL, priv->tx_qdid, 0,
++		err = dpaa2_io_service_enqueue_qd(fq->channel->dpio,
++						  priv->tx_qdid, 0,
+ 						  fq->tx_qdbin, fd);
+ 		if (err != -EBUSY)
+ 			break;
+@@ -282,10 +284,10 @@ static void free_bufs(struct dpaa2_eth_priv *priv, u64 *buf_array, int count)
+ 
+ 	for (i = 0; i < count; i++) {
+ 		/* Same logic as on regular Rx path */
+-		vaddr = dpaa2_eth_iova_to_virt(priv->iommu_domain, buf_array[i]);
++		vaddr = dpaa2_iova_to_virt(priv->iommu_domain, buf_array[i]);
+ 		dma_unmap_single(dev, buf_array[i], DPAA2_ETH_RX_BUF_SIZE,
+ 				 DMA_BIDIRECTIONAL);
+-		put_page(virt_to_head_page(vaddr));
++		skb_free_frag(vaddr);
+ 	}
+ }
+ 
+@@ -299,14 +301,12 @@ static void release_fd_buf(struct dpaa2_eth_priv *priv,
+ 	if (likely(ch->rel_buf_cnt < DPAA2_ETH_BUFS_PER_CMD))
+ 		return;
+ 
+-	do {
+-		err = dpaa2_io_service_release(NULL, priv->bpid,
++	while ((err = dpaa2_io_service_release(ch->dpio, priv->bpid,
+ 					       ch->rel_buf_array,
+-					       ch->rel_buf_cnt);
++					       ch->rel_buf_cnt)) == -EBUSY)
+ 		cpu_relax();
+-	} while (err == -EBUSY);
+ 
+- 	if (unlikely(err))
++	if (err)
+ 		free_bufs(priv, ch->rel_buf_array, ch->rel_buf_cnt);
+ 
+ 	ch->rel_buf_cnt = 0;
+@@ -336,14 +336,12 @@ static void dpaa2_eth_rx(struct dpaa2_eth_priv *priv,
+ 	/* Tracing point */
+ 	trace_dpaa2_rx_fd(priv->net_dev, fd);
+ 
+-	vaddr = dpaa2_eth_iova_to_virt(priv->iommu_domain, addr);
++	vaddr = dpaa2_iova_to_virt(priv->iommu_domain, addr);
+ 	dma_sync_single_for_cpu(dev, addr, DPAA2_ETH_RX_BUF_SIZE,
+ 				DMA_BIDIRECTIONAL);
+ 
+-	/* HWA - FAS, timestamp */
+-	fas = dpaa2_eth_get_fas(vaddr, false);
++	fas = dpaa2_get_fas(vaddr, false);
+ 	prefetch(fas);
+-	/* data / SG table */
+ 	buf_data = vaddr + dpaa2_fd_get_offset(fd);
+ 	prefetch(buf_data);
+ 
+@@ -352,8 +350,7 @@ static void dpaa2_eth_rx(struct dpaa2_eth_priv *priv,
+ 
+ 	xdp_prog = READ_ONCE(ch->xdp_prog);
+ 
+-	switch (fd_format) {
+-	case dpaa2_fd_single:
++	if (fd_format == dpaa2_fd_single) {
+ 		if (xdp_prog) {
+ 			xdp.data = buf_data;
+ 			xdp.data_end = buf_data + dpaa2_fd_get_len(fd);
+@@ -366,6 +363,8 @@ static void dpaa2_eth_rx(struct dpaa2_eth_priv *priv,
+ 			percpu_stats->rx_packets++;
+ 			percpu_stats->rx_bytes += dpaa2_fd_get_len(fd);
+ 
++			rcu_read_lock();
++
+ 			xdp_act = bpf_prog_run_xdp(xdp_prog, &xdp);
+ 			switch (xdp_act) {
+ 			case XDP_PASS:
+@@ -375,6 +374,7 @@ static void dpaa2_eth_rx(struct dpaa2_eth_priv *priv,
+ 			case XDP_ABORTED:
+ 			case XDP_DROP:
+ 				release_fd_buf(priv, ch, addr);
++				rcu_read_unlock();
+ 				goto drop_cnt;
+ 			case XDP_TX:
+ 				if (dpaa2_eth_xdp_tx(priv, (struct dpaa2_fd *)fd, vaddr,
+@@ -385,22 +385,23 @@ static void dpaa2_eth_rx(struct dpaa2_eth_priv *priv,
+ 					free_rx_fd(priv, fd, vaddr);
+ 					ch->buf_count--;
+ 				}
++				rcu_read_unlock();
+ 				return;
+ 			}
++
++			rcu_read_unlock();
+ 		}
+ 		dma_unmap_single(dev, addr, DPAA2_ETH_RX_BUF_SIZE,
+ 				 DMA_BIDIRECTIONAL);
+ 		skb = build_linear_skb(priv, ch, fd, vaddr);
+-		break;
+-	case dpaa2_fd_sg:
++	} else if (fd_format == dpaa2_fd_sg) {
+ 		dma_unmap_single(dev, addr, DPAA2_ETH_RX_BUF_SIZE,
+ 				 DMA_BIDIRECTIONAL);
+ 		skb = build_frag_skb(priv, ch, buf_data);
+-		put_page(virt_to_head_page(vaddr));
++		skb_free_frag(vaddr);
+ 		percpu_extras->rx_sg_frames++;
+ 		percpu_extras->rx_sg_bytes += dpaa2_fd_get_len(fd);
+-		break;
+-	default:
++	} else {
+ 		/* We don't support any other format */
+ 		goto drop_cnt;
+ 	}
+@@ -413,7 +414,7 @@ static void dpaa2_eth_rx(struct dpaa2_eth_priv *priv,
+ 	/* Get the timestamp value */
+ 	if (priv->ts_rx_en) {
+ 		struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
+-		u64 *ns = dpaa2_eth_get_ts(vaddr, false);
++		u64 *ns = dpaa2_get_ts(vaddr, false);
+ 
+ 		*ns = DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS * le64_to_cpup(ns);
+ 		memset(shhwtstamps, 0, sizeof(*shhwtstamps));
+@@ -463,23 +464,25 @@ static void dpaa2_eth_rx_err(struct dpaa2_eth_priv *priv,
+ 	struct rtnl_link_stats64 *percpu_stats;
+ 	struct dpaa2_fas *fas;
+ 	u32 status = 0;
+-	bool check_fas_errors = false;
++	u32 fd_errors;
++	bool has_fas_errors = false;
+ 
+-	vaddr = dpaa2_eth_iova_to_virt(priv->iommu_domain, addr);
++	vaddr = dpaa2_iova_to_virt(priv->iommu_domain, addr);
+ 	dma_unmap_single(dev, addr, DPAA2_ETH_RX_BUF_SIZE, DMA_BIDIRECTIONAL);
+ 
+ 	/* check frame errors in the FD field */
+-	if (fd->simple.ctrl & DPAA2_FD_RX_ERR_MASK) {
+-		check_fas_errors = !!(fd->simple.ctrl & FD_CTRL_FAERR) &&
+-			!!(dpaa2_fd_get_frc(fd) & DPAA2_FD_FRC_FASV);
++	fd_errors = dpaa2_fd_get_ctrl(fd) & DPAA2_FD_RX_ERR_MASK;
++	if (likely(fd_errors)) {
++		has_fas_errors = (fd_errors & FD_CTRL_FAERR) &&
++				 !!(dpaa2_fd_get_frc(fd) & DPAA2_FD_FRC_FASV);
+ 		if (net_ratelimit())
+-			netdev_dbg(priv->net_dev, "Rx frame FD err: %x08\n",
+-				   fd->simple.ctrl & DPAA2_FD_RX_ERR_MASK);
++			netdev_dbg(priv->net_dev, "RX frame FD err: %08x\n",
++				   fd_errors);
+ 	}
+ 
+ 	/* check frame errors in the FAS field */
+-	if (check_fas_errors) {
+-		fas = dpaa2_eth_get_fas(vaddr, false);
++	if (has_fas_errors) {
++		fas = dpaa2_get_fas(vaddr, false);
+ 		status = le32_to_cpu(fas->status);
+ 		if (net_ratelimit())
+ 			netdev_dbg(priv->net_dev, "Rx frame FAS err: 0x%08x\n",
+@@ -489,6 +492,7 @@ static void dpaa2_eth_rx_err(struct dpaa2_eth_priv *priv,
+ 
+ 	percpu_stats = this_cpu_ptr(priv->percpu_stats);
+ 	percpu_stats->rx_errors++;
++	ch->buf_count--;
+ }
+ #endif
+ 
+@@ -523,11 +527,9 @@ static bool consume_frames(struct dpaa2_eth_channel *ch, int *rx_cleaned,
+ 		}
+ 
+ 		fd = dpaa2_dq_fd(dq);
+-
+-		/* prefetch the frame descriptor */
+ 		prefetch(fd);
+ 
+-		fq = (struct dpaa2_eth_fq *)dpaa2_dq_fqd_ctx(dq);
++		fq = (struct dpaa2_eth_fq *)(uintptr_t)dpaa2_dq_fqd_ctx(dq);
+ 		fq->consume(priv, ch, fd, &ch->napi, fq->flowid);
+ 		cleaned++;
+ 	} while (!is_last);
+@@ -553,8 +555,7 @@ static bool consume_frames(struct dpaa2_eth_channel *ch, int *rx_cleaned,
+ static void enable_tx_tstamp(struct dpaa2_fd *fd, void *buf_start)
+ {
+ 	struct dpaa2_faead *faead;
+-	u32 ctrl;
+-	u32 frc;
++	u32 ctrl, frc;
+ 
+ 	/* Mark the egress frame annotation area as valid */
+ 	frc = dpaa2_fd_get_frc(fd);
+@@ -568,7 +569,7 @@ static void enable_tx_tstamp(struct dpaa2_fd *fd, void *buf_start)
+ 	 * hardware frame annotation area
+ 	 */
+ 	ctrl = DPAA2_FAEAD_A2V | DPAA2_FAEAD_UPDV | DPAA2_FAEAD_UPD;
+-	faead = dpaa2_eth_get_faead(buf_start, true);
++	faead = dpaa2_get_faead(buf_start, true);
+ 	faead->ctrl = cpu_to_le32(ctrl);
+ }
+ 
+@@ -611,13 +612,14 @@ static int build_sg_fd(struct dpaa2_eth_priv *priv,
+ 
+ 	/* Prepare the HW SGT structure */
+ 	sgt_buf_size = priv->tx_data_offset +
+-		       sizeof(struct dpaa2_sg_entry) * (1 + num_dma_bufs);
+-	sgt_buf = kzalloc(sgt_buf_size + DPAA2_ETH_TX_BUF_ALIGN, GFP_ATOMIC);
++		       sizeof(struct dpaa2_sg_entry) *  num_dma_bufs;
++	sgt_buf = netdev_alloc_frag(sgt_buf_size + DPAA2_ETH_TX_BUF_ALIGN);
+ 	if (unlikely(!sgt_buf)) {
+ 		err = -ENOMEM;
+ 		goto sgt_buf_alloc_failed;
+ 	}
+ 	sgt_buf = PTR_ALIGN(sgt_buf, DPAA2_ETH_TX_BUF_ALIGN);
++	memset(sgt_buf, 0, sgt_buf_size);
+ 
+ 	sgt = (struct dpaa2_sg_entry *)(sgt_buf + priv->tx_data_offset);
+ 
+@@ -644,7 +646,7 @@ static int build_sg_fd(struct dpaa2_eth_priv *priv,
+ 	swa->sg.skb = skb;
+ 	swa->sg.scl = scl;
+ 	swa->sg.num_sg = num_sg;
+-	swa->sg.num_dma_bufs = num_dma_bufs;
++	swa->sg.sgt_size = sgt_buf_size;
+ 
+ 	/* Separately map the SGT buffer */
+ 	addr = dma_map_single(dev, sgt_buf, sgt_buf_size, DMA_BIDIRECTIONAL);
+@@ -664,7 +666,7 @@ static int build_sg_fd(struct dpaa2_eth_priv *priv,
+ 	return 0;
+ 
+ dma_map_single_failed:
+-	kfree(sgt_buf);
++	skb_free_frag(sgt_buf);
+ sgt_buf_alloc_failed:
+ 	dma_unmap_sg(dev, scl, num_sg, DMA_BIDIRECTIONAL);
+ dma_map_sg_failed:
+@@ -733,16 +735,13 @@ static void free_tx_fd(struct dpaa2_eth_priv *priv,
+ {
+ 	struct device *dev = priv->net_dev->dev.parent;
+ 	dma_addr_t fd_addr;
+-	struct sk_buff *skb;
++	struct sk_buff *skb = NULL;
+ 	unsigned char *buffer_start;
+-	int unmap_size;
+-	struct scatterlist *scl;
+-	int num_sg, num_dma_bufs;
+ 	struct dpaa2_eth_swa *swa;
+ 	u8 fd_format = dpaa2_fd_get_format(fd);
+ 
+ 	fd_addr = dpaa2_fd_get_addr(fd);
+-	buffer_start = dpaa2_eth_iova_to_virt(priv->iommu_domain, fd_addr);
++	buffer_start = dpaa2_iova_to_virt(priv->iommu_domain, fd_addr);
+ 	swa = (struct dpaa2_eth_swa *)buffer_start;
+ 
+ 	if (fd_format == dpaa2_fd_single) {
+@@ -755,18 +754,17 @@ static void free_tx_fd(struct dpaa2_eth_priv *priv,
+ 				 DMA_BIDIRECTIONAL);
+ 	} else if (fd_format == dpaa2_fd_sg) {
+ 		skb = swa->sg.skb;
+-		scl = swa->sg.scl;
+-		num_sg = swa->sg.num_sg;
+-		num_dma_bufs = swa->sg.num_dma_bufs;
+ 
+ 		/* Unmap the scatterlist */
+-		dma_unmap_sg(dev, scl, num_sg, DMA_BIDIRECTIONAL);
+-		kfree(scl);
++		dma_unmap_sg(dev, swa->sg.scl, swa->sg.num_sg, DMA_BIDIRECTIONAL);
++		kfree(swa->sg.scl);
+ 
+ 		/* Unmap the SGT buffer */
+-		unmap_size = priv->tx_data_offset +
+-		       sizeof(struct dpaa2_sg_entry) * (1 + num_dma_bufs);
+-		dma_unmap_single(dev, fd_addr, unmap_size, DMA_BIDIRECTIONAL);
++		dma_unmap_single(dev, fd_addr, swa->sg.sgt_size,
++				 DMA_BIDIRECTIONAL);
++	} else {
++		netdev_dbg(priv->net_dev, "Invalid FD format\n");
++		return;
+ 	}
+ 
+ 	/* Get the timestamp value */
+@@ -776,21 +774,21 @@ static void free_tx_fd(struct dpaa2_eth_priv *priv,
+ 
+ 		memset(&shhwtstamps, 0, sizeof(shhwtstamps));
+ 
+-		ns = dpaa2_eth_get_ts(buffer_start, true);
++		ns = dpaa2_get_ts(buffer_start, true);
+ 		*ns = DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS * le64_to_cpup(ns);
+ 		shhwtstamps.hwtstamp = ns_to_ktime(*ns);
+ 		skb_tstamp_tx(skb, &shhwtstamps);
+ 	}
+ 
+-	/* Free SGT buffer kmalloc'ed on tx */
++	/* Free SGT buffer allocated on tx */
+ 	if (fd_format != dpaa2_fd_single)
+-		kfree(buffer_start);
++		skb_free_frag(buffer_start);
+ 
+ 	/* Move on with skb release */
+ 	napi_consume_skb(skb, in_napi);
+ }
+ 
+-static int dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
++static netdev_tx_t dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
+ {
+ 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
+ 	struct device *dev = net_dev->dev.parent;
+@@ -799,14 +797,15 @@ static int dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
+ 	struct dpaa2_eth_drv_stats *percpu_extras;
+ 	unsigned int needed_headroom;
+ 	struct dpaa2_eth_fq *fq;
+-	u16 queue_mapping = skb_get_queue_mapping(skb);
+-	int err, i;
++	u16 queue_mapping;
++	int err, i, ch_id = 0, qpri = 0;
+ 
+-	/* If we're congested, stop this tx queue; transmission of the
+-	 * current skb happens regardless of congestion state
+-	 */
++	queue_mapping = skb_get_queue_mapping(skb);
+ 	fq = &priv->fq[queue_mapping];
+ 
++	/* If we're congested, stop this tx queue; transmission of
++	 * the current skb happens regardless of congestion state
++	 */
+ 	dma_sync_single_for_cpu(dev, priv->cscn_dma,
+ 				DPAA2_CSCN_SIZE, DMA_FROM_DEVICE);
+ 	if (unlikely(dpaa2_cscn_state_congested(priv->cscn_mem))) {
+@@ -828,6 +827,8 @@ static int dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
+ 			goto err_alloc_headroom;
+ 		}
+ 		percpu_extras->tx_reallocs++;
++		if (skb->sk)
++			skb_set_owner_w(ns, skb->sk);
+ 		dev_kfree_skb(skb);
+ 		skb = ns;
+ 	}
+@@ -861,13 +862,16 @@ static int dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
+ 	/* Tracing point */
+ 	trace_dpaa2_tx_fd(net_dev, &fd);
+ 
++	if (dpaa2_eth_ceetm_is_enabled(priv)) {
++		err = dpaa2_ceetm_classify(skb, net_dev->qdisc, &ch_id, &qpri);
++		if (err)
++			goto err_ceetm_classify;
++	}
++
+ 	for (i = 0; i < DPAA2_ETH_ENQUEUE_RETRIES; i++) {
+-		err = dpaa2_io_service_enqueue_qd(NULL, priv->tx_qdid, 0,
++		err = dpaa2_io_service_enqueue_qd(fq->channel->dpio,
++						  priv->tx_qdid, qpri,
+ 						  fq->tx_qdbin, &fd);
+-		/* TODO: This doesn't work. Check on simulator.
+-		 * err = dpaa2_io_service_enqueue_fq(NULL,
+-		 *			priv->fq[0].fqid_tx, &fd);
+-		 */
+ 		if (err != -EBUSY)
+ 			break;
+ 	}
+@@ -883,6 +887,8 @@ static int dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
+ 
+ 	return NETDEV_TX_OK;
+ 
++err_ceetm_classify:
++	free_tx_fd(priv, &fd, false);
+ err_build_fd:
+ err_alloc_headroom:
+ 	dev_kfree_skb(skb);
+@@ -900,7 +906,7 @@ static void dpaa2_eth_tx_conf(struct dpaa2_eth_priv *priv,
+ 	struct device *dev = priv->net_dev->dev.parent;
+ 	struct rtnl_link_stats64 *percpu_stats;
+ 	struct dpaa2_eth_drv_stats *percpu_extras;
+-	bool errors = !!(fd->simple.ctrl & DPAA2_FD_TX_ERR_MASK);
++	u32 fd_errors;
+ 
+ 	/* Tracing point */
+ 	trace_dpaa2_tx_conf_fd(priv->net_dev, fd);
+@@ -917,16 +923,16 @@ static void dpaa2_eth_tx_conf(struct dpaa2_eth_priv *priv,
+ 			netif_tx_wake_all_queues(priv->net_dev);
+ 	}
+ 
+-	/* check frame errors in the FD field */
++	/* Check frame errors in the FD field */
++	fd_errors = dpaa2_fd_get_ctrl(fd) & DPAA2_FD_TX_ERR_MASK;
+ 	free_tx_fd(priv, fd, true);
+ 
+-	/* if there are no errors, we're done */
+-	if (likely(!errors))
++	if (likely(!fd_errors))
+ 		return;
+ 
+ 	if (net_ratelimit())
+ 		netdev_dbg(priv->net_dev, "TX frame FD error: 0x%08x\n",
+-			   errors);
++			   fd_errors);
+ 
+ 	percpu_stats = this_cpu_ptr(priv->percpu_stats);
+ 	/* Tx-conf logically pertains to the egress path. */
+@@ -941,7 +947,7 @@ static int set_rx_csum(struct dpaa2_eth_priv *priv, bool enable)
+ 			       DPNI_OFF_RX_L3_CSUM, enable);
+ 	if (err) {
+ 		netdev_err(priv->net_dev,
+-			   "dpni_set_offload() DPNI_OFF_RX_L3_CSUM failed\n");
++			   "dpni_set_offload(RX_L3_CSUM) failed\n");
+ 		return err;
+ 	}
+ 
+@@ -949,7 +955,7 @@ static int set_rx_csum(struct dpaa2_eth_priv *priv, bool enable)
+ 			       DPNI_OFF_RX_L4_CSUM, enable);
+ 	if (err) {
+ 		netdev_err(priv->net_dev,
+-			   "dpni_set_offload() DPNI_OFF_RX_L4_CSUM failed\n");
++			   "dpni_set_offload(RX_L4_CSUM) failed\n");
+ 		return err;
+ 	}
+ 
+@@ -963,16 +969,14 @@ static int set_tx_csum(struct dpaa2_eth_priv *priv, bool enable)
+ 	err = dpni_set_offload(priv->mc_io, 0, priv->mc_token,
+ 			       DPNI_OFF_TX_L3_CSUM, enable);
+ 	if (err) {
+-		netdev_err(priv->net_dev,
+-			   "dpni_set_offload() DPNI_OFF_RX_L3_CSUM failed\n");
++		netdev_err(priv->net_dev, "dpni_set_offload(TX_L3_CSUM) failed\n");
+ 		return err;
+ 	}
+ 
+ 	err = dpni_set_offload(priv->mc_io, 0, priv->mc_token,
+ 			       DPNI_OFF_TX_L4_CSUM, enable);
+ 	if (err) {
+-		netdev_err(priv->net_dev,
+-			   "dpni_set_offload() DPNI_OFF_RX_L4_CSUM failed\n");
++		netdev_err(priv->net_dev, "dpni_set_offload(TX_L4_CSUM) failed\n");
+ 		return err;
+ 	}
+ 
+@@ -982,7 +986,8 @@ static int set_tx_csum(struct dpaa2_eth_priv *priv, bool enable)
+ /* Perform a single release command to add buffers
+  * to the specified buffer pool
+  */
+-static int add_bufs(struct dpaa2_eth_priv *priv, u16 bpid)
++static int add_bufs(struct dpaa2_eth_priv *priv,
++		    struct dpaa2_eth_channel *ch, u16 bpid)
+ {
+ 	struct device *dev = priv->net_dev->dev.parent;
+ 	u64 buf_array[DPAA2_ETH_BUFS_PER_CMD];
+@@ -992,7 +997,7 @@ static int add_bufs(struct dpaa2_eth_priv *priv, u16 bpid)
+ 
+ 	for (i = 0; i < DPAA2_ETH_BUFS_PER_CMD; i++) {
+ 		/* Allocate buffer visible to WRIOP + skb shared info +
+-		 * alignment padding.
++		 * alignment padding
+ 		 */
+ 		buf = napi_alloc_frag(dpaa2_eth_buf_raw_size(priv));
+ 		if (unlikely(!buf))
+@@ -1016,14 +1021,14 @@ static int add_bufs(struct dpaa2_eth_priv *priv, u16 bpid)
+ 
+ release_bufs:
+ 	/* In case the portal is busy, retry until successful */
+-	while ((err = dpaa2_io_service_release(NULL, bpid,
++	while ((err = dpaa2_io_service_release(ch->dpio, bpid,
+ 					       buf_array, i)) == -EBUSY)
+ 		cpu_relax();
+ 
+ 	/* If release command failed, clean up and bail out; not much
+ 	 * else we can do about it
+ 	 */
+-	if (unlikely(err)) {
++	if (err) {
+ 		free_bufs(priv, buf_array, i);
+ 		return 0;
+ 	}
+@@ -1031,7 +1036,7 @@ static int add_bufs(struct dpaa2_eth_priv *priv, u16 bpid)
+ 	return i;
+ 
+ err_map:
+-	put_page(virt_to_head_page(buf));
++	skb_free_frag(buf);
+ err_alloc:
+ 	/* If we managed to allocate at least some buffers, release them */
+ 	if (i)
+@@ -1054,9 +1059,9 @@ static int seed_pool(struct dpaa2_eth_priv *priv, u16 bpid)
+ 	preempt_disable();
+ 	for (j = 0; j < priv->num_channels; j++) {
+ 		priv->channel[j]->buf_count = 0;
+-		for (i = 0; i < priv->num_bufs;
++		for (i = 0; i < priv->max_bufs_per_ch;
+ 		     i += DPAA2_ETH_BUFS_PER_CMD) {
+-			new_count = add_bufs(priv, bpid);
++			new_count = add_bufs(priv, priv->channel[j], bpid);
+ 			priv->channel[j]->buf_count += new_count;
+ 
+ 			if (new_count < DPAA2_ETH_BUFS_PER_CMD) {
+@@ -1111,15 +1116,15 @@ static int refill_pool(struct dpaa2_eth_priv *priv,
+ 		return 0;
+ 
+ 	do {
+-		new_count = add_bufs(priv, bpid);
++		new_count = add_bufs(priv, ch, bpid);
+ 		if (unlikely(!new_count)) {
+ 			/* Out of memory; abort for now, we'll try later on */
+ 			break;
+ 		}
+ 		ch->buf_count += new_count;
+-	} while (ch->buf_count < priv->num_bufs);
++	} while (ch->buf_count < priv->max_bufs_per_ch);
+ 
+-	if (unlikely(ch->buf_count < priv->num_bufs))
++	if (unlikely(ch->buf_count < priv->max_bufs_per_ch))
+ 		return -ENOMEM;
+ 
+ 	return 0;
+@@ -1132,7 +1137,8 @@ static int pull_channel(struct dpaa2_eth_channel *ch)
+ 
+ 	/* Retry while portal is busy */
+ 	do {
+-		err = dpaa2_io_service_pull_channel(NULL, ch->ch_id, ch->store);
++		err = dpaa2_io_service_pull_channel(ch->dpio, ch->ch_id,
++						    ch->store);
+ 		dequeues++;
+ 		cpu_relax();
+ 	} while (err == -EBUSY);
+@@ -1153,7 +1159,7 @@ static int pull_channel(struct dpaa2_eth_channel *ch)
+ static int dpaa2_eth_poll(struct napi_struct *napi, int budget)
+ {
+ 	struct dpaa2_eth_channel *ch;
+-	int  rx_cleaned = 0, tx_conf_cleaned = 0;
++	int rx_cleaned = 0, tx_conf_cleaned = 0;
+ 	bool store_cleaned;
+ 	struct dpaa2_eth_priv *priv;
+ 	int err;
+@@ -1181,13 +1187,15 @@ static int dpaa2_eth_poll(struct napi_struct *napi, int budget)
+ 	} while (store_cleaned);
+ 
+ 	/* We didn't consume the entire budget, finish napi and
+-	 * re-enable data availability notifications.
++	 * re-enable data availability notifications
+ 	 */
+ 	napi_complete(napi);
+ 	do {
+-		err = dpaa2_io_service_rearm(NULL, &ch->nctx);
++		err = dpaa2_io_service_rearm(ch->dpio, &ch->nctx);
+ 		cpu_relax();
+ 	} while (err == -EBUSY);
++		WARN_ONCE(err, "CDAN notifications rearm failed on core %d",
++			  ch->nctx.desired_cpu);
+ 
+ 	return max(rx_cleaned, 1);
+ }
+@@ -1239,7 +1247,7 @@ static int link_state_update(struct dpaa2_eth_priv *priv)
+ 		netif_carrier_off(priv->net_dev);
+ 	}
+ 
+-	netdev_info(priv->net_dev, "Link Event: state %s",
++	netdev_info(priv->net_dev, "Link Event: state %s\n",
+ 		    state.up ? "up" : "down");
+ 
+ 	return 0;
+@@ -1272,10 +1280,7 @@ static int dpaa2_eth_open(struct net_device *net_dev)
+ 			   priv->dpbp_dev->obj_desc.id, priv->bpid);
+ 	}
+ 
+-	if (priv->tx_pause_frames)
+-		priv->refill_thresh = priv->num_bufs - DPAA2_ETH_BUFS_PER_CMD;
+-	else
+-		priv->refill_thresh = DPAA2_ETH_REFILL_THRESH_TD;
++	priv->refill_thresh = DPAA2_ETH_REFILL_THRESH(priv);
+ 
+ 	err = dpni_enable(priv->mc_io, 0, priv->mc_token);
+ 	if (err < 0) {
+@@ -1318,7 +1323,7 @@ static int dpaa2_eth_stop(struct net_device *net_dev)
+ 		dpni_disable(priv->mc_io, 0, priv->mc_token);
+ 		dpni_is_enabled(priv->mc_io, 0, priv->mc_token, &dpni_enabled);
+ 		if (dpni_enabled)
+-			/* Allow the MC some slack */
++			/* Allow the hardware some slack */
+ 			msleep(100);
+ 	} while (dpni_enabled && --retries);
+ 	if (!retries) {
+@@ -1332,7 +1337,7 @@ static int dpaa2_eth_stop(struct net_device *net_dev)
+ 	priv->refill_thresh = 0;
+ 
+ 	/* Wait for all running napi poll routines to finish, so that no
+-	 * new refill operations are started.
++	 * new refill operations are started
+ 	 */
+ 	for (i = 0; i < priv->num_channels; i++)
+ 		napi_synchronize(&priv->channel[i]->napi);
+@@ -1343,34 +1348,6 @@ static int dpaa2_eth_stop(struct net_device *net_dev)
+ 	return err;
+ }
+ 
+-static int dpaa2_eth_init(struct net_device *net_dev)
+-{
+-	u64 supported = 0;
+-	u64 not_supported = 0;
+-	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
+-	u32 options = priv->dpni_attrs.options;
+-
+-	/* Capabilities listing */
+-	supported |= IFF_LIVE_ADDR_CHANGE;
+-
+-	if (options & DPNI_OPT_NO_MAC_FILTER)
+-		not_supported |= IFF_UNICAST_FLT;
+-	else
+-		supported |= IFF_UNICAST_FLT;
+-
+-	net_dev->priv_flags |= supported;
+-	net_dev->priv_flags &= ~not_supported;
+-
+-	/* Features */
+-	net_dev->features = NETIF_F_RXCSUM |
+-			    NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+-			    NETIF_F_SG | NETIF_F_HIGHDMA |
+-			    NETIF_F_LLTX;
+-	net_dev->hw_features = net_dev->features;
+-
+-	return 0;
+-}
+-
+ static int dpaa2_eth_set_addr(struct net_device *net_dev, void *addr)
+ {
+ 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
+@@ -1414,25 +1391,6 @@ static void dpaa2_eth_get_stats(struct net_device *net_dev,
+ 	}
+ }
+ 
+-static int dpaa2_eth_change_mtu(struct net_device *net_dev, int mtu)
+-{
+-	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
+-	int err;
+-
+-	/* Set the maximum Rx frame length to match the transmit side;
+-	 * account for L2 headers when computing the MFL
+-	 */
+-	err = dpni_set_max_frame_length(priv->mc_io, 0, priv->mc_token,
+-					(u16)DPAA2_ETH_L2_MAX_FRM(mtu));
+-	if (err) {
+-		netdev_err(net_dev, "dpni_set_max_frame_length() failed\n");
+-		return err;
+-	}
+-
+-	net_dev->mtu = mtu;
+-	return 0;
+-}
+-
+ /* Copy mac unicast addresses from @net_dev to @priv.
+  * Its sole purpose is to make dpaa2_eth_set_rx_mode() more readable.
+  */
+@@ -1497,7 +1455,7 @@ static void dpaa2_eth_set_rx_mode(struct net_device *net_dev)
+ 	}
+ 	if (mc_count + uc_count > max_mac) {
+ 		netdev_info(net_dev,
+-			    "Unicast + Multicast addr count reached %d, max allowed is %d; forcing promisc\n",
++			    "Unicast + multicast addr count reached %d, max allowed is %d; forcing promisc\n",
+ 			    uc_count + mc_count, max_mac);
+ 		goto force_mc_promisc;
+ 	}
+@@ -1633,6 +1591,78 @@ static int dpaa2_eth_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+ 	return -EINVAL;
+ }
+ 
++static int set_buffer_layout(struct dpaa2_eth_priv *priv)
++{
++	struct device *dev = priv->net_dev->dev.parent;
++	struct dpni_buffer_layout buf_layout = {0};
++	int err;
++
++	/* We need to check for WRIOP version 1.0.0, but depending on the MC
++	 * version, this number is not always provided correctly on rev1.
++	 * We need to check for both alternatives in this situation.
++	 */
++	if (priv->dpni_attrs.wriop_version == DPAA2_WRIOP_VERSION(0, 0, 0) ||
++	    priv->dpni_attrs.wriop_version == DPAA2_WRIOP_VERSION(1, 0, 0))
++		priv->rx_buf_align = DPAA2_ETH_RX_BUF_ALIGN_REV1;
++	else
++		priv->rx_buf_align = DPAA2_ETH_RX_BUF_ALIGN;
++
++	/* tx buffer */
++	buf_layout.pass_timestamp = true;
++	buf_layout.private_data_size = DPAA2_ETH_SWA_SIZE;
++	buf_layout.options = DPNI_BUF_LAYOUT_OPT_TIMESTAMP |
++			     DPNI_BUF_LAYOUT_OPT_PRIVATE_DATA_SIZE;
++	err = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,
++				     DPNI_QUEUE_TX, &buf_layout);
++	if (err) {
++		dev_err(dev, "dpni_set_buffer_layout(TX) failed\n");
++		return err;
++	}
++
++	/* tx-confirm buffer */
++	buf_layout.options = DPNI_BUF_LAYOUT_OPT_TIMESTAMP;
++	err = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,
++				     DPNI_QUEUE_TX_CONFIRM, &buf_layout);
++	if (err) {
++		dev_err(dev, "dpni_set_buffer_layout(TX_CONF) failed\n");
++		return err;
++	}
++
++	/* Now that we've set our tx buffer layout, retrieve the minimum
++	 * required tx data offset.
++	 */
++	err = dpni_get_tx_data_offset(priv->mc_io, 0, priv->mc_token,
++				      &priv->tx_data_offset);
++	if (err) {
++		dev_err(dev, "dpni_get_tx_data_offset() failed\n");
++		return err;
++	}
++
++	if ((priv->tx_data_offset % 64) != 0)
++		dev_warn(dev, "Tx data offset (%d) not a multiple of 64B\n",
++			 priv->tx_data_offset);
++
++	/* rx buffer */
++	buf_layout.pass_frame_status = true;
++	buf_layout.pass_parser_result = true;
++	buf_layout.data_align = priv->rx_buf_align;
++	buf_layout.private_data_size = 0;
++	buf_layout.data_head_room = dpaa2_eth_rx_headroom(priv);
++	buf_layout.options = DPNI_BUF_LAYOUT_OPT_PARSER_RESULT |
++			     DPNI_BUF_LAYOUT_OPT_FRAME_STATUS |
++			     DPNI_BUF_LAYOUT_OPT_DATA_ALIGN |
++			     DPNI_BUF_LAYOUT_OPT_DATA_HEAD_ROOM |
++			     DPNI_BUF_LAYOUT_OPT_TIMESTAMP;
++	err = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,
++				     DPNI_QUEUE_RX, &buf_layout);
++	if (err) {
++		dev_err(dev, "dpni_set_buffer_layout(RX) failed\n");
++		return err;
++	}
++
++	return 0;
++}
++
+ static int dpaa2_eth_set_xdp(struct net_device *net_dev, struct bpf_prog *prog)
+ {
+ 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
+@@ -1693,10 +1723,8 @@ static const struct net_device_ops dpaa2_eth_ops = {
+ 	.ndo_open = dpaa2_eth_open,
+ 	.ndo_start_xmit = dpaa2_eth_tx,
+ 	.ndo_stop = dpaa2_eth_stop,
+-	.ndo_init = dpaa2_eth_init,
+ 	.ndo_set_mac_address = dpaa2_eth_set_addr,
+ 	.ndo_get_stats64 = dpaa2_eth_get_stats,
+-	.ndo_change_mtu = dpaa2_eth_change_mtu,
+ 	.ndo_set_rx_mode = dpaa2_eth_set_rx_mode,
+ 	.ndo_set_features = dpaa2_eth_set_features,
+ 	.ndo_do_ioctl = dpaa2_eth_ioctl,
+@@ -1733,34 +1761,32 @@ static struct fsl_mc_device *setup_dpcon(struct dpaa2_eth_priv *priv)
+ 	err = dpcon_open(priv->mc_io, 0, dpcon->obj_desc.id, &dpcon->mc_handle);
+ 	if (err) {
+ 		dev_err(dev, "dpcon_open() failed\n");
+-		goto err_open;
++		goto free;
+ 	}
+ 
+ 	err = dpcon_reset(priv->mc_io, 0, dpcon->mc_handle);
+ 	if (err) {
+ 		dev_err(dev, "dpcon_reset() failed\n");
+-		goto err_reset;
++		goto close;
+ 	}
+ 
+ 	err = dpcon_get_attributes(priv->mc_io, 0, dpcon->mc_handle, &attrs);
+ 	if (err) {
+ 		dev_err(dev, "dpcon_get_attributes() failed\n");
+-		goto err_get_attr;
++		goto close;
+ 	}
+ 
+ 	err = dpcon_enable(priv->mc_io, 0, dpcon->mc_handle);
+ 	if (err) {
+ 		dev_err(dev, "dpcon_enable() failed\n");
+-		goto err_enable;
++		goto close;
+ 	}
+ 
+ 	return dpcon;
+ 
+-err_enable:
+-err_get_attr:
+-err_reset:
++close:
+ 	dpcon_close(priv->mc_io, 0, dpcon->mc_handle);
+-err_open:
++free:
+ 	fsl_mc_object_free(dpcon);
+ 
+ 	return NULL;
+@@ -1851,6 +1877,7 @@ static int setup_dpio(struct dpaa2_eth_priv *priv)
+ 		if (!channel) {
+ 			dev_info(dev,
+ 				 "No affine channel for cpu %d and above\n", i);
++			err = -ENODEV;
+ 			goto err_alloc_ch;
+ 		}
+ 
+@@ -1863,12 +1890,16 @@ static int setup_dpio(struct dpaa2_eth_priv *priv)
+ 		nctx->desired_cpu = i;
+ 
+ 		/* Register the new context */
+-		err = dpaa2_io_service_register(NULL, nctx);
++		channel->dpio = dpaa2_io_service_select(i);
++		err = dpaa2_io_service_register(channel->dpio, nctx);
+ 		if (err) {
+ 			dev_dbg(dev, "No affine DPIO for cpu %d\n", i);
+ 			/* If no affine DPIO for this core, there's probably
+-			 * none available for next cores either.
++			 * none available for next cores either. Signal we want
++			 * to retry later, in case the DPIO devices weren't
++			 * probed yet.
+ 			 */
++			err = -EPROBE_DEFER;
+ 			goto err_service_reg;
+ 		}
+ 
+@@ -1897,23 +1928,17 @@ static int setup_dpio(struct dpaa2_eth_priv *priv)
+ 			break;
+ 	}
+ 
+-	/* Tx confirmation queues can only be serviced by cpus
+-	 * with an affine DPIO/channel
+-	 */
+-	cpumask_copy(&priv->txconf_cpumask, &priv->dpio_cpumask);
+-
+ 	return 0;
+ 
+ err_set_cdan:
+-	dpaa2_io_service_deregister(NULL, nctx);
++	dpaa2_io_service_deregister(channel->dpio, nctx);
+ err_service_reg:
+ 	free_channel(priv, channel);
+ err_alloc_ch:
+ 	if (cpumask_empty(&priv->dpio_cpumask)) {
+-		dev_dbg(dev, "No cpu with an affine DPIO/DPCON\n");
+-		return -ENODEV;
++		dev_err(dev, "No cpu with an affine DPIO/DPCON\n");
++		return err;
+ 	}
+-	cpumask_copy(&priv->txconf_cpumask, &priv->dpio_cpumask);
+ 
+ 	dev_info(dev, "Cores %*pbl available for processing ingress traffic\n",
+ 		 cpumask_pr_args(&priv->dpio_cpumask));
+@@ -1929,7 +1954,7 @@ static void free_dpio(struct dpaa2_eth_priv *priv)
+ 	/* deregister CDAN notifications and free channels */
+ 	for (i = 0; i < priv->num_channels; i++) {
+ 		ch = priv->channel[i];
+-		dpaa2_io_service_deregister(NULL, &ch->nctx);
++		dpaa2_io_service_deregister(ch->dpio, &ch->nctx);
+ 		free_channel(priv, ch);
+ 	}
+ }
+@@ -1955,7 +1980,7 @@ static struct dpaa2_eth_channel *get_affine_channel(struct dpaa2_eth_priv *priv,
+ static void set_fq_affinity(struct dpaa2_eth_priv *priv)
+ {
+ 	struct device *dev = priv->net_dev->dev.parent;
+-	struct cpumask xps_mask = CPU_MASK_NONE;
++	struct cpumask xps_mask;
+ 	struct dpaa2_eth_fq *fq;
+ 	int rx_cpu, txc_cpu;
+ 	int i, err;
+@@ -1964,8 +1989,7 @@ static void set_fq_affinity(struct dpaa2_eth_priv *priv)
+ 	 * This may well change at runtime, either through irqbalance or
+ 	 * through direct user intervention.
+ 	 */
+-	rx_cpu = cpumask_first(&priv->dpio_cpumask);
+-	txc_cpu = cpumask_first(&priv->txconf_cpumask);
++	rx_cpu = txc_cpu = cpumask_first(&priv->dpio_cpumask);
+ 
+ 	for (i = 0; i < priv->num_fqs; i++) {
+ 		fq = &priv->fq[i];
+@@ -1980,18 +2004,19 @@ static void set_fq_affinity(struct dpaa2_eth_priv *priv)
+ 		case DPAA2_TX_CONF_FQ:
+ 			fq->target_cpu = txc_cpu;
+ 
+-			/* register txc_cpu to XPS */
++			/* Tell the stack to affine to txc_cpu the Tx queue
++			 * associated with the confirmation one
++			 */
++			cpumask_clear(&xps_mask);
+ 			cpumask_set_cpu(txc_cpu, &xps_mask);
+ 			err = netif_set_xps_queue(priv->net_dev, &xps_mask,
+ 						  fq->flowid);
+ 			if (err)
+-				dev_info_once(dev,
+-					      "Tx: error setting XPS queue\n");
+-			cpumask_clear_cpu(txc_cpu, &xps_mask);
++				dev_info_once(dev, "Error setting XPS queue\n");
+ 
+-			txc_cpu = cpumask_next(txc_cpu, &priv->txconf_cpumask);
++			txc_cpu = cpumask_next(txc_cpu, &priv->dpio_cpumask);
+ 			if (txc_cpu >= nr_cpu_ids)
+-				txc_cpu = cpumask_first(&priv->txconf_cpumask);
++				txc_cpu = cpumask_first(&priv->dpio_cpumask);
+ 			break;
+ 		default:
+ 			dev_err(dev, "Unknown FQ type: %d\n", fq->type);
+@@ -2004,10 +2029,9 @@ static void setup_fqs(struct dpaa2_eth_priv *priv)
+ {
+ 	int i, j;
+ 
+-	/* We have one TxConf FQ per Tx flow. Tx queues MUST be at the
+-	 * beginning of the queue array.
+-	 * Number of Rx and Tx queues are the same.
+-	 * We only support one traffic class for now.
++	/* We have one TxConf FQ per Tx flow.
++	 * The number of Tx and Rx queues is the same.
++	 * Tx queues come first in the fq array.
+ 	 */
+ 	for (i = 0; i < dpaa2_eth_queue_count(priv); i++) {
+ 		priv->fq[priv->num_fqs].type = DPAA2_TX_CONF_FQ;
+@@ -2038,8 +2062,8 @@ static int setup_dpbp(struct dpaa2_eth_priv *priv)
+ {
+ 	int err;
+ 	struct fsl_mc_device *dpbp_dev;
+-	struct dpbp_attr dpbp_attrs;
+ 	struct device *dev = priv->net_dev->dev.parent;
++	struct dpbp_attr dpbp_attrs;
+ 
+ 	err = fsl_mc_object_allocate(to_fsl_mc_device(dev), FSL_MC_POOL_DPBP,
+ 				     &dpbp_dev);
+@@ -2075,9 +2099,10 @@ static int setup_dpbp(struct dpaa2_eth_priv *priv)
+ 		dev_err(dev, "dpbp_get_attributes() failed\n");
+ 		goto err_get_attr;
+ 	}
+-
+ 	priv->bpid = dpbp_attrs.bpid;
+-	priv->num_bufs = DPAA2_ETH_NUM_BUFS_FC / priv->num_channels;
++
++	/* By default we start with flow control enabled */
++	priv->max_bufs_per_ch = DPAA2_ETH_NUM_BUFS_FC / priv->num_channels;
+ 
+ 	return 0;
+ 
+@@ -2102,12 +2127,13 @@ static void free_dpbp(struct dpaa2_eth_priv *priv)
+ 
+ static int setup_tx_congestion(struct dpaa2_eth_priv *priv)
+ {
+-	struct dpni_congestion_notification_cfg cong_notif_cfg = { 0 };
++	struct dpni_congestion_notification_cfg notif_cfg = {0};
+ 	struct device *dev = priv->net_dev->dev.parent;
+ 	int err;
+ 
+ 	priv->cscn_unaligned = kzalloc(DPAA2_CSCN_SIZE + DPAA2_CSCN_ALIGN,
+ 				       GFP_KERNEL);
++
+ 	if (!priv->cscn_unaligned)
+ 		return -ENOMEM;
+ 
+@@ -2120,17 +2146,16 @@ static int setup_tx_congestion(struct dpaa2_eth_priv *priv)
+ 		goto err_dma_map;
+ 	}
+ 
+-	cong_notif_cfg.units = DPNI_CONGESTION_UNIT_BYTES;
+-	cong_notif_cfg.threshold_entry = DPAA2_ETH_TX_CONG_ENTRY_THRESH;
+-	cong_notif_cfg.threshold_exit = DPAA2_ETH_TX_CONG_EXIT_THRESH;
+-	cong_notif_cfg.message_ctx = (u64)priv;
+-	cong_notif_cfg.message_iova = priv->cscn_dma;
+-	cong_notif_cfg.notification_mode = DPNI_CONG_OPT_WRITE_MEM_ON_ENTER |
+-					   DPNI_CONG_OPT_WRITE_MEM_ON_EXIT |
+-					   DPNI_CONG_OPT_COHERENT_WRITE;
++	notif_cfg.units = DPNI_CONGESTION_UNIT_BYTES;
++	notif_cfg.threshold_entry = DPAA2_ETH_TX_CONG_ENTRY_THRESH;
++	notif_cfg.threshold_exit = DPAA2_ETH_TX_CONG_EXIT_THRESH;
++	notif_cfg.message_ctx = (u64)priv;
++	notif_cfg.message_iova = priv->cscn_dma;
++	notif_cfg.notification_mode = DPNI_CONG_OPT_WRITE_MEM_ON_ENTER |
++				      DPNI_CONG_OPT_WRITE_MEM_ON_EXIT |
++				      DPNI_CONG_OPT_COHERENT_WRITE;
+ 	err = dpni_set_congestion_notification(priv->mc_io, 0, priv->mc_token,
+-					       DPNI_QUEUE_TX, 0,
+-					       &cong_notif_cfg);
++					       DPNI_QUEUE_TX, 0, &notif_cfg);
+ 	if (err) {
+ 		dev_err(dev, "dpni_set_congestion_notification failed\n");
+ 		goto err_set_cong;
+@@ -2152,20 +2177,32 @@ static int setup_dpni(struct fsl_mc_device *ls_dev)
+ 	struct device *dev = &ls_dev->dev;
+ 	struct dpaa2_eth_priv *priv;
+ 	struct net_device *net_dev;
+-	struct dpni_buffer_layout buf_layout;
+ 	struct dpni_link_cfg cfg = {0};
+ 	int err;
+ 
+ 	net_dev = dev_get_drvdata(dev);
+ 	priv = netdev_priv(net_dev);
+ 
+-	priv->dpni_id = ls_dev->obj_desc.id;
+-
+ 	/* get a handle for the DPNI object */
+-	err = dpni_open(priv->mc_io, 0, priv->dpni_id, &priv->mc_token);
++	err = dpni_open(priv->mc_io, 0, ls_dev->obj_desc.id, &priv->mc_token);
+ 	if (err) {
+ 		dev_err(dev, "dpni_open() failed\n");
+-		goto err_open;
++		return err;
++	}
++
++	/* Check if we can work with this DPNI object */
++	err = dpni_get_api_version(priv->mc_io, 0, &priv->dpni_ver_major,
++				   &priv->dpni_ver_minor);
++	if (err) {
++		dev_err(dev, "dpni_get_api_version() failed\n");
++		goto close;
++	}
++	if (dpaa2_eth_cmp_dpni_ver(priv, DPNI_VER_MAJOR, DPNI_VER_MINOR) < 0) {
++		dev_err(dev, "DPNI version %u.%u not supported, need >= %u.%u\n",
++			priv->dpni_ver_major, priv->dpni_ver_minor,
++			DPNI_VER_MAJOR, DPNI_VER_MINOR);
++		err = -ENOTSUPP;
++		goto close;
+ 	}
+ 
+ 	ls_dev->mc_io = priv->mc_io;
+@@ -2174,130 +2211,47 @@ static int setup_dpni(struct fsl_mc_device *ls_dev)
+ 	err = dpni_reset(priv->mc_io, 0, priv->mc_token);
+ 	if (err) {
+ 		dev_err(dev, "dpni_reset() failed\n");
+-		goto err_reset;
++		goto close;
+ 	}
+ 
+ 	err = dpni_get_attributes(priv->mc_io, 0, priv->mc_token,
+ 				  &priv->dpni_attrs);
+-
+ 	if (err) {
+ 		dev_err(dev, "dpni_get_attributes() failed (err=%d)\n", err);
+-		goto err_get_attr;
+-	}
+-
+-	/* due to a limitation in WRIOP 1.0.0 (ERR009354), the Rx buf
+-	 * align value must be a multiple of 256.
+-	 */
+-	if (priv->dpni_attrs.wriop_version == DPAA2_WRIOP_VERSION(0, 0, 0) ||
+-	    priv->dpni_attrs.wriop_version == DPAA2_WRIOP_VERSION(1, 0, 0))
+-		priv->rx_buf_align = DPAA2_ETH_RX_BUF_ALIGN_V1;
+-	else
+-		priv->rx_buf_align = DPAA2_ETH_RX_BUF_ALIGN;
+-
+-	/* Update number of logical FQs in netdev */
+-	err = netif_set_real_num_tx_queues(net_dev,
+-					   dpaa2_eth_queue_count(priv));
+-	if (err) {
+-		dev_err(dev, "netif_set_real_num_tx_queues failed (%d)\n", err);
+-		goto err_set_tx_queues;
+-	}
+-
+-	err = netif_set_real_num_rx_queues(net_dev,
+-					   dpaa2_eth_queue_count(priv));
+-	if (err) {
+-		dev_err(dev, "netif_set_real_num_rx_queues failed (%d)\n", err);
+-		goto err_set_rx_queues;
+-	}
+-
+-	/* Configure buffer layouts */
+-	/* tx buffer */
+-	buf_layout.pass_timestamp = true;
+-	buf_layout.private_data_size = DPAA2_ETH_SWA_SIZE;
+-	buf_layout.options = DPNI_BUF_LAYOUT_OPT_TIMESTAMP |
+-			     DPNI_BUF_LAYOUT_OPT_PRIVATE_DATA_SIZE;
+-	err = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,
+-				     DPNI_QUEUE_TX, &buf_layout);
+-	if (err) {
+-		dev_err(dev,
+-			"dpni_set_buffer_layout(TX) failed\n");
+-		goto err_buf_layout;
+-	}
+-
+-	/* tx-confirm buffer */
+-	buf_layout.options = DPNI_BUF_LAYOUT_OPT_TIMESTAMP;
+-	err = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,
+-				     DPNI_QUEUE_TX_CONFIRM, &buf_layout);
+-	if (err) {
+-		dev_err(dev, "dpni_set_buffer_layout(TX_CONF) failed\n");
+-		goto err_buf_layout;
+-	}
+-
+-	/* Now that we've set our tx buffer layout, retrieve the minimum
+-	 * required tx data offset.
+-	 */
+-	err = dpni_get_tx_data_offset(priv->mc_io, 0, priv->mc_token,
+-				      &priv->tx_data_offset);
+-	if (err) {
+-		dev_err(dev, "dpni_get_tx_data_offset() failed\n");
+-		goto err_data_offset;
++		goto close;
+ 	}
+ 
+-	if ((priv->tx_data_offset % 64) != 0)
+-		dev_warn(dev, "Tx data offset (%d) not a multiple of 64B\n",
+-			 priv->tx_data_offset);
+-
+-	/* rx buffer */
+-	buf_layout.pass_frame_status = true;
+-	buf_layout.pass_parser_result = true;
+-	buf_layout.data_align = priv->rx_buf_align;
+-	buf_layout.private_data_size = 0;
+-	buf_layout.data_head_room = dpaa2_eth_rx_headroom(priv);
+-	buf_layout.options = DPNI_BUF_LAYOUT_OPT_PARSER_RESULT |
+-			     DPNI_BUF_LAYOUT_OPT_FRAME_STATUS |
+-			     DPNI_BUF_LAYOUT_OPT_DATA_ALIGN |
+-			     DPNI_BUF_LAYOUT_OPT_DATA_HEAD_ROOM |
+-			     DPNI_BUF_LAYOUT_OPT_TIMESTAMP;
+-	err = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,
+-				     DPNI_QUEUE_RX, &buf_layout);
+-	if (err) {
+-		dev_err(dev, "dpni_set_buffer_layout(RX) failed\n");
+-		goto err_buf_layout;
+-	}
++	err = set_buffer_layout(priv);
++	if (err)
++		goto close;
+ 
+ 	/* Enable congestion notifications for Tx queues */
+ 	err = setup_tx_congestion(priv);
+ 	if (err)
+-		goto err_tx_cong;
++		goto close;
+ 
+ 	/* allocate classification rule space */
+ 	priv->cls_rule = kzalloc(sizeof(*priv->cls_rule) *
+ 				 dpaa2_eth_fs_count(priv), GFP_KERNEL);
+ 	if (!priv->cls_rule)
+-		goto err_cls_rule;
++		goto close;
+ 
+ 	/* Enable flow control */
+ 	cfg.options = DPNI_LINK_OPT_AUTONEG | DPNI_LINK_OPT_PAUSE;
+-	priv->tx_pause_frames = 1;
+-
++	priv->tx_pause_frames = true;
+ 	err = dpni_set_link_cfg(priv->mc_io, 0, priv->mc_token, &cfg);
+ 	if (err) {
+-		netdev_err(net_dev, "ERROR %d setting link cfg", err);
+-		goto err_set_link_cfg;
++		dev_err(dev, "dpni_set_link_cfg() failed\n");
++		goto cls_free;
+ 	}
+ 
+ 	return 0;
+ 
+-err_set_link_cfg:
+-err_cls_rule:
+-err_tx_cong:
+-err_data_offset:
+-err_buf_layout:
+-err_set_rx_queues:
+-err_set_tx_queues:
+-err_get_attr:
+-err_reset:
++cls_free:
++	kfree(priv->cls_rule);
++close:
+ 	dpni_close(priv->mc_io, 0, priv->mc_token);
+-err_open:
++
+ 	return err;
+ }
+ 
+@@ -2319,12 +2273,44 @@ static void free_dpni(struct dpaa2_eth_priv *priv)
+ 	kfree(priv->cscn_unaligned);
+ }
+ 
+-static int set_queue_taildrop(struct dpaa2_eth_priv *priv,
+-			      struct dpni_taildrop *td)
++static int setup_rx_flow(struct dpaa2_eth_priv *priv,
++			 struct dpaa2_eth_fq *fq)
+ {
+ 	struct device *dev = priv->net_dev->dev.parent;
+-	int err, i;
++	struct dpni_queue queue;
++	struct dpni_queue_id qid;
++	int err;
+ 
++	err = dpni_get_queue(priv->mc_io, 0, priv->mc_token,
++			     DPNI_QUEUE_RX, fq->tc, fq->flowid, &queue, &qid);
++	if (err) {
++		dev_err(dev, "dpni_get_queue(RX) failed\n");
++		return err;
++	}
++
++	fq->fqid = qid.fqid;
++
++	queue.destination.id = fq->channel->dpcon_id;
++	queue.destination.type = DPNI_DEST_DPCON;
++	queue.destination.priority = 1;
++	queue.user_context = (u64)(uintptr_t)fq;
++	err = dpni_set_queue(priv->mc_io, 0, priv->mc_token,
++			     DPNI_QUEUE_RX, fq->tc, fq->flowid,
++			     DPNI_QUEUE_OPT_USER_CTX | DPNI_QUEUE_OPT_DEST,
++			     &queue);
++	if (err) {
++		dev_err(dev, "dpni_set_queue(RX) failed\n");
++		return err;
++	}
++
++	return 0;
++}
++
++static int set_queue_taildrop(struct dpaa2_eth_priv *priv,
++			      struct dpni_taildrop *td)
++{
++	struct device *dev = priv->net_dev->dev.parent;
++	int i, err;
+ 
+ 	for (i = 0; i < priv->num_fqs; i++) {
+ 		if (priv->fq[i].type != DPAA2_RX_FQ)
+@@ -2339,9 +2325,9 @@ static int set_queue_taildrop(struct dpaa2_eth_priv *priv,
+ 			return err;
+ 		}
+ 
+-		dev_dbg(dev, "%s taildrop for Rx queue id %d tc %d\n",
++		dev_dbg(dev, "%s taildrop for Rx group tc %d\n",
+ 			(td->enable ? "Enabled" : "Disabled"),
+-			priv->fq[i].flowid, priv->fq[i].tc);
++			i);
+ 	}
+ 
+ 	return 0;
+@@ -2373,9 +2359,9 @@ static int set_group_taildrop(struct dpaa2_eth_priv *priv,
+ 			return err;
+ 		}
+ 
+-		dev_dbg(dev, "%s taildrop for Rx group tc %d\n",
++		dev_dbg(dev, "%s taildrop for Rx queue id %d tc %d\n",
+ 			(tc_td->enable ? "Enabled" : "Disabled"),
+-			i);
++			priv->fq[i].flowid, priv->fq[i].tc);
+ 	}
+ 
+ 	return 0;
+@@ -2398,7 +2384,7 @@ int set_rx_taildrop(struct dpaa2_eth_priv *priv)
+ 	case DPAA2_ETH_TD_NONE:
+ 		memset(&td_queue, 0, sizeof(struct dpni_taildrop));
+ 		memset(&td_group, 0, sizeof(struct dpni_taildrop));
+-		priv->num_bufs = DPAA2_ETH_NUM_BUFS_FC /
++		priv->max_bufs_per_ch = DPAA2_ETH_NUM_BUFS_FC /
+ 					priv->num_channels;
+ 		break;
+ 	case DPAA2_ETH_TD_QUEUE:
+@@ -2407,7 +2393,7 @@ int set_rx_taildrop(struct dpaa2_eth_priv *priv)
+ 		td_queue.units = DPNI_CONGESTION_UNIT_BYTES;
+ 		td_queue.threshold = DPAA2_ETH_TAILDROP_THRESH /
+ 				     dpaa2_eth_tc_count(priv);
+-		priv->num_bufs = DPAA2_ETH_NUM_BUFS_TD;
++		priv->max_bufs_per_ch = DPAA2_ETH_NUM_BUFS_PER_CH;
+ 		break;
+ 	case DPAA2_ETH_TD_GROUP:
+ 		memset(&td_queue, 0, sizeof(struct dpni_taildrop));
+@@ -2415,7 +2401,7 @@ int set_rx_taildrop(struct dpaa2_eth_priv *priv)
+ 		td_group.units = DPNI_CONGESTION_UNIT_FRAMES;
+ 		td_group.threshold = NAPI_POLL_WEIGHT *
+ 				     dpaa2_eth_queue_count(priv);
+-		priv->num_bufs = NAPI_POLL_WEIGHT *
++		priv->max_bufs_per_ch = NAPI_POLL_WEIGHT *
+ 					dpaa2_eth_tc_count(priv);
+ 		break;
+ 	default:
+@@ -2430,39 +2416,7 @@ int set_rx_taildrop(struct dpaa2_eth_priv *priv)
+ 	if (err)
+ 		return err;
+ 
+-	priv->refill_thresh = priv->num_bufs - DPAA2_ETH_BUFS_PER_CMD;
+-
+-	return 0;
+-}
+-
+-static int setup_rx_flow(struct dpaa2_eth_priv *priv,
+-			 struct dpaa2_eth_fq *fq)
+-{
+-	struct device *dev = priv->net_dev->dev.parent;
+-	struct dpni_queue q = { { 0 } };
+-	struct dpni_queue_id qid;
+-	u8 q_opt = DPNI_QUEUE_OPT_USER_CTX | DPNI_QUEUE_OPT_DEST;
+-	int err;
+-
+-	err = dpni_get_queue(priv->mc_io, 0, priv->mc_token,
+-			     DPNI_QUEUE_RX, fq->tc, fq->flowid, &q, &qid);
+-	if (err) {
+-		dev_err(dev, "dpni_get_queue() failed (%d)\n", err);
+-		return err;
+-	}
+-
+-	fq->fqid = qid.fqid;
+-
+-	q.destination.id = fq->channel->dpcon_id;
+-	q.destination.type = DPNI_DEST_DPCON;
+-	q.destination.priority = 1;
+-	q.user_context = (u64)fq;
+-	err = dpni_set_queue(priv->mc_io, 0, priv->mc_token,
+-			     DPNI_QUEUE_RX, fq->tc, fq->flowid, q_opt, &q);
+-	if (err) {
+-		dev_err(dev, "dpni_set_queue() failed (%d)\n", err);
+-		return err;
+-	}
++	priv->refill_thresh = DPAA2_ETH_REFILL_THRESH(priv);
+ 
+ 	return 0;
+ }
+@@ -2471,37 +2425,39 @@ static int setup_tx_flow(struct dpaa2_eth_priv *priv,
+ 			 struct dpaa2_eth_fq *fq)
+ {
+ 	struct device *dev = priv->net_dev->dev.parent;
+-	struct dpni_queue q = { { 0 } };
++	struct dpni_queue queue;
+ 	struct dpni_queue_id qid;
+-	u8 q_opt = DPNI_QUEUE_OPT_USER_CTX | DPNI_QUEUE_OPT_DEST;
+ 	int err;
+ 
+ 	err = dpni_get_queue(priv->mc_io, 0, priv->mc_token,
+-			     DPNI_QUEUE_TX, 0, fq->flowid, &q, &qid);
++			     DPNI_QUEUE_TX, 0, fq->flowid, &queue, &qid);
+ 	if (err) {
+-		dev_err(dev, "dpni_get_queue() failed (%d)\n", err);
++		dev_err(dev, "dpni_get_queue(TX) failed\n");
+ 		return err;
+ 	}
+ 
+ 	fq->tx_qdbin = qid.qdbin;
+ 
+ 	err = dpni_get_queue(priv->mc_io, 0, priv->mc_token,
+-			     DPNI_QUEUE_TX_CONFIRM, 0, fq->flowid, &q, &qid);
++			     DPNI_QUEUE_TX_CONFIRM, 0, fq->flowid,
++			     &queue, &qid);
+ 	if (err) {
+-		dev_err(dev, "dpni_get_queue() failed (%d)\n", err);
++		dev_err(dev, "dpni_get_queue(TX_CONF) failed\n");
+ 		return err;
+ 	}
+ 
+ 	fq->fqid = qid.fqid;
+ 
+-	q.destination.id = fq->channel->dpcon_id;
+-	q.destination.type = DPNI_DEST_DPCON;
+-	q.destination.priority = 0;
+-	q.user_context = (u64)fq;
++	queue.destination.id = fq->channel->dpcon_id;
++	queue.destination.type = DPNI_DEST_DPCON;
++	queue.destination.priority = 0;
++	queue.user_context = (u64)(uintptr_t)fq;
+ 	err = dpni_set_queue(priv->mc_io, 0, priv->mc_token,
+-			     DPNI_QUEUE_TX_CONFIRM, 0, fq->flowid, q_opt, &q);
++			     DPNI_QUEUE_TX_CONFIRM, 0, fq->flowid,
++			     DPNI_QUEUE_OPT_USER_CTX | DPNI_QUEUE_OPT_DEST,
++			     &queue);
+ 	if (err) {
+-		dev_err(dev, "dpni_get_queue() failed (%d)\n", err);
++		dev_err(dev, "dpni_set_queue(TX_CONF) failed\n");
+ 		return err;
+ 	}
+ 
+@@ -2543,16 +2499,18 @@ static int setup_rx_err_flow(struct dpaa2_eth_priv *priv,
+ #endif
+ 
+ /* default hash key fields */
+-static struct dpaa2_eth_hash_fields default_hash_fields[] = {
++static struct dpaa2_eth_dist_fields default_dist_fields[] = {
+ 	{
+ 		/* L2 header */
+ 		.rxnfc_field = RXH_L2DA,
+ 		.cls_prot = NET_PROT_ETH,
+ 		.cls_field = NH_FLD_ETH_DA,
++		.id = DPAA2_ETH_DIST_ETHDST,
+ 		.size = 6,
+ 	}, {
+ 		.cls_prot = NET_PROT_ETH,
+ 		.cls_field = NH_FLD_ETH_SA,
++		.id = DPAA2_ETH_DIST_ETHSRC,
+ 		.size = 6,
+ 	}, {
+ 		/* This is the last ethertype field parsed:
+@@ -2561,28 +2519,33 @@ static struct dpaa2_eth_hash_fields default_hash_fields[] = {
+ 		 */
+ 		.cls_prot = NET_PROT_ETH,
+ 		.cls_field = NH_FLD_ETH_TYPE,
++		.id = DPAA2_ETH_DIST_ETHTYPE,
+ 		.size = 2,
+ 	}, {
+ 		/* VLAN header */
+ 		.rxnfc_field = RXH_VLAN,
+ 		.cls_prot = NET_PROT_VLAN,
+ 		.cls_field = NH_FLD_VLAN_TCI,
++		.id = DPAA2_ETH_DIST_VLAN,
+ 		.size = 2,
+ 	}, {
+ 		/* IP header */
+ 		.rxnfc_field = RXH_IP_SRC,
+ 		.cls_prot = NET_PROT_IP,
+ 		.cls_field = NH_FLD_IP_SRC,
++		.id = DPAA2_ETH_DIST_IPSRC,
+ 		.size = 4,
+ 	}, {
+ 		.rxnfc_field = RXH_IP_DST,
+ 		.cls_prot = NET_PROT_IP,
+ 		.cls_field = NH_FLD_IP_DST,
++		.id = DPAA2_ETH_DIST_IPDST,
+ 		.size = 4,
+ 	}, {
+ 		.rxnfc_field = RXH_L3_PROTO,
+ 		.cls_prot = NET_PROT_IP,
+ 		.cls_field = NH_FLD_IP_PROTO,
++		.id = DPAA2_ETH_DIST_IPPROTO,
+ 		.size = 1,
+ 	}, {
+ 		/* Using UDP ports, this is functionally equivalent to raw
+@@ -2591,85 +2554,182 @@ static struct dpaa2_eth_hash_fields default_hash_fields[] = {
+ 		.rxnfc_field = RXH_L4_B_0_1,
+ 		.cls_prot = NET_PROT_UDP,
+ 		.cls_field = NH_FLD_UDP_PORT_SRC,
++		.id = DPAA2_ETH_DIST_L4SRC,
+ 		.size = 2,
+ 	}, {
+ 		.rxnfc_field = RXH_L4_B_2_3,
+ 		.cls_prot = NET_PROT_UDP,
+ 		.cls_field = NH_FLD_UDP_PORT_DST,
++		.id = DPAA2_ETH_DIST_L4DST,
+ 		.size = 2,
+ 	},
+ };
+ 
+-/* Set RX hash options */
+-static int set_hash(struct dpaa2_eth_priv *priv)
++static int legacy_config_dist_key(struct dpaa2_eth_priv *priv,
++				  dma_addr_t key_iova)
+ {
+ 	struct device *dev = priv->net_dev->dev.parent;
+-	struct dpkg_profile_cfg cls_cfg;
+ 	struct dpni_rx_tc_dist_cfg dist_cfg;
+-	u8 *dma_mem;
+-	int i;
+-	int err = 0;
++	int i, err;
++
++	/* In legacy mode, we can't configure flow steering independently */
++	if (!dpaa2_eth_hash_enabled(priv))
++		return -EOPNOTSUPP;
++
++	memset(&dist_cfg, 0, sizeof(dist_cfg));
++
++	dist_cfg.key_cfg_iova = key_iova;
++	dist_cfg.dist_size = dpaa2_eth_queue_count(priv);
++	if (dpaa2_eth_fs_enabled(priv)) {
++		dist_cfg.dist_mode = DPNI_DIST_MODE_FS;
++		dist_cfg.fs_cfg.miss_action = DPNI_FS_MISS_HASH;
++	} else {
++		dist_cfg.dist_mode = DPNI_DIST_MODE_HASH;
++	}
++
++	for (i = 0; i < dpaa2_eth_tc_count(priv); i++) {
++		err = dpni_set_rx_tc_dist(priv->mc_io, 0, priv->mc_token, i,
++					  &dist_cfg);
++		if (err) {
++			dev_err(dev, "dpni_set_rx_tc_dist failed\n");
++			return err;
++		}
++	}
++
++	return 0;
++}
++
++static int config_hash_key(struct dpaa2_eth_priv *priv, dma_addr_t key_iova)
++{
++	struct device *dev = priv->net_dev->dev.parent;
++	struct dpni_rx_dist_cfg dist_cfg;
++	int i, err;
++
++	if (!dpaa2_eth_hash_enabled(priv))
++		return -EOPNOTSUPP;
++
++	memset(&dist_cfg, 0, sizeof(dist_cfg));
++
++	dist_cfg.key_cfg_iova = key_iova;
++	dist_cfg.dist_size = dpaa2_eth_queue_count(priv);
++	dist_cfg.enable = true;
++
++	for (i = 0; i < dpaa2_eth_tc_count(priv); i++) {
++		dist_cfg.tc = i;
++
++		err = dpni_set_rx_hash_dist(priv->mc_io, 0,
++					    priv->mc_token, &dist_cfg);
++		if (err) {
++			dev_err(dev, "dpni_set_rx_hash_dist failed\n");
++			return err;
++		}
++	}
++
++	return 0;
++}
++
++static int config_fs_key(struct dpaa2_eth_priv *priv, dma_addr_t key_iova)
++{
++	struct device *dev = priv->net_dev->dev.parent;
++	struct dpni_rx_dist_cfg dist_cfg;
++	int i, err;
++
++	if (!dpaa2_eth_fs_enabled(priv))
++		return -EOPNOTSUPP;
++
++	memset(&dist_cfg, 0, sizeof(dist_cfg));
++
++	dist_cfg.key_cfg_iova = key_iova;
++	dist_cfg.dist_size = dpaa2_eth_queue_count(priv);
++	dist_cfg.enable = true;
++
++	for (i = 0; i < dpaa2_eth_tc_count(priv); i++) {
++		dist_cfg.tc = i;
++
++		err = dpni_set_rx_fs_dist(priv->mc_io, 0,
++					  priv->mc_token, &dist_cfg);
++		if (err) {
++			dev_err(dev, "dpni_set_rx_fs_dist failed\n");
++			return err;
++		}
++	}
++
++	return 0;
++}
++
++int dpaa2_eth_set_dist_key(struct dpaa2_eth_priv *priv,
++			   enum dpaa2_eth_rx_dist type, u32 key_fields)
++{
++	struct device *dev = priv->net_dev->dev.parent;
++	struct dpkg_profile_cfg cls_cfg;
++	struct dpkg_extract *key;
++	u32 hash_fields = 0;
++	dma_addr_t key_iova;
++	u8 *key_mem;
++	int i, err;
+ 
+ 	memset(&cls_cfg, 0, sizeof(cls_cfg));
+ 
+-	for (i = 0; i < priv->num_hash_fields; i++) {
+-		struct dpkg_extract *key =
+-			&cls_cfg.extracts[cls_cfg.num_extracts];
++	for (i = 0; i < priv->num_dist_fields; i++) {
++		if (!(key_fields & priv->dist_fields[i].id))
++			continue;
+ 
++		key = &cls_cfg.extracts[cls_cfg.num_extracts];
+ 		key->type = DPKG_EXTRACT_FROM_HDR;
+-		key->extract.from_hdr.prot = priv->hash_fields[i].cls_prot;
++		key->extract.from_hdr.prot = priv->dist_fields[i].cls_prot;
+ 		key->extract.from_hdr.type = DPKG_FULL_FIELD;
+-		key->extract.from_hdr.field = priv->hash_fields[i].cls_field;
++		key->extract.from_hdr.field = priv->dist_fields[i].cls_field;
+ 		cls_cfg.num_extracts++;
+ 
+-		priv->rx_flow_hash |= priv->hash_fields[i].rxnfc_field;
++		hash_fields |= priv->dist_fields[i].rxnfc_field;
+ 	}
+ 
+-	dma_mem = kzalloc(DPAA2_CLASSIFIER_DMA_SIZE, GFP_DMA | GFP_KERNEL);
+-	if (!dma_mem)
++	key_mem = kzalloc(DPAA2_CLASSIFIER_DMA_SIZE, GFP_KERNEL);
++	if (!key_mem)
+ 		return -ENOMEM;
+ 
+-	err = dpni_prepare_key_cfg(&cls_cfg, dma_mem);
++	err = dpni_prepare_key_cfg(&cls_cfg, key_mem);
+ 	if (err) {
+-		dev_err(dev, "dpni_prepare_key_cfg() failed (%d)", err);
+-		goto err_prep_key;
++		dev_err(dev, "dpni_prepare_key_cfg error %d\n", err);
++		goto free_key;
+ 	}
+ 
+-	memset(&dist_cfg, 0, sizeof(dist_cfg));
+-
+-	/* Prepare for setting the rx dist */
+-	dist_cfg.key_cfg_iova = dma_map_single(dev, dma_mem,
+-					       DPAA2_CLASSIFIER_DMA_SIZE,
+-					       DMA_TO_DEVICE);
+-	if (dma_mapping_error(dev, dist_cfg.key_cfg_iova)) {
++	key_iova = dma_map_single(dev, key_mem, DPAA2_CLASSIFIER_DMA_SIZE,
++				  DMA_TO_DEVICE);
++	if (dma_mapping_error(dev, key_iova)) {
+ 		dev_err(dev, "DMA mapping failed\n");
+ 		err = -ENOMEM;
+-		goto err_dma_map;
++		goto free_key;
+ 	}
+ 
+-	dist_cfg.dist_size = dpaa2_eth_queue_count(priv);
+-	if (dpaa2_eth_fs_enabled(priv)) {
+-		dist_cfg.dist_mode = DPNI_DIST_MODE_FS;
+-		dist_cfg.fs_cfg.miss_action = DPNI_FS_MISS_HASH;
+-	} else {
+-		dist_cfg.dist_mode = DPNI_DIST_MODE_HASH;
++	switch (type) {
++	case DPAA2_ETH_RX_DIST_LEGACY:
++		err = legacy_config_dist_key(priv, key_iova);
++		break;
++	case DPAA2_ETH_RX_DIST_HASH:
++		err = config_hash_key(priv, key_iova);
++		break;
++	case DPAA2_ETH_RX_DIST_FS:
++		err = config_fs_key(priv, key_iova);
++		break;
++	default:
++		err = -EINVAL;
++		break;
+ 	}
+ 
+-	for (i = 0; i < dpaa2_eth_tc_count(priv); i++) {
+-		err = dpni_set_rx_tc_dist(priv->mc_io, 0, priv->mc_token, i,
+-					  &dist_cfg);
+-		if (err)
+-			break;
++	dma_unmap_single(dev, key_iova, DPAA2_CLASSIFIER_DMA_SIZE,
++			 DMA_TO_DEVICE);
++	if (err) {
++		if (err != -EOPNOTSUPP)
++			dev_err(dev, "Distribution key config failed\n");
++		goto free_key;
+ 	}
+ 
+-	dma_unmap_single(dev, dist_cfg.key_cfg_iova,
+-			 DPAA2_CLASSIFIER_DMA_SIZE, DMA_TO_DEVICE);
+-	if (err)
+-		dev_err(dev, "dpni_set_rx_tc_dist() failed (%d)\n", err);
++	if (type != DPAA2_ETH_RX_DIST_FS)
++		priv->rx_hash_fields = hash_fields;
+ 
+-err_dma_map:
+-err_prep_key:
+-	kfree(dma_mem);
++free_key:
++	kfree(key_mem);
+ 	return err;
+ }
+ 
+@@ -2699,19 +2759,23 @@ static int bind_dpni(struct dpaa2_eth_priv *priv)
+ 	/* Verify classification options and disable hashing and/or
+ 	 * flow steering support in case of invalid configuration values
+ 	 */
+-	priv->hash_fields = default_hash_fields;
+-	priv->num_hash_fields = ARRAY_SIZE(default_hash_fields);
++	priv->dist_fields = default_dist_fields;
++	priv->num_dist_fields = ARRAY_SIZE(default_dist_fields);
+ 	check_cls_support(priv);
+ 
+ 	/* have the interface implicitly distribute traffic based on
+-	 * a static hash key
++	 * a static hash key. Also configure flow steering key, if supported.
++	 * Errors here are not blocking, so just let the called function
++	 * print its error message and move along.
+ 	 */
+-	if (dpaa2_eth_hash_enabled(priv)) {
+-		err = set_hash(priv);
+-		if (err) {
+-			dev_err(dev, "Hashing configuration failed\n");
+-			return err;
+-		}
++	if (dpaa2_eth_has_legacy_dist(priv)) {
++		dpaa2_eth_set_dist_key(priv, DPAA2_ETH_RX_DIST_LEGACY,
++				       DPAA2_ETH_DIST_ALL);
++	} else {
++		dpaa2_eth_set_dist_key(priv, DPAA2_ETH_RX_DIST_HASH,
++				       DPAA2_ETH_DIST_DEFAULT_HASH);
++		dpaa2_eth_set_dist_key(priv, DPAA2_ETH_RX_DIST_FS,
++				       DPAA2_ETH_DIST_ALL);
+ 	}
+ 
+ 	/* Configure handling of error frames */
+@@ -2725,7 +2789,7 @@ static int bind_dpni(struct dpaa2_eth_priv *priv)
+ 	err = dpni_set_errors_behavior(priv->mc_io, 0, priv->mc_token,
+ 				       &err_cfg);
+ 	if (err) {
+-		dev_err(dev, "dpni_set_errors_behavior() failed (%d)\n", err);
++		dev_err(dev, "dpni_set_errors_behavior failed\n");
+ 		return err;
+ 	}
+ 
+@@ -2751,8 +2815,8 @@ static int bind_dpni(struct dpaa2_eth_priv *priv)
+ 			return err;
+ 	}
+ 
+-	err = dpni_get_qdid(priv->mc_io, 0, priv->mc_token, DPNI_QUEUE_TX,
+-			    &priv->tx_qdid);
++	err = dpni_get_qdid(priv->mc_io, 0, priv->mc_token,
++			    DPNI_QUEUE_TX, &priv->tx_qdid);
+ 	if (err) {
+ 		dev_err(dev, "dpni_get_qdid() failed\n");
+ 		return err;
+@@ -2797,67 +2861,61 @@ static void free_rings(struct dpaa2_eth_priv *priv)
+ 		dpaa2_io_store_destroy(priv->channel[i]->store);
+ }
+ 
+-static int netdev_init(struct net_device *net_dev)
++static int set_mac_addr(struct dpaa2_eth_priv *priv)
+ {
+-	int err;
++	struct net_device *net_dev = priv->net_dev;
+ 	struct device *dev = net_dev->dev.parent;
+-	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
+ 	u8 mac_addr[ETH_ALEN], dpni_mac_addr[ETH_ALEN];
+-	u8 bcast_addr[ETH_ALEN];
+-
+-	net_dev->netdev_ops = &dpaa2_eth_ops;
++	int err;
+ 
+ 	/* Get firmware address, if any */
+ 	err = dpni_get_port_mac_addr(priv->mc_io, 0, priv->mc_token, mac_addr);
+ 	if (err) {
+-		dev_err(dev, "dpni_get_port_mac_addr() failed (%d)\n", err);
++		dev_err(dev, "dpni_get_port_mac_addr() failed\n");
+ 		return err;
+ 	}
+ 
+-	/* Get DPNI atttributes address, if any */
++	/* Get DPNI attributes address, if any */
+ 	err = dpni_get_primary_mac_addr(priv->mc_io, 0, priv->mc_token,
+ 					dpni_mac_addr);
+ 	if (err) {
+-		dev_err(dev, "dpni_get_primary_mac_addr() failed (%d)\n", err);
++		dev_err(dev, "dpni_get_primary_mac_addr() failed\n");
+ 		return err;
+ 	}
+ 
+ 	/* First check if firmware has any address configured by bootloader */
+ 	if (!is_zero_ether_addr(mac_addr)) {
+-		/* If the DPMAC addr != the DPNI addr, update it */
++		/* If the DPMAC addr != DPNI addr, update it */
+ 		if (!ether_addr_equal(mac_addr, dpni_mac_addr)) {
+ 			err = dpni_set_primary_mac_addr(priv->mc_io, 0,
+ 							priv->mc_token,
+ 							mac_addr);
+ 			if (err) {
+-				dev_err(dev,
+-				"dpni_set_primary_mac_addr() failed (%d)\n",
+-				err);
++				dev_err(dev, "dpni_set_primary_mac_addr() failed\n");
+ 				return err;
+ 			}
+ 		}
+ 		memcpy(net_dev->dev_addr, mac_addr, net_dev->addr_len);
+ 	} else if (is_zero_ether_addr(dpni_mac_addr)) {
+-		/* Fills in net_dev->dev_addr, as required by
+-		 * register_netdevice()
++		/* No MAC address configured, fill in net_dev->dev_addr
++		 * with a random one
+ 		 */
+ 		eth_hw_addr_random(net_dev);
+-		/* Make the user aware, without cluttering the boot log */
+-		dev_dbg_once(dev, " device(s) have all-zero hwaddr, replaced with random\n");
+-		err = dpni_set_primary_mac_addr(priv->mc_io, 0,
+-					     priv->mc_token, net_dev->dev_addr);
++		dev_dbg_once(dev, "device(s) have all-zero hwaddr, replaced with random\n");
++
++		err = dpni_set_primary_mac_addr(priv->mc_io, 0, priv->mc_token,
++						net_dev->dev_addr);
+ 		if (err) {
+-			dev_err(dev,
+-			"dpni_set_primary_mac_addr() failed  (%d)\n", err);
++			dev_err(dev, "dpni_set_primary_mac_addr() failed\n");
+ 			return err;
+ 		}
++
+ 		/* Override NET_ADDR_RANDOM set by eth_hw_addr_random(); for all
+ 		 * practical purposes, this will be our "permanent" mac address,
+ 		 * at least until the next reboot. This move will also permit
+ 		 * register_netdevice() to properly fill up net_dev->perm_addr.
+ 		 */
+ 		net_dev->addr_assign_type = NET_ADDR_PERM;
+-	/* If DPMAC address is non-zero, use that one */
+ 	} else {
+ 		/* NET_ADDR_PERM is default, all we have to do is
+ 		 * fill in the device addr.
+@@ -2865,27 +2923,74 @@ static int netdev_init(struct net_device *net_dev)
+ 		memcpy(net_dev->dev_addr, dpni_mac_addr, net_dev->addr_len);
+ 	}
+ 
+-	/* Explicitly add the broadcast address to the MAC filtering table;
+-	 * the MC won't do that for us.
+-	 */
++	return 0;
++}
++
++static int netdev_init(struct net_device *net_dev)
++{
++	struct device *dev = net_dev->dev.parent;
++	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
++	u32 options = priv->dpni_attrs.options;
++	u64 supported = 0, not_supported = 0;
++	u8 bcast_addr[ETH_ALEN];
++	u8 num_queues;
++	int err;
++
++	net_dev->netdev_ops = &dpaa2_eth_ops;
++	net_dev->ethtool_ops = &dpaa2_ethtool_ops;
++
++	err = set_mac_addr(priv);
++	if (err)
++		return err;
++
++	/* Explicitly add the broadcast address to the MAC filtering table */
+ 	eth_broadcast_addr(bcast_addr);
+ 	err = dpni_add_mac_addr(priv->mc_io, 0, priv->mc_token, bcast_addr);
+ 	if (err) {
+-		dev_warn(dev, "dpni_add_mac_addr() failed (%d)\n", err);
+-		/* Won't return an error; at least, we'd have egress traffic */
++		dev_err(dev, "dpni_add_mac_addr() failed\n");
++		return err;
+ 	}
+ 
+-	/* Set MTU limits */
+-	net_dev->min_mtu = 68;
++	/* Set MTU upper limit; lower limit is default (68B) */
+ 	net_dev->max_mtu = DPAA2_ETH_MAX_MTU;
++	err = dpni_set_max_frame_length(priv->mc_io, 0, priv->mc_token,
++					(u16)DPAA2_ETH_MFL);
++	if (err) {
++		dev_err(dev, "dpni_set_max_frame_length() failed\n");
++		return err;
++	}
+ 
+-	/* Our .ndo_init will be called herein */
+-	err = register_netdev(net_dev);
+-	if (err < 0) {
+-		dev_err(dev, "register_netdev() failed (%d)\n", err);
++	/* Set actual number of queues in the net device */
++	num_queues = dpaa2_eth_queue_count(priv);
++	err = netif_set_real_num_tx_queues(net_dev, num_queues);
++	if (err) {
++		dev_err(dev, "netif_set_real_num_tx_queues() failed\n");
++		return err;
++	}
++	err = netif_set_real_num_rx_queues(net_dev, num_queues);
++	if (err) {
++		dev_err(dev, "netif_set_real_num_rx_queues() failed\n");
+ 		return err;
+ 	}
+ 
++	/* Capabilities listing */
++	supported |= IFF_LIVE_ADDR_CHANGE;
++
++	if (options & DPNI_OPT_NO_MAC_FILTER)
++		not_supported |= IFF_UNICAST_FLT;
++	else
++		supported |= IFF_UNICAST_FLT;
++
++	net_dev->priv_flags |= supported;
++	net_dev->priv_flags &= ~not_supported;
++
++	/* Features */
++	net_dev->features = NETIF_F_RXCSUM |
++			    NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
++			    NETIF_F_SG | NETIF_F_HIGHDMA |
++			    NETIF_F_LLTX;
++	net_dev->hw_features = net_dev->features;
++
+ 	return 0;
+ }
+ 
+@@ -2905,14 +3010,9 @@ static int poll_link_state(void *arg)
+ 	return 0;
+ }
+ 
+-static irqreturn_t dpni_irq0_handler(int irq_num, void *arg)
+-{
+-	return IRQ_WAKE_THREAD;
+-}
+-
+ static irqreturn_t dpni_irq0_handler_thread(int irq_num, void *arg)
+ {
+-	u32 status = 0, clear = 0;
++	u32 status = ~0;
+ 	struct device *dev = (struct device *)arg;
+ 	struct fsl_mc_device *dpni_dev = to_fsl_mc_device(dev);
+ 	struct net_device *net_dev = dev_get_drvdata(dev);
+@@ -2921,19 +3021,13 @@ static irqreturn_t dpni_irq0_handler_thread(int irq_num, void *arg)
+ 	err = dpni_get_irq_status(dpni_dev->mc_io, 0, dpni_dev->mc_handle,
+ 				  DPNI_IRQ_INDEX, &status);
+ 	if (unlikely(err)) {
+-		netdev_err(net_dev, "Can't get irq status (err %d)", err);
+-		clear = 0xffffffff;
+-		goto out;
++		netdev_err(net_dev, "Can't get irq status (err %d)\n", err);
++		return IRQ_HANDLED;
+ 	}
+ 
+-	if (status & DPNI_IRQ_EVENT_LINK_CHANGED) {
+-		clear |= DPNI_IRQ_EVENT_LINK_CHANGED;
++	if (status & DPNI_IRQ_EVENT_LINK_CHANGED)
+ 		link_state_update(netdev_priv(net_dev));
+-	}
+ 
+-out:
+-	dpni_clear_irq_status(dpni_dev->mc_io, 0, dpni_dev->mc_handle,
+-			      DPNI_IRQ_INDEX, clear);
+ 	return IRQ_HANDLED;
+ }
+ 
+@@ -2950,26 +3044,25 @@ static int setup_irqs(struct fsl_mc_device *ls_dev)
+ 
+ 	irq = ls_dev->irqs[0];
+ 	err = devm_request_threaded_irq(&ls_dev->dev, irq->msi_desc->irq,
+-					dpni_irq0_handler,
+-					dpni_irq0_handler_thread,
++					NULL, dpni_irq0_handler_thread,
+ 					IRQF_NO_SUSPEND | IRQF_ONESHOT,
+ 					dev_name(&ls_dev->dev), &ls_dev->dev);
+ 	if (err < 0) {
+-		dev_err(&ls_dev->dev, "devm_request_threaded_irq(): %d", err);
++		dev_err(&ls_dev->dev, "devm_request_threaded_irq(): %d\n", err);
+ 		goto free_mc_irq;
+ 	}
+ 
+ 	err = dpni_set_irq_mask(ls_dev->mc_io, 0, ls_dev->mc_handle,
+ 				DPNI_IRQ_INDEX, DPNI_IRQ_EVENT_LINK_CHANGED);
+ 	if (err < 0) {
+-		dev_err(&ls_dev->dev, "dpni_set_irq_mask(): %d", err);
++		dev_err(&ls_dev->dev, "dpni_set_irq_mask(): %d\n", err);
+ 		goto free_irq;
+ 	}
+ 
+ 	err = dpni_set_irq_enable(ls_dev->mc_io, 0, ls_dev->mc_handle,
+ 				  DPNI_IRQ_INDEX, 1);
+ 	if (err < 0) {
+-		dev_err(&ls_dev->dev, "dpni_set_irq_enable(): %d", err);
++		dev_err(&ls_dev->dev, "dpni_set_irq_enable(): %d\n", err);
+ 		goto free_irq;
+ 	}
+ 
+@@ -3026,7 +3119,7 @@ static ssize_t dpaa2_eth_write_tx_shaping(struct device *dev,
+ {
+ 	int err, items;
+ 	struct dpaa2_eth_priv *priv = netdev_priv(to_net_dev(dev));
+-	struct dpni_tx_shaping_cfg scfg;
++	struct dpni_tx_shaping_cfg scfg, ercfg = { 0 };
+ 
+ 	items = sscanf(buf, "%u %hu", &scfg.rate_limit, &scfg.max_burst_size);
+ 	if (items != 2) {
+@@ -3040,7 +3133,8 @@ static ssize_t dpaa2_eth_write_tx_shaping(struct device *dev,
+ 		return -EINVAL;
+ 	}
+ 
+-	err = dpni_set_tx_shaping(priv->mc_io, 0, priv->mc_token, &scfg);
++	err = dpni_set_tx_shaping(priv->mc_io, 0, priv->mc_token, &scfg,
++				  &ercfg, 0);
+ 	if (err) {
+ 		dev_err(dev, "dpni_set_tx_shaping() failed\n");
+ 		return -EPERM;
+@@ -3051,79 +3145,7 @@ static ssize_t dpaa2_eth_write_tx_shaping(struct device *dev,
+ 	return count;
+ }
+ 
+-static ssize_t dpaa2_eth_show_txconf_cpumask(struct device *dev,
+-					     struct device_attribute *attr,
+-					     char *buf)
+-{
+-	struct dpaa2_eth_priv *priv = netdev_priv(to_net_dev(dev));
+-
+-	return cpumap_print_to_pagebuf(1, buf, &priv->txconf_cpumask);
+-}
+-
+-static ssize_t dpaa2_eth_write_txconf_cpumask(struct device *dev,
+-					      struct device_attribute *attr,
+-					      const char *buf,
+-					      size_t count)
+-{
+-	struct dpaa2_eth_priv *priv = netdev_priv(to_net_dev(dev));
+-	struct dpaa2_eth_fq *fq;
+-	bool running = netif_running(priv->net_dev);
+-	int i, err;
+-
+-	err = cpulist_parse(buf, &priv->txconf_cpumask);
+-	if (err)
+-		return err;
+-
+-	/* Only accept CPUs that have an affine DPIO */
+-	if (!cpumask_subset(&priv->txconf_cpumask, &priv->dpio_cpumask)) {
+-		netdev_info(priv->net_dev,
+-			    "cpumask must be a subset of 0x%lx\n",
+-			    *cpumask_bits(&priv->dpio_cpumask));
+-		cpumask_and(&priv->txconf_cpumask, &priv->dpio_cpumask,
+-			    &priv->txconf_cpumask);
+-	}
+-
+-	/* Rewiring the TxConf FQs requires interface shutdown.
+-	 */
+-	if (running) {
+-		err = dpaa2_eth_stop(priv->net_dev);
+-		if (err)
+-			return -ENODEV;
+-	}
+-
+-	/* Set the new TxConf FQ affinities */
+-	set_fq_affinity(priv);
+-
+-	/* dpaa2_eth_open() below will *stop* the Tx queues until an explicit
+-	 * link up notification is received. Give the polling thread enough time
+-	 * to detect the link state change, or else we'll end up with the
+-	 * transmission side forever shut down.
+-	 */
+-	if (priv->do_link_poll)
+-		msleep(2 * DPAA2_ETH_LINK_STATE_REFRESH);
+-
+-	for (i = 0; i < priv->num_fqs; i++) {
+-		fq = &priv->fq[i];
+-		if (fq->type != DPAA2_TX_CONF_FQ)
+-			continue;
+-		setup_tx_flow(priv, fq);
+-	}
+-
+-	if (running) {
+-		err = dpaa2_eth_open(priv->net_dev);
+-		if (err)
+-			return -ENODEV;
+-	}
+-
+-	return count;
+-}
+-
+ static struct device_attribute dpaa2_eth_attrs[] = {
+-	__ATTR(txconf_cpumask,
+-	       0600,
+-	       dpaa2_eth_show_txconf_cpumask,
+-	       dpaa2_eth_write_txconf_cpumask),
+-
+ 	__ATTR(tx_shaping,
+ 	       0600,
+ 	       dpaa2_eth_show_tx_shaping,
+@@ -3493,8 +3515,10 @@ static int dpaa2_eth_probe(struct fsl_mc_device *dpni_dev)
+ 	err = fsl_mc_portal_allocate(dpni_dev, FSL_MC_IO_ATOMIC_CONTEXT_PORTAL,
+ 				     &priv->mc_io);
+ 	if (err) {
+-		dev_dbg(dev, "MC portal allocation failed\n");
+-		err = -EPROBE_DEFER;
++		if (err == -ENXIO)
++			err = -EPROBE_DEFER;
++		else
++			dev_err(dev, "MC portal allocation failed\n");
+ 		goto err_portal_alloc;
+ 	}
+ 
+@@ -3504,11 +3528,8 @@ static int dpaa2_eth_probe(struct fsl_mc_device *dpni_dev)
+ 		goto err_dpni_setup;
+ 
+ 	err = setup_dpio(priv);
+-	if (err) {
+-		dev_info(dev, "Defer probing as no DPIO available\n");
+-		err = -EPROBE_DEFER;
++	if (err)
+ 		goto err_dpio_setup;
+-	}
+ 
+ 	setup_fqs(priv);
+ 
+@@ -3552,7 +3573,6 @@ static int dpaa2_eth_probe(struct fsl_mc_device *dpni_dev)
+ 	if (err)
+ 		goto err_alloc_rings;
+ 
+-	net_dev->ethtool_ops = &dpaa2_ethtool_ops;
+ #ifdef CONFIG_FSL_DPAA2_ETH_DCB
+ 	net_dev->dcbnl_ops = &dpaa2_eth_dcbnl_ops;
+ 	priv->dcbx_mode = DCB_CAP_DCBX_HOST | DCB_CAP_DCBX_VER_IEEE;
+@@ -3568,12 +3588,18 @@ static int dpaa2_eth_probe(struct fsl_mc_device *dpni_dev)
+ 		priv->poll_thread = kthread_run(poll_link_state, priv,
+ 						"%s_poll_link", net_dev->name);
+ 		if (IS_ERR(priv->poll_thread)) {
+-			netdev_err(net_dev, "Error starting polling thread\n");
++			dev_err(dev, "Error starting polling thread\n");
+ 			goto err_poll_thread;
+ 		}
+ 		priv->do_link_poll = true;
+ 	}
+ 
++	err = register_netdev(net_dev);
++	if (err < 0) {
++		dev_err(dev, "register_netdev() failed\n");
++		goto err_netdev_reg;
++	}
++
+ 	dpaa2_eth_sysfs_init(&net_dev->dev);
+ #ifdef CONFIG_FSL_DPAA2_ETH_DEBUGFS
+ 	dpaa2_dbg_add(priv);
+@@ -3582,11 +3608,15 @@ static int dpaa2_eth_probe(struct fsl_mc_device *dpni_dev)
+ 	dev_info(dev, "Probed interface %s\n", net_dev->name);
+ 	return 0;
+ 
++err_netdev_reg:
++	if (priv->do_link_poll)
++		kthread_stop(priv->poll_thread);
++	else
++		fsl_mc_free_irqs(dpni_dev);
+ err_poll_thread:
+ 	free_rings(priv);
+ err_alloc_rings:
+ err_csum:
+-	unregister_netdev(net_dev);
+ err_netdev_init:
+ 	free_percpu(priv->percpu_extras);
+ err_alloc_percpu_extras:
+@@ -3624,12 +3654,11 @@ static int dpaa2_eth_remove(struct fsl_mc_device *ls_dev)
+ #endif
+ 	dpaa2_eth_sysfs_remove(&net_dev->dev);
+ 
++	unregister_netdev(net_dev);
++
+ 	disable_ch_napi(priv);
+ 	del_ch_napi(priv);
+ 
+-	unregister_netdev(net_dev);
+-	dev_info(net_dev->dev.parent, "Removed interface %s\n", net_dev->name);
+-
+ 	if (priv->do_link_poll)
+ 		kthread_stop(priv->poll_thread);
+ 	else
+@@ -3647,6 +3676,8 @@ static int dpaa2_eth_remove(struct fsl_mc_device *ls_dev)
+ 	dev_set_drvdata(dev, NULL);
+ 	free_netdev(net_dev);
+ 
++	dev_dbg(net_dev->dev.parent, "Removed interface %s\n", net_dev->name);
++
+ 	return 0;
+ }
+ 
+@@ -3675,18 +3706,27 @@ static int __init dpaa2_eth_driver_init(void)
+ 
+ 	dpaa2_eth_dbg_init();
+ 	err = fsl_mc_driver_register(&dpaa2_eth_driver);
+-	if (err) {
+-		dpaa2_eth_dbg_exit();
+-		return err;
+-	}
++	if (err)
++		goto out_debugfs_err;
++
++	err = dpaa2_ceetm_register();
++	if (err)
++		goto out_ceetm_err;
+ 
+ 	return 0;
++
++out_ceetm_err:
++	fsl_mc_driver_unregister(&dpaa2_eth_driver);
++out_debugfs_err:
++	dpaa2_eth_dbg_exit();
++	return err;
+ }
+ 
+ static void __exit dpaa2_eth_driver_exit(void)
+ {
+-	dpaa2_eth_dbg_exit();
++	dpaa2_ceetm_unregister();
+ 	fsl_mc_driver_unregister(&dpaa2_eth_driver);
++	dpaa2_eth_dbg_exit();
+ }
+ 
+ module_init(dpaa2_eth_driver_init);
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
+index 107dfcde..34c8e5b1 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
+@@ -1,4 +1,5 @@
+-/* Copyright 2014-2015 Freescale Semiconductor Inc.
++/* Copyright 2014-2016 Freescale Semiconductor Inc.
++ * Copyright 2016 NXP
+  *
+  * Redistribution and use in source and binary forms, with or without
+  * modification, are permitted provided that the following conditions are met:
+@@ -32,14 +33,16 @@
+ #ifndef __DPAA2_ETH_H
+ #define __DPAA2_ETH_H
+ 
+-#include <linux/atomic.h>
+ #include <linux/dcbnl.h>
+ #include <linux/netdevice.h>
+ #include <linux/if_vlan.h>
++
+ #include "../../fsl-mc/include/dpaa2-io.h"
++#include "../../fsl-mc/include/dpaa2-fd.h"
+ #include "dpni.h"
+-#include "net.h"
++#include "dpni-cmd.h"
+ 
++#include "dpaa2-eth-trace.h"
+ #include "dpaa2-eth-debugfs.h"
+ 
+ #define DPAA2_WRIOP_VERSION(x, y, z) ((x) << 10 | (y) << 5 | (z) << 0)
+@@ -84,29 +87,33 @@
+  * to accommodate the buffer refill delay.
+  */
+ #define DPAA2_ETH_MAX_FRAMES_PER_QUEUE	(DPAA2_ETH_TAILDROP_THRESH / 64)
+-#define DPAA2_ETH_NUM_BUFS_TD		(DPAA2_ETH_MAX_FRAMES_PER_QUEUE + 256)
+-#define DPAA2_ETH_REFILL_THRESH_TD	\
+-	(DPAA2_ETH_NUM_BUFS_TD - DPAA2_ETH_BUFS_PER_CMD)
++#define DPAA2_ETH_NUM_BUFS_PER_CH	(DPAA2_ETH_MAX_FRAMES_PER_QUEUE + 256)
++#define DPAA2_ETH_REFILL_THRESH(priv)	\
++	((priv)->max_bufs_per_ch - DPAA2_ETH_BUFS_PER_CMD)
+ 
+-/* Buffer quota per queue to use when flow control is active. */
++/* Global buffer quota in case flow control is enabled */
+ #define DPAA2_ETH_NUM_BUFS_FC		256
+ 
+-/* Hardware requires alignment for ingress/egress buffer addresses
+- * and ingress buffer lengths.
+- */
+-#define DPAA2_ETH_RX_BUF_SIZE		2048
++/* Hardware requires alignment for ingress/egress buffer addresses */
+ #define DPAA2_ETH_TX_BUF_ALIGN		64
++
++/* Due to a limitation in WRIOP 1.0.0, the RX buffer data must be aligned
++ * to 256B. For newer revisions, the requirement is only for 64B alignment
++ */
++#define DPAA2_ETH_RX_BUF_ALIGN_REV1	256
+ #define DPAA2_ETH_RX_BUF_ALIGN		64
+-#define DPAA2_ETH_RX_BUF_ALIGN_V1	256
+ 
+-/* rx_extra_head prevents reallocations in L3 processing. */
++#define DPAA2_ETH_RX_BUF_SIZE		2048
+ #define DPAA2_ETH_SKB_SIZE \
+-	(DPAA2_ETH_RX_BUF_SIZE + \
+-	 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
++	(DPAA2_ETH_RX_BUF_SIZE + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
+ 
+ /* PTP nominal frequency 1GHz */
+ #define DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS 1
+ 
++/* Hardware annotation area in RX/TX buffers */
++#define DPAA2_ETH_RX_HWA_SIZE		64
++#define DPAA2_ETH_TX_HWA_SIZE		128
++
+ /* We are accommodating a skb backpointer and some S/G info
+  * in the frame's software annotation. The hardware
+  * options are either 0 or 64, so we choose the latter.
+@@ -132,7 +139,7 @@ struct dpaa2_eth_swa {
+ 			struct sk_buff *skb;
+ 			struct scatterlist *scl;
+ 			int num_sg;
+-			int num_dma_bufs;
++			int sgt_size;
+ 		} sg;
+ 	};
+ };
+@@ -154,20 +161,16 @@ struct dpaa2_eth_swa {
+ /* Annotation bits in FD CTRL */
+ #define DPAA2_FD_CTRL_ASAL		0x00020000	/* ASAL = 128 */
+ 
+-/* Hardware annotation area in RX/TX buffers */
+-#define DPAA2_ETH_RX_HWA_SIZE		64
+-#define DPAA2_ETH_TX_HWA_SIZE		128
+-
+ /* Frame annotation status */
+ struct dpaa2_fas {
+ 	u8 reserved;
+ 	u8 ppid;
+ 	__le16 ifpid;
+ 	__le32 status;
+-} __packed;
++};
+ 
+ /* Frame annotation status word is located in the first 8 bytes
+- * of the buffer's hardware annotation area
++ * of the buffer's hardware annoatation area
+  */
+ #define DPAA2_FAS_OFFSET		0
+ #define DPAA2_FAS_SIZE			(sizeof(struct dpaa2_fas))
+@@ -191,25 +194,25 @@ struct dpaa2_faead {
+ #define DPAA2_FAEAD_EBDDV		0x00002000
+ #define DPAA2_FAEAD_UPD			0x00000010
+ 
+-/* accessors for the hardware annotation fields that we use */
+-static inline void *dpaa2_eth_get_hwa(void *buf_addr, bool swa)
++/* Accessors for the hardware annotation fields that we use */
++static inline void *dpaa2_get_hwa(void *buf_addr, bool swa)
+ {
+ 	return buf_addr + (swa ? DPAA2_ETH_SWA_SIZE : 0);
+ }
+ 
+-static inline struct dpaa2_fas *dpaa2_eth_get_fas(void *buf_addr, bool swa)
++static inline struct dpaa2_fas *dpaa2_get_fas(void *buf_addr, bool swa)
+ {
+-	return dpaa2_eth_get_hwa(buf_addr, swa) + DPAA2_FAS_OFFSET;
++	return dpaa2_get_hwa(buf_addr, swa) + DPAA2_FAS_OFFSET;
+ }
+ 
+-static inline u64 *dpaa2_eth_get_ts(void *buf_addr, bool swa)
++static inline u64 *dpaa2_get_ts(void *buf_addr, bool swa)
+ {
+-	return dpaa2_eth_get_hwa(buf_addr, swa) + DPAA2_TS_OFFSET;
++	return dpaa2_get_hwa(buf_addr, swa) + DPAA2_TS_OFFSET;
+ }
+ 
+-static inline struct dpaa2_faead *dpaa2_eth_get_faead(void *buf_addr, bool swa)
++static inline struct dpaa2_faead *dpaa2_get_faead(void *buf_addr, bool swa)
+ {
+-	return dpaa2_eth_get_hwa(buf_addr, swa) + DPAA2_FAEAD_OFFSET;
++	return dpaa2_get_hwa(buf_addr, swa) + DPAA2_FAEAD_OFFSET;
+ }
+ 
+ /* Error and status bits in the frame annotation status word */
+@@ -244,19 +247,19 @@ static inline struct dpaa2_faead *dpaa2_eth_get_faead(void *buf_addr, bool swa)
+ /* L4 csum error */
+ #define DPAA2_FAS_L4CE			0x00000001
+ /* Possible errors on the ingress path */
+-#define DPAA2_FAS_RX_ERR_MASK		((DPAA2_FAS_KSE)	| \
+-					 (DPAA2_FAS_EOFHE)	| \
+-					 (DPAA2_FAS_MNLE)	| \
+-					 (DPAA2_FAS_TIDE)	| \
+-					 (DPAA2_FAS_PIEE)	| \
+-					 (DPAA2_FAS_FLE)	| \
+-					 (DPAA2_FAS_FPE)	| \
+-					 (DPAA2_FAS_PTE)	| \
+-					 (DPAA2_FAS_ISP)	| \
+-					 (DPAA2_FAS_PHE)	| \
+-					 (DPAA2_FAS_BLE)	| \
+-					 (DPAA2_FAS_L3CE)	| \
+-					 (DPAA2_FAS_L4CE))
++#define DPAA2_FAS_RX_ERR_MASK		(DPAA2_FAS_KSE		| \
++					 DPAA2_FAS_EOFHE	| \
++					 DPAA2_FAS_MNLE		| \
++					 DPAA2_FAS_TIDE		| \
++					 DPAA2_FAS_PIEE		| \
++					 DPAA2_FAS_FLE		| \
++					 DPAA2_FAS_FPE		| \
++					 DPAA2_FAS_PTE		| \
++					 DPAA2_FAS_ISP		| \
++					 DPAA2_FAS_PHE		| \
++					 DPAA2_FAS_BLE		| \
++					 DPAA2_FAS_L3CE		| \
++					 DPAA2_FAS_L4CE)
+ 
+ /* Time in milliseconds between link state updates */
+ #define DPAA2_ETH_LINK_STATE_REFRESH	1000
+@@ -265,14 +268,15 @@ static inline struct dpaa2_faead *dpaa2_eth_get_faead(void *buf_addr, bool swa)
+  * Value determined empirically, in order to minimize the number
+  * of frames dropped on Tx
+  */
+-#define DPAA2_ETH_ENQUEUE_RETRIES      10
++#define DPAA2_ETH_ENQUEUE_RETRIES	10
+ 
+ /* Tx congestion entry & exit thresholds, in number of bytes.
+  * We allow a maximum of 512KB worth of frames pending processing on the Tx
+  * queues of an interface
+  */
+-#define DPAA2_ETH_TX_CONG_ENTRY_THRESH	(512 * 1024)
+-#define DPAA2_ETH_TX_CONG_EXIT_THRESH	(DPAA2_ETH_TX_CONG_ENTRY_THRESH * 9/10)
++#define DPAA2_ETH_TX_CONG_ENTRY_THRESH  (512 * 1024)
++#define DPAA2_ETH_TX_CONG_EXIT_THRESH	\
++	(DPAA2_ETH_TX_CONG_ENTRY_THRESH * 9 / 10)
+ 
+ /* Driver statistics, other than those in struct rtnl_link_stats64.
+  * These are usually collected per-CPU and aggregated by ethtool.
+@@ -309,7 +313,6 @@ struct dpaa2_eth_ch_stats {
+ 	__u64 pull_err;
+ };
+ 
+-#define DPAA2_ETH_MAX_DPCONS		NR_CPUS
+ #define DPAA2_ETH_MAX_TCS		8
+ 
+ /* Maximum number of queues associated with a DPNI */
+@@ -320,6 +323,8 @@ struct dpaa2_eth_ch_stats {
+ 					DPAA2_ETH_MAX_TX_QUEUES + \
+ 					DPAA2_ETH_MAX_RX_ERR_QUEUES)
+ 
++#define DPAA2_ETH_MAX_DPCONS		16
++
+ enum dpaa2_eth_fq_type {
+ 	DPAA2_RX_FQ = 0,
+ 	DPAA2_TX_CONF_FQ,
+@@ -350,8 +355,8 @@ struct dpaa2_eth_channel {
+ 	struct fsl_mc_device *dpcon;
+ 	int dpcon_id;
+ 	int ch_id;
+-	int dpio_id;
+ 	struct napi_struct napi;
++	struct dpaa2_io *dpio;
+ 	struct dpaa2_io_store *store;
+ 	struct dpaa2_eth_priv *priv;
+ 	int buf_count;
+@@ -366,61 +371,51 @@ struct dpaa2_eth_cls_rule {
+ 	bool in_use;
+ };
+ 
+-struct dpaa2_eth_hash_fields {
++struct dpaa2_eth_dist_fields {
+ 	u64 rxnfc_field;
+ 	enum net_prot cls_prot;
+ 	int cls_field;
+ 	int offset;
+ 	int size;
++	u32 id;
+ };
+ 
+ /* Driver private data */
+ struct dpaa2_eth_priv {
+ 	struct net_device *net_dev;
+-
+ 	/* Standard statistics */
+ 	struct rtnl_link_stats64 __percpu *percpu_stats;
+ 	/* Extra stats, in addition to the ones known by the kernel */
+ 	struct dpaa2_eth_drv_stats __percpu *percpu_extras;
+-	struct iommu_domain *iommu_domain;
+-
+ 	bool ts_tx_en; /* Tx timestamping enabled */
+ 	bool ts_rx_en; /* Rx timestamping enabled */
+-
+ 	u16 tx_data_offset;
+-	u16 rx_buf_align;
+-
+ 	u16 bpid;
+ 	u16 tx_qdid;
+-
+-	int tx_pause_frames;
+-	int num_bufs;
++	u16 rx_buf_align;
++	struct iommu_domain *iommu_domain;
++	int max_bufs_per_ch;
+ 	int refill_thresh;
+ 	bool has_xdp_prog;
+ 
+-	/* Tx congestion notifications are written here */
+-	void *cscn_mem;
++	void *cscn_mem;	/* Tx congestion notifications are written here */
+ 	void *cscn_unaligned;
+ 	dma_addr_t cscn_dma;
+ 
+ 	u8 num_fqs;
+-	/* Tx queues are at the beginning of the array */
+ 	struct dpaa2_eth_fq fq[DPAA2_ETH_MAX_QUEUES];
+ 
+ 	u8 num_channels;
+ 	struct dpaa2_eth_channel *channel[DPAA2_ETH_MAX_DPCONS];
+ 
+-	int dpni_id;
+ 	struct dpni_attr dpni_attrs;
++	u16 dpni_ver_major;
++	u16 dpni_ver_minor;
+ 	struct fsl_mc_device *dpbp_dev;
+ 
+ 	struct fsl_mc_io *mc_io;
+-	/* SysFS-controlled affinity mask for TxConf FQs */
+-	struct cpumask txconf_cpumask;
+ 	/* Cores which have an affine DPIO/DPCON.
+-	 * This is the cpu set on which Rx frames are processed;
+-	 * Tx confirmation frames are processed on a subset of this,
+-	 * depending on user settings.
++	 * This is the cpu set on which Rx and Tx conf frames are processed
+ 	 */
+ 	struct cpumask dpio_cpumask;
+ 
+@@ -430,25 +425,52 @@ struct dpaa2_eth_priv {
+ 	bool do_link_poll;
+ 	struct task_struct *poll_thread;
+ 
+-	struct dpaa2_eth_hash_fields *hash_fields;
+-	u8 num_hash_fields;
++	/* Rx distribution (hash and flow steering) header fields
++	 * supported by the driver
++	 */
++	struct dpaa2_eth_dist_fields *dist_fields;
++	u8 num_dist_fields;
+ 	/* enabled ethtool hashing bits */
+-	u64 rx_flow_hash;
+-
++	u64 rx_hash_fields;
+ #ifdef CONFIG_FSL_DPAA2_ETH_DEBUGFS
+ 	struct dpaa2_debugfs dbg;
+ #endif
+-
+ 	/* array of classification rules */
+ 	struct dpaa2_eth_cls_rule *cls_rule;
+-
+ 	struct dpni_tx_shaping_cfg shaping_cfg;
+ 
+ 	u8 dcbx_mode;
+ 	struct ieee_pfc pfc;
+ 	bool vlan_clsf_set;
++	bool tx_pause_frames;
++
++	bool ceetm_en;
++};
++
++enum dpaa2_eth_rx_dist {
++	DPAA2_ETH_RX_DIST_HASH,
++	DPAA2_ETH_RX_DIST_FS,
++	DPAA2_ETH_RX_DIST_LEGACY
+ };
+ 
++/* Supported Rx distribution field ids */
++#define DPAA2_ETH_DIST_ETHSRC		BIT(0)
++#define DPAA2_ETH_DIST_ETHDST		BIT(1)
++#define DPAA2_ETH_DIST_ETHTYPE		BIT(2)
++#define DPAA2_ETH_DIST_VLAN		BIT(3)
++#define DPAA2_ETH_DIST_IPSRC		BIT(4)
++#define DPAA2_ETH_DIST_IPDST		BIT(5)
++#define DPAA2_ETH_DIST_IPPROTO		BIT(6)
++#define DPAA2_ETH_DIST_L4SRC		BIT(7)
++#define DPAA2_ETH_DIST_L4DST		BIT(8)
++#define DPAA2_ETH_DIST_ALL		(~0U)
++
++/* Default Rx hash key */
++#define DPAA2_ETH_DIST_DEFAULT_HASH \
++	(DPAA2_ETH_DIST_IPPROTO | \
++	 DPAA2_ETH_DIST_IPSRC | DPAA2_ETH_DIST_IPDST | \
++	 DPAA2_ETH_DIST_L4SRC | DPAA2_ETH_DIST_L4DST)
++
+ #define dpaa2_eth_hash_enabled(priv)	\
+ 	((priv)->dpni_attrs.num_queues > 1)
+ 
+@@ -461,12 +483,29 @@ struct dpaa2_eth_priv {
+ #define dpaa2_eth_fs_count(priv)	\
+ 	((priv)->dpni_attrs.fs_entries)
+ 
+-/* size of DMA memory used to pass configuration to classifier, in bytes */
++/* Required by struct dpni_rx_tc_dist_cfg::key_cfg_iova */
+ #define DPAA2_CLASSIFIER_DMA_SIZE 256
+ 
+ extern const struct ethtool_ops dpaa2_ethtool_ops;
+ extern const char dpaa2_eth_drv_version[];
+ 
++static inline int dpaa2_eth_cmp_dpni_ver(struct dpaa2_eth_priv *priv,
++					 u16 ver_major, u16 ver_minor)
++{
++	if (priv->dpni_ver_major == ver_major)
++		return priv->dpni_ver_minor - ver_minor;
++	return priv->dpni_ver_major - ver_major;
++}
++
++#define DPNI_DIST_KEY_VER_MAJOR			7
++#define DPNI_DIST_KEY_VER_MINOR			5
++
++static inline bool dpaa2_eth_has_legacy_dist(struct dpaa2_eth_priv *priv)
++{
++	return (dpaa2_eth_cmp_dpni_ver(priv, DPNI_DIST_KEY_VER_MAJOR,
++				       DPNI_DIST_KEY_VER_MINOR) < 0);
++}
++
+ /* Hardware only sees DPAA2_ETH_RX_BUF_SIZE, but the skb built around
+  * the buffer also needs space for its shared info struct, and we need
+  * to allocate enough to accommodate hardware alignment restrictions
+@@ -547,7 +586,16 @@ dpaa2_eth_get_td_type(struct dpaa2_eth_priv *priv)
+ 		return DPAA2_ETH_TD_QUEUE;
+ }
+ 
++static inline int dpaa2_eth_ch_count(struct dpaa2_eth_priv *priv)
++{
++	return 1;
++}
++
+ void check_cls_support(struct dpaa2_eth_priv *priv);
+ 
+ int set_rx_taildrop(struct dpaa2_eth_priv *priv);
++
++int dpaa2_eth_set_dist_key(struct dpaa2_eth_priv *priv,
++			   enum dpaa2_eth_rx_dist type, u32 key_fields);
++
+ #endif	/* __DPAA2_H */
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-ethtool.c b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-ethtool.c
+index a87c3bc7..1a1fc825 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-ethtool.c
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-ethtool.c
+@@ -1,4 +1,5 @@
+-/* Copyright 2014-2015 Freescale Semiconductor Inc.
++/* Copyright 2014-2016 Freescale Semiconductor Inc.
++ * Copyright 2016-2017 NXP
+  *
+  * Redistribution and use in source and binary forms, with or without
+  * modification, are permitted provided that the following conditions are met:
+@@ -32,50 +33,44 @@
+ #include "dpni.h"	/* DPNI_LINK_OPT_* */
+ #include "dpaa2-eth.h"
+ 
+-/* To be kept in sync with dpni_statistics */
++/* To be kept in sync with DPNI statistics */
+ static char dpaa2_ethtool_stats[][ETH_GSTRING_LEN] = {
+-	"rx frames",
+-	"rx bytes",
+-	"rx mcast frames",
+-	"rx mcast bytes",
+-	"rx bcast frames",
+-	"rx bcast bytes",
+-	"tx frames",
+-	"tx bytes",
+-	"tx mcast frames",
+-	"tx mcast bytes",
+-	"tx bcast frames",
+-	"tx bcast bytes",
+-	"rx filtered frames",
+-	"rx discarded frames",
+-	"rx nobuffer discards",
+-	"tx discarded frames",
+-	"tx confirmed frames",
++	"[hw] rx frames",
++	"[hw] rx bytes",
++	"[hw] rx mcast frames",
++	"[hw] rx mcast bytes",
++	"[hw] rx bcast frames",
++	"[hw] rx bcast bytes",
++	"[hw] tx frames",
++	"[hw] tx bytes",
++	"[hw] tx mcast frames",
++	"[hw] tx mcast bytes",
++	"[hw] tx bcast frames",
++	"[hw] tx bcast bytes",
++	"[hw] rx filtered frames",
++	"[hw] rx discarded frames",
++	"[hw] rx nobuffer discards",
++	"[hw] tx discarded frames",
++	"[hw] tx confirmed frames",
+ };
+ 
+ #define DPAA2_ETH_NUM_STATS	ARRAY_SIZE(dpaa2_ethtool_stats)
+ 
+-/* To be kept in sync with 'struct dpaa2_eth_drv_stats' */
+ static char dpaa2_ethtool_extras[][ETH_GSTRING_LEN] = {
+ 	/* per-cpu stats */
+-
+-	"tx conf frames",
+-	"tx conf bytes",
+-	"tx sg frames",
+-	"tx sg bytes",
+-	"tx realloc frames",
+-	"rx sg frames",
+-	"rx sg bytes",
+-	/* how many times we had to retry the enqueue command */
+-	"enqueue portal busy",
+-
++	"[drv] tx conf frames",
++	"[drv] tx conf bytes",
++	"[drv] tx sg frames",
++	"[drv] tx sg bytes",
++	"[drv] tx realloc frames",
++	"[drv] rx sg frames",
++	"[drv] rx sg bytes",
++	"[drv] enqueue portal busy",
+ 	/* Channel stats */
+-	/* How many times we had to retry the volatile dequeue command */
+-	"dequeue portal busy",
+-	"channel pull errors",
+-	/* Number of notifications received */
+-	"cdan",
+-	"tx congestion state",
++	"[drv] dequeue portal busy",
++	"[drv] channel pull errors",
++	"[drv] cdan",
++	"[drv] tx congestion state",
+ #ifdef CONFIG_FSL_QBMAN_DEBUG
+ 	/* FQ stats */
+ 	"rx pending frames",
+@@ -91,16 +86,22 @@ static char dpaa2_ethtool_extras[][ETH_GSTRING_LEN] = {
+ static void dpaa2_eth_get_drvinfo(struct net_device *net_dev,
+ 				  struct ethtool_drvinfo *drvinfo)
+ {
++	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
++
+ 	strlcpy(drvinfo->driver, KBUILD_MODNAME, sizeof(drvinfo->driver));
+ 	strlcpy(drvinfo->version, dpaa2_eth_drv_version,
+ 		sizeof(drvinfo->version));
+-	strlcpy(drvinfo->fw_version, "N/A", sizeof(drvinfo->fw_version));
++
++	snprintf(drvinfo->fw_version, sizeof(drvinfo->fw_version),
++		 "%u.%u", priv->dpni_ver_major, priv->dpni_ver_minor);
++
+ 	strlcpy(drvinfo->bus_info, dev_name(net_dev->dev.parent->parent),
+ 		sizeof(drvinfo->bus_info));
+ }
+ 
+-static int dpaa2_eth_get_settings(struct net_device *net_dev,
+-				  struct ethtool_cmd *cmd)
++static int
++dpaa2_eth_get_link_ksettings(struct net_device *net_dev,
++			     struct ethtool_link_ksettings *link_settings)
+ {
+ 	struct dpni_link_state state = {0};
+ 	int err = 0;
+@@ -108,7 +109,7 @@ static int dpaa2_eth_get_settings(struct net_device *net_dev,
+ 
+ 	err = dpni_get_link_state(priv->mc_io, 0, priv->mc_token, &state);
+ 	if (err) {
+-		netdev_err(net_dev, "ERROR %d getting link state", err);
++		netdev_err(net_dev, "ERROR %d getting link state\n", err);
+ 		goto out;
+ 	}
+ 
+@@ -118,39 +119,52 @@ static int dpaa2_eth_get_settings(struct net_device *net_dev,
+ 	 * beyond the DPNI attributes.
+ 	 */
+ 	if (state.options & DPNI_LINK_OPT_AUTONEG)
+-		cmd->autoneg = AUTONEG_ENABLE;
++		link_settings->base.autoneg = AUTONEG_ENABLE;
+ 	if (!(state.options & DPNI_LINK_OPT_HALF_DUPLEX))
+-		cmd->duplex = DUPLEX_FULL;
+-	ethtool_cmd_speed_set(cmd, state.rate);
++		link_settings->base.duplex = DUPLEX_FULL;
++	link_settings->base.speed = state.rate;
+ 
+ out:
+ 	return err;
+ }
+ 
+-static int dpaa2_eth_set_settings(struct net_device *net_dev,
+-				  struct ethtool_cmd *cmd)
++#define DPNI_DYNAMIC_LINK_SET_VER_MAJOR		7
++#define DPNI_DYNAMIC_LINK_SET_VER_MINOR		1
++static int
++dpaa2_eth_set_link_ksettings(struct net_device *net_dev,
++			     const struct ethtool_link_ksettings *link_settings)
+ {
+ 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
+ 	struct dpni_link_state state = {0};
+ 	struct dpni_link_cfg cfg = {0};
+ 	int err = 0;
+ 
+-	netdev_dbg(net_dev, "Setting link parameters...");
++	/* If using an older MC version, the DPNI must be down
++ 	 * in order to be able to change link settings. Taking steps to let
++ 	 * the user know that.
++ 	 */
++	if (dpaa2_eth_cmp_dpni_ver(priv, DPNI_DYNAMIC_LINK_SET_VER_MAJOR,
++				   DPNI_DYNAMIC_LINK_SET_VER_MINOR) < 0) {
++		if (netif_running(net_dev)) {
++			netdev_info(net_dev, "Interface must be brought down first.\n");
++			return -EACCES;
++		}
++	}
+ 
+-	/* Need to interrogate on link state to get flow control params */
++	/* Need to interrogate link state to get flow control params */
+ 	err = dpni_get_link_state(priv->mc_io, 0, priv->mc_token, &state);
+ 	if (err) {
+-		netdev_err(net_dev, "ERROR %d getting link state", err);
++		netdev_err(net_dev, "Error getting link state\n");
+ 		goto out;
+ 	}
+ 
+ 	cfg.options = state.options;
+-	cfg.rate = ethtool_cmd_speed(cmd);
+-	if (cmd->autoneg == AUTONEG_ENABLE)
++	cfg.rate = link_settings->base.speed;
++	if (link_settings->base.autoneg == AUTONEG_ENABLE)
+ 		cfg.options |= DPNI_LINK_OPT_AUTONEG;
+ 	else
+ 		cfg.options &= ~DPNI_LINK_OPT_AUTONEG;
+-	if (cmd->duplex  == DUPLEX_HALF)
++	if (link_settings->base.duplex  == DUPLEX_HALF)
+ 		cfg.options |= DPNI_LINK_OPT_HALF_DUPLEX;
+ 	else
+ 		cfg.options &= ~DPNI_LINK_OPT_HALF_DUPLEX;
+@@ -160,7 +174,7 @@ static int dpaa2_eth_set_settings(struct net_device *net_dev,
+ 		/* ethtool will be loud enough if we return an error; no point
+ 		 * in putting our own error message on the console by default
+ 		 */
+-		netdev_dbg(net_dev, "ERROR %d setting link cfg", err);
++		netdev_dbg(net_dev, "ERROR %d setting link cfg\n", err);
+ 
+ out:
+ 	return err;
+@@ -175,13 +189,13 @@ static void dpaa2_eth_get_pauseparam(struct net_device *net_dev,
+ 
+ 	err = dpni_get_link_state(priv->mc_io, 0, priv->mc_token, &state);
+ 	if (err)
+-		netdev_dbg(net_dev, "ERROR %d getting link state", err);
++		netdev_dbg(net_dev, "Error getting link state\n");
+ 
+-	/* for now, pause frames autonegotiation is not separate */
++	/* Report general port autonegotiation status */
+ 	pause->autoneg = !!(state.options & DPNI_LINK_OPT_AUTONEG);
+ 	pause->rx_pause = !!(state.options & DPNI_LINK_OPT_PAUSE);
+ 	pause->tx_pause = pause->rx_pause ^
+-		!!(state.options & DPNI_LINK_OPT_ASYM_PAUSE);
++			  !!(state.options & DPNI_LINK_OPT_ASYM_PAUSE);
+ }
+ 
+ static int dpaa2_eth_set_pauseparam(struct net_device *net_dev,
+@@ -195,7 +209,7 @@ static int dpaa2_eth_set_pauseparam(struct net_device *net_dev,
+ 
+ 	err = dpni_get_link_state(priv->mc_io, 0, priv->mc_token, &state);
+ 	if (err) {
+-		netdev_dbg(net_dev, "ERROR %d getting link state", err);
++		netdev_dbg(net_dev, "Error getting link state\n");
+ 		goto out;
+ 	}
+ 
+@@ -204,9 +218,12 @@ static int dpaa2_eth_set_pauseparam(struct net_device *net_dev,
+ 	current_tx_pause = !!(cfg.options & DPNI_LINK_OPT_PAUSE) ^
+ 			   !!(cfg.options & DPNI_LINK_OPT_ASYM_PAUSE);
+ 
++	/* We don't support changing pause frame autonegotiation separately
++	 * from general port autoneg
++	 */
+ 	if (pause->autoneg != !!(state.options & DPNI_LINK_OPT_AUTONEG))
+ 		netdev_warn(net_dev,
+-			"WARN: Can't change pause frames autoneg separately\n");
++			    "Cannot change pause frame autoneg separately\n");
+ 
+ 	if (pause->rx_pause)
+ 		cfg.options |= DPNI_LINK_OPT_PAUSE;
+@@ -220,23 +237,19 @@ static int dpaa2_eth_set_pauseparam(struct net_device *net_dev,
+ 
+ 	err = dpni_set_link_cfg(priv->mc_io, 0, priv->mc_token, &cfg);
+ 	if (err) {
+-		/* ethtool will be loud enough if we return an error; no point
+-		 * in putting our own error message on the console by default
+-		 */
+-		netdev_dbg(net_dev, "ERROR %d setting link cfg", err);
++		netdev_dbg(net_dev, "Error setting link\n");
+ 		goto out;
+ 	}
+ 
+-	/* Enable / disable taildrops if Tx pause frames have changed */
++	/* Enable/disable Rx FQ taildrop if Tx pause frames have changed */
+ 	if (current_tx_pause == pause->tx_pause)
+ 		goto out;
+ 
+ 	priv->tx_pause_frames = pause->tx_pause;
+ 	err = set_rx_taildrop(priv);
+ 	if (err)
+-		netdev_dbg(net_dev, "ERROR %d configuring taildrop", err);
++		netdev_dbg(net_dev, "Error configuring taildrop\n");
+ 
+-	priv->tx_pause_frames = pause->tx_pause;
+ out:
+ 	return err;
+ }
+@@ -277,8 +290,9 @@ static void dpaa2_eth_get_ethtool_stats(struct net_device *net_dev,
+ 					struct ethtool_stats *stats,
+ 					u64 *data)
+ {
+-	int i = 0; /* Current index in the data array */
+-	int j = 0, k, err;
++	int i = 0;
++	int j, k, err;
++	int num_cnt;
+ 	union dpni_statistics dpni_stats;
+ 
+ #ifdef CONFIG_FSL_QBMAN_DEBUG
+@@ -299,38 +313,22 @@ static void dpaa2_eth_get_ethtool_stats(struct net_device *net_dev,
+ 	/* Print standard counters, from DPNI statistics */
+ 	for (j = 0; j <= 2; j++) {
+ 		err = dpni_get_statistics(priv->mc_io, 0, priv->mc_token,
+-					  j, &dpni_stats);
++					  j, 0, &dpni_stats);
+ 		if (err != 0)
+-			netdev_warn(net_dev, "Err %d getting DPNI stats page %d",
+-				    err, j);
+-
++			netdev_warn(net_dev, "dpni_get_stats(%d) failed\n", j);
+ 		switch (j) {
+ 		case 0:
+-		*(data + i++) = dpni_stats.page_0.ingress_all_frames;
+-		*(data + i++) = dpni_stats.page_0.ingress_all_bytes;
+-		*(data + i++) = dpni_stats.page_0.ingress_multicast_frames;
+-		*(data + i++) = dpni_stats.page_0.ingress_multicast_bytes;
+-		*(data + i++) = dpni_stats.page_0.ingress_broadcast_frames;
+-		*(data + i++) = dpni_stats.page_0.ingress_broadcast_bytes;
+-		break;
++			num_cnt = sizeof(dpni_stats.page_0) / sizeof(u64);
++			break;
+ 		case 1:
+-		*(data + i++) = dpni_stats.page_1.egress_all_frames;
+-		*(data + i++) = dpni_stats.page_1.egress_all_bytes;
+-		*(data + i++) = dpni_stats.page_1.egress_multicast_frames;
+-		*(data + i++) = dpni_stats.page_1.egress_multicast_bytes;
+-		*(data + i++) = dpni_stats.page_1.egress_broadcast_frames;
+-		*(data + i++) = dpni_stats.page_1.egress_broadcast_bytes;
+-		break;
++			num_cnt = sizeof(dpni_stats.page_1) / sizeof(u64);
++			break;
+ 		case 2:
+-		*(data + i++) = dpni_stats.page_2.ingress_filtered_frames;
+-		*(data + i++) = dpni_stats.page_2.ingress_discarded_frames;
+-		*(data + i++) = dpni_stats.page_2.ingress_nobuffer_discards;
+-		*(data + i++) = dpni_stats.page_2.egress_discarded_frames;
+-		*(data + i++) = dpni_stats.page_2.egress_confirmed_frames;
+-		break;
+-		default:
+-		break;
++			num_cnt = sizeof(dpni_stats.page_2) / sizeof(u64);
++			break;
+ 		}
++		for (k = 0; k < num_cnt; k++)
++			*(data + i++) = dpni_stats.raw.counter[k];
+ 	}
+ 
+ 	/* Print per-cpu extra stats */
+@@ -339,10 +337,8 @@ static void dpaa2_eth_get_ethtool_stats(struct net_device *net_dev,
+ 		for (j = 0; j < sizeof(*extras) / sizeof(__u64); j++)
+ 			*((__u64 *)data + i + j) += *((__u64 *)extras + j);
+ 	}
+-
+ 	i += j;
+ 
+-	/* We may be using fewer DPIOs than actual CPUs */
+ 	for (j = 0; j < priv->num_channels; j++) {
+ 		ch_stats = &priv->channel[j]->stats;
+ 		cdan += ch_stats->cdan;
+@@ -393,11 +389,11 @@ static int cls_key_off(struct dpaa2_eth_priv *priv, int prot, int field)
+ {
+ 	int i, off = 0;
+ 
+-	for (i = 0; i < priv->num_hash_fields; i++) {
+-		if (priv->hash_fields[i].cls_prot == prot &&
+-		    priv->hash_fields[i].cls_field == field)
++	for (i = 0; i < priv->num_dist_fields; i++) {
++		if (priv->dist_fields[i].cls_prot == prot &&
++		    priv->dist_fields[i].cls_field == field)
+ 			return off;
+-		off += priv->hash_fields[i].size;
++		off += priv->dist_fields[i].size;
+ 	}
+ 
+ 	return -1;
+@@ -407,8 +403,8 @@ static u8 cls_key_size(struct dpaa2_eth_priv *priv)
+ {
+ 	u8 i, size = 0;
+ 
+-	for (i = 0; i < priv->num_hash_fields; i++)
+-		size += priv->hash_fields[i].size;
++	for (i = 0; i < priv->num_dist_fields; i++)
++		size += priv->dist_fields[i].size;
+ 
+ 	return size;
+ }
+@@ -425,7 +421,7 @@ void check_cls_support(struct dpaa2_eth_priv *priv)
+ 				 key_size);
+ 			goto disable_fs;
+ 		}
+-		if (priv->num_hash_fields > DPKG_MAX_NUM_OF_EXTRACTS) {
++		if (priv->num_dist_fields > DPKG_MAX_NUM_OF_EXTRACTS) {
+ 			dev_info(dev, "Too many key fields (max = %d). Hashing and steering are disabled\n",
+ 				 DPKG_MAX_NUM_OF_EXTRACTS);
+ 			goto disable_fs;
+@@ -691,8 +687,8 @@ static int do_cls(struct net_device *net_dev,
+ 	     fs->location >= rule_cnt)
+ 		return -EINVAL;
+ 
+-	/* When adding a new rule, check if location if available,
+-	 * and if not free the existing table entry before inserting
++	/* When adding a new rule, check if location if available
++	 * and if not, free the existing table entry before inserting
+ 	 * the new one
+ 	 */
+ 	if (add && (priv->cls_rule[fs->location].in_use == true))
+@@ -777,6 +773,22 @@ static int del_cls(struct net_device *net_dev, int location)
+ 	return 0;
+ }
+ 
++static int set_hash(struct net_device *net_dev, u64 data)
++{
++	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
++	u32 key = 0;
++	int i;
++
++	if (data & RXH_DISCARD)
++		return -EOPNOTSUPP;
++
++	for (i = 0; i < priv->num_dist_fields; i++)
++		if (priv->dist_fields[i].rxnfc_field & data)
++			key |= priv->dist_fields[i].id;
++
++	return dpaa2_eth_set_dist_key(priv, DPAA2_ETH_RX_DIST_HASH, key);
++}
++
+ static int dpaa2_eth_set_rxnfc(struct net_device *net_dev,
+ 			       struct ethtool_rxnfc *rxnfc)
+ {
+@@ -786,11 +798,12 @@ static int dpaa2_eth_set_rxnfc(struct net_device *net_dev,
+ 	case ETHTOOL_SRXCLSRLINS:
+ 		err = add_cls(net_dev, &rxnfc->fs);
+ 		break;
+-
+ 	case ETHTOOL_SRXCLSRLDEL:
+ 		err = del_cls(net_dev, rxnfc->fs.location);
+ 		break;
+-
++	case ETHTOOL_SRXFH:
++		err = set_hash(net_dev, rxnfc->data);
++		break;
+ 	default:
+ 		err = -EOPNOTSUPP;
+ 	}
+@@ -807,12 +820,12 @@ static int dpaa2_eth_get_rxnfc(struct net_device *net_dev,
+ 
+ 	switch (rxnfc->cmd) {
+ 	case ETHTOOL_GRXFH:
+-		/* we purposely ignore cmd->flow_type, because the hashing key
+-		 * is the same (and fixed) for all protocols
++		/* we purposely ignore cmd->flow_type for now, because the
++		 * classifier only supports a single set of fields for all
++		 * protocols
+ 		 */
+-		rxnfc->data = priv->rx_flow_hash;
++		rxnfc->data = priv->rx_hash_fields;
+ 		break;
+-
+ 	case ETHTOOL_GRXRINGS:
+ 		rxnfc->data = dpaa2_eth_queue_count(priv);
+ 		break;
+@@ -853,8 +866,8 @@ static int dpaa2_eth_get_rxnfc(struct net_device *net_dev,
+ const struct ethtool_ops dpaa2_ethtool_ops = {
+ 	.get_drvinfo = dpaa2_eth_get_drvinfo,
+ 	.get_link = ethtool_op_get_link,
+-	.get_settings = dpaa2_eth_get_settings,
+-	.set_settings = dpaa2_eth_set_settings,
++	.get_link_ksettings = dpaa2_eth_get_link_ksettings,
++	.set_link_ksettings = dpaa2_eth_set_link_ksettings,
+ 	.get_pauseparam = dpaa2_eth_get_pauseparam,
+ 	.set_pauseparam = dpaa2_eth_set_pauseparam,
+ 	.get_sset_count = dpaa2_eth_get_sset_count,
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpni-cmd.h b/drivers/staging/fsl-dpaa2/ethernet/dpni-cmd.h
+index 92795eb6..e1e8dd8e 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/dpni-cmd.h
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpni-cmd.h
+@@ -33,6 +33,8 @@
+ #ifndef _FSL_DPNI_CMD_H
+ #define _FSL_DPNI_CMD_H
+ 
++#include "dpni.h"
++
+ /* DPNI Version */
+ #define DPNI_VER_MAJOR				7
+ #define DPNI_VER_MINOR				0
+@@ -73,7 +75,7 @@
+ #define DPNI_CMDID_SET_MAX_FRAME_LENGTH			DPNI_CMD(0x216)
+ #define DPNI_CMDID_GET_MAX_FRAME_LENGTH			DPNI_CMD(0x217)
+ #define DPNI_CMDID_SET_LINK_CFG				DPNI_CMD(0x21A)
+-#define DPNI_CMDID_SET_TX_SHAPING			DPNI_CMD(0x21B)
++#define DPNI_CMDID_SET_TX_SHAPING			DPNI_CMD_V2(0x21B)
+ 
+ #define DPNI_CMDID_SET_MCAST_PROMISC			DPNI_CMD(0x220)
+ #define DPNI_CMDID_GET_MCAST_PROMISC			DPNI_CMD(0x221)
+@@ -94,7 +96,8 @@
+ #define DPNI_CMDID_REMOVE_FS_ENT			DPNI_CMD(0x245)
+ #define DPNI_CMDID_CLR_FS_ENT				DPNI_CMD(0x246)
+ 
+-#define DPNI_CMDID_GET_STATISTICS			DPNI_CMD(0x25D)
++#define DPNI_CMDID_SET_TX_PRIORITIES			DPNI_CMD_V2(0x250)
++#define DPNI_CMDID_GET_STATISTICS			DPNI_CMD_V2(0x25D)
+ #define DPNI_CMDID_RESET_STATISTICS			DPNI_CMD(0x25E)
+ #define DPNI_CMDID_GET_QUEUE				DPNI_CMD(0x25F)
+ #define DPNI_CMDID_SET_QUEUE				DPNI_CMD(0x260)
+@@ -114,6 +117,9 @@
+ #define DPNI_CMDID_GET_OFFLOAD				DPNI_CMD(0x26B)
+ #define DPNI_CMDID_SET_OFFLOAD				DPNI_CMD(0x26C)
+ 
++#define DPNI_CMDID_SET_RX_FS_DIST			DPNI_CMD(0x273)
++#define DPNI_CMDID_SET_RX_HASH_DIST			DPNI_CMD(0x274)
++
+ /* Macros for accessing command fields smaller than 1byte */
+ #define DPNI_MASK(field)	\
+ 	GENMASK(DPNI_##field##_SHIFT + DPNI_##field##_SIZE - 1, \
+@@ -308,6 +314,7 @@ struct dpni_rsp_get_tx_data_offset {
+ 
+ struct dpni_cmd_get_statistics {
+ 	u8 page_number;
++	u8 param;
+ };
+ 
+ struct dpni_rsp_get_statistics {
+@@ -340,12 +347,20 @@ struct dpni_rsp_get_link_state {
+ 	__le64 options;
+ };
+ 
++#define DPNI_COUPLED_SHIFT	0
++#define DPNI_COUPLED_SIZE	1
++
+ struct dpni_cmd_set_tx_shaping {
+ 	/* cmd word 0 */
+-	__le16 max_burst_size;
+-	__le16 pad0[3];
++	__le16 tx_cr_max_burst_size;
++	__le16 tx_er_max_burst_size;
++	__le32 pad;
+ 	/* cmd word 1 */
+-	__le32 rate_limit;
++	__le32 tx_cr_rate_limit;
++	__le32 tx_er_rate_limit;
++	/* cmd word 2 */
++	/* from LSB: coupled:1 */
++	u8 coupled;
+ };
+ 
+ struct dpni_cmd_set_max_frame_length {
+@@ -407,6 +422,24 @@ struct dpni_cmd_clear_mac_filters {
+ 	u8 flags;
+ };
+ 
++#define DPNI_SEPARATE_GRP_SHIFT 0
++#define DPNI_SEPARATE_GRP_SIZE  1
++#define DPNI_MODE_1_SHIFT		0
++#define DPNI_MODE_1_SIZE		4
++#define DPNI_MODE_2_SHIFT		4
++#define DPNI_MODE_2_SIZE		4
++
++struct dpni_cmd_set_tx_priorities {
++	__le16 flags;
++	u8 prio_group_A;
++	u8 prio_group_B;
++	__le32 pad0;
++	u8 modes[4];
++	__le32 pad1;
++	__le64 pad2;
++	__le16 delta_bandwidth[8];
++};
++
+ #define DPNI_DIST_MODE_SHIFT		0
+ #define DPNI_DIST_MODE_SIZE		4
+ #define DPNI_MISS_ACTION_SHIFT		4
+@@ -608,6 +641,11 @@ struct dpni_rsp_get_taildrop {
+ 	__le32 threshold;
+ };
+ 
++struct dpni_rsp_get_api_version {
++	u16 major;
++	u16 minor;
++};
++
+ #define DPNI_DEST_TYPE_SHIFT		0
+ #define DPNI_DEST_TYPE_SIZE		4
+ #define DPNI_CONG_UNITS_SHIFT		4
+@@ -656,4 +694,26 @@ struct dpni_rsp_get_congestion_notification {
+ 	__le32 threshold_entry;
+ 	__le32 threshold_exit;
+ };
++
++#define DPNI_RX_FS_DIST_ENABLE_SHIFT	0
++#define DPNI_RX_FS_DIST_ENABLE_SIZE	1
++struct dpni_cmd_set_rx_fs_dist {
++	__le16 dist_size;
++	u8 enable;
++	u8 tc;
++	__le16 miss_flow_id;
++	__le16 pad;
++	__le64 key_cfg_iova;
++};
++
++#define DPNI_RX_HASH_DIST_ENABLE_SHIFT	0
++#define DPNI_RX_HASH_DIST_ENABLE_SIZE	1
++struct dpni_cmd_set_rx_hash_dist {
++	__le16 dist_size;
++	u8 enable;
++	u8 tc;
++	__le32 pad;
++	__le64 key_cfg_iova;
++};
++
+ #endif /* _FSL_DPNI_CMD_H */
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpni.c b/drivers/staging/fsl-dpaa2/ethernet/dpni.c
+index a42a314b..a3e12dc9 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/dpni.c
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpni.c
+@@ -30,8 +30,9 @@
+  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+  * POSSIBILITY OF SUCH DAMAGE.
+  */
+-#include "../../fsl-mc/include/mc-sys.h"
+-#include "../../fsl-mc/include/mc-cmd.h"
++#include <linux/kernel.h>
++#include <linux/errno.h>
++#include <linux/fsl/mc.h>
+ #include "dpni.h"
+ #include "dpni-cmd.h"
+ 
+@@ -121,7 +122,7 @@ int dpni_open(struct fsl_mc_io *mc_io,
+ 	      int dpni_id,
+ 	      u16 *token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_open *cmd_params;
+ 
+ 	int err;
+@@ -159,7 +160,7 @@ int dpni_close(struct fsl_mc_io *mc_io,
+ 	       u32 cmd_flags,
+ 	       u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_CLOSE,
+@@ -187,7 +188,7 @@ int dpni_set_pools(struct fsl_mc_io *mc_io,
+ 		   u16 token,
+ 		   const struct dpni_pools_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_pools *cmd_params;
+ 	int i;
+ 
+@@ -224,7 +225,7 @@ int dpni_enable(struct fsl_mc_io *mc_io,
+ 		u32 cmd_flags,
+ 		u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ENABLE,
+@@ -247,7 +248,7 @@ int dpni_disable(struct fsl_mc_io *mc_io,
+ 		 u32 cmd_flags,
+ 		 u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_DISABLE,
+@@ -272,7 +273,7 @@ int dpni_is_enabled(struct fsl_mc_io *mc_io,
+ 		    u16 token,
+ 		    int *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_is_enabled *rsp_params;
+ 	int err;
+ 
+@@ -305,7 +306,7 @@ int dpni_reset(struct fsl_mc_io *mc_io,
+ 	       u32 cmd_flags,
+ 	       u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_RESET,
+@@ -337,7 +338,7 @@ int dpni_set_irq_enable(struct fsl_mc_io *mc_io,
+ 			u8 irq_index,
+ 			u8 en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_irq_enable *cmd_params;
+ 
+ 	/* prepare command */
+@@ -368,7 +369,7 @@ int dpni_get_irq_enable(struct fsl_mc_io *mc_io,
+ 			u8 irq_index,
+ 			u8 *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_irq_enable *cmd_params;
+ 	struct dpni_rsp_get_irq_enable *rsp_params;
+ 
+@@ -415,7 +416,7 @@ int dpni_set_irq_mask(struct fsl_mc_io *mc_io,
+ 		      u8 irq_index,
+ 		      u32 mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_irq_mask *cmd_params;
+ 
+ 	/* prepare command */
+@@ -449,7 +450,7 @@ int dpni_get_irq_mask(struct fsl_mc_io *mc_io,
+ 		      u8 irq_index,
+ 		      u32 *mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_irq_mask *cmd_params;
+ 	struct dpni_rsp_get_irq_mask *rsp_params;
+ 	int err;
+@@ -491,7 +492,7 @@ int dpni_get_irq_status(struct fsl_mc_io *mc_io,
+ 			u8 irq_index,
+ 			u32 *status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_irq_status *cmd_params;
+ 	struct dpni_rsp_get_irq_status *rsp_params;
+ 	int err;
+@@ -534,7 +535,7 @@ int dpni_clear_irq_status(struct fsl_mc_io *mc_io,
+ 			  u8 irq_index,
+ 			  u32 status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_clear_irq_status *cmd_params;
+ 
+ 	/* prepare command */
+@@ -563,7 +564,7 @@ int dpni_get_attributes(struct fsl_mc_io *mc_io,
+ 			u16 token,
+ 			struct dpni_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_get_attr *rsp_params;
+ 
+ 	int err;
+@@ -611,7 +612,7 @@ int dpni_set_errors_behavior(struct fsl_mc_io *mc_io,
+ 			     u16 token,
+ 			     struct dpni_error_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_errors_behavior *cmd_params;
+ 
+ 	/* prepare command */
+@@ -643,7 +644,7 @@ int dpni_get_buffer_layout(struct fsl_mc_io *mc_io,
+ 			   enum dpni_queue_type qtype,
+ 			   struct dpni_buffer_layout *layout)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_buffer_layout *cmd_params;
+ 	struct dpni_rsp_get_buffer_layout *rsp_params;
+ 	int err;
+@@ -691,7 +692,7 @@ int dpni_set_buffer_layout(struct fsl_mc_io *mc_io,
+ 			   enum dpni_queue_type qtype,
+ 			   const struct dpni_buffer_layout *layout)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_buffer_layout *cmd_params;
+ 
+ 	/* prepare command */
+@@ -733,7 +734,7 @@ int dpni_set_offload(struct fsl_mc_io *mc_io,
+ 		     enum dpni_offload type,
+ 		     u32 config)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_offload *cmd_params;
+ 
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_OFFLOAD,
+@@ -752,7 +753,7 @@ int dpni_get_offload(struct fsl_mc_io *mc_io,
+ 		     enum dpni_offload type,
+ 		     u32 *config)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_offload *cmd_params;
+ 	struct dpni_rsp_get_offload *rsp_params;
+ 	int err;
+@@ -794,7 +795,7 @@ int dpni_get_qdid(struct fsl_mc_io *mc_io,
+ 		  enum dpni_queue_type qtype,
+ 		  u16 *qdid)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_qdid *cmd_params;
+ 	struct dpni_rsp_get_qdid *rsp_params;
+ 	int err;
+@@ -832,7 +833,7 @@ int dpni_get_tx_data_offset(struct fsl_mc_io *mc_io,
+ 			    u16 token,
+ 			    u16 *data_offset)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_get_tx_data_offset *rsp_params;
+ 	int err;
+ 
+@@ -867,7 +868,7 @@ int dpni_set_link_cfg(struct fsl_mc_io *mc_io,
+ 		      u16 token,
+ 		      const struct dpni_link_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_link_cfg *cmd_params;
+ 
+ 	/* prepare command */
+@@ -896,7 +897,7 @@ int dpni_get_link_state(struct fsl_mc_io *mc_io,
+ 			u16 token,
+ 			struct dpni_link_state *state)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_get_link_state *rsp_params;
+ 	int err;
+ 
+@@ -921,19 +922,23 @@ int dpni_get_link_state(struct fsl_mc_io *mc_io,
+ 
+ /**
+  * dpni_set_tx_shaping() - Set the transmit shaping
+- * @mc_io:     Pointer to MC portal's I/O object
+- * @cmd_flags: Command flags; one or more of 'MC_CMD_FLAG_'
+- * @token:     Token of DPNI object
+- * @tx_shaper:  tx shaping configuration
++ * @mc_io:		Pointer to MC portal's I/O object
++ * @cmd_flags:		Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:		Token of DPNI object
++ * @tx_cr_shaper:	TX committed rate shaping configuration
++ * @tx_er_shaper:	TX excess rate shaping configuration
++ * @coupled:		Committed and excess rate shapers are coupled
+  *
+- * Return:     '0' on Success; Error code otherwise.
++ * Return:	'0' on Success; Error code otherwise.
+  */
+ int dpni_set_tx_shaping(struct fsl_mc_io *mc_io,
+ 			u32 cmd_flags,
+ 			u16 token,
+-			const struct dpni_tx_shaping_cfg *tx_shaper)
++			const struct dpni_tx_shaping_cfg *tx_cr_shaper,
++			const struct dpni_tx_shaping_cfg *tx_er_shaper,
++			int coupled)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_tx_shaping *cmd_params;
+ 
+ 	/* prepare command */
+@@ -941,8 +946,13 @@ int dpni_set_tx_shaping(struct fsl_mc_io *mc_io,
+ 					  cmd_flags,
+ 					  token);
+ 	cmd_params = (struct dpni_cmd_set_tx_shaping *)cmd.params;
+-	cmd_params->max_burst_size = cpu_to_le16(tx_shaper->max_burst_size);
+-	cmd_params->rate_limit = cpu_to_le32(tx_shaper->rate_limit);
++	cmd_params->tx_cr_max_burst_size =
++				cpu_to_le16(tx_cr_shaper->max_burst_size);
++	cmd_params->tx_er_max_burst_size =
++				cpu_to_le16(tx_er_shaper->max_burst_size);
++	cmd_params->tx_cr_rate_limit = cpu_to_le32(tx_cr_shaper->rate_limit);
++	cmd_params->tx_er_rate_limit = cpu_to_le32(tx_er_shaper->rate_limit);
++	dpni_set_field(cmd_params->coupled, COUPLED, coupled);
+ 
+ 	/* send command to mc*/
+ 	return mc_send_command(mc_io, &cmd);
+@@ -964,7 +974,7 @@ int dpni_set_max_frame_length(struct fsl_mc_io *mc_io,
+ 			      u16 token,
+ 			      u16 max_frame_length)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_max_frame_length *cmd_params;
+ 
+ 	/* prepare command */
+@@ -994,7 +1004,7 @@ int dpni_get_max_frame_length(struct fsl_mc_io *mc_io,
+ 			      u16 token,
+ 			      u16 *max_frame_length)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_get_max_frame_length *rsp_params;
+ 	int err;
+ 
+@@ -1029,7 +1039,7 @@ int dpni_set_multicast_promisc(struct fsl_mc_io *mc_io,
+ 			       u16 token,
+ 			       int en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_multicast_promisc *cmd_params;
+ 
+ 	/* prepare command */
+@@ -1057,7 +1067,7 @@ int dpni_get_multicast_promisc(struct fsl_mc_io *mc_io,
+ 			       u16 token,
+ 			       int *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_get_multicast_promisc *rsp_params;
+ 	int err;
+ 
+@@ -1092,7 +1102,7 @@ int dpni_set_unicast_promisc(struct fsl_mc_io *mc_io,
+ 			     u16 token,
+ 			     int en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_unicast_promisc *cmd_params;
+ 
+ 	/* prepare command */
+@@ -1120,7 +1130,7 @@ int dpni_get_unicast_promisc(struct fsl_mc_io *mc_io,
+ 			     u16 token,
+ 			     int *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_get_unicast_promisc *rsp_params;
+ 	int err;
+ 
+@@ -1155,7 +1165,7 @@ int dpni_set_primary_mac_addr(struct fsl_mc_io *mc_io,
+ 			      u16 token,
+ 			      const u8 mac_addr[6])
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_primary_mac_addr *cmd_params;
+ 	int i;
+ 
+@@ -1185,7 +1195,7 @@ int dpni_get_primary_mac_addr(struct fsl_mc_io *mc_io,
+ 			      u16 token,
+ 			      u8 mac_addr[6])
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_get_primary_mac_addr *rsp_params;
+ 	int i, err;
+ 
+@@ -1224,7 +1234,7 @@ int dpni_get_port_mac_addr(struct fsl_mc_io *mc_io,
+ 			   u16 token,
+ 			   u8 mac_addr[6])
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_rsp_get_port_mac_addr *rsp_params;
+ 	int i, err;
+ 
+@@ -1260,7 +1270,7 @@ int dpni_add_mac_addr(struct fsl_mc_io *mc_io,
+ 		      u16 token,
+ 		      const u8 mac_addr[6])
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_add_mac_addr *cmd_params;
+ 	int i;
+ 
+@@ -1290,7 +1300,7 @@ int dpni_remove_mac_addr(struct fsl_mc_io *mc_io,
+ 			 u16 token,
+ 			 const u8 mac_addr[6])
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_remove_mac_addr *cmd_params;
+ 	int i;
+ 
+@@ -1324,7 +1334,7 @@ int dpni_clear_mac_filters(struct fsl_mc_io *mc_io,
+ 			   int unicast,
+ 			   int multicast)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_clear_mac_filters *cmd_params;
+ 
+ 	/* prepare command */
+@@ -1339,6 +1349,55 @@ int dpni_clear_mac_filters(struct fsl_mc_io *mc_io,
+ 	return mc_send_command(mc_io, &cmd);
+ }
+ 
++/**
++ * dpni_set_tx_priorities() - Set transmission TC priority configuration
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPNI object
++ * @cfg:	Transmission selection configuration
++ *
++ * warning:	Allowed only when DPNI is disabled
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpni_set_tx_priorities(struct fsl_mc_io *mc_io,
++			   u32 cmd_flags,
++			   u16 token,
++			   const struct dpni_tx_priorities_cfg *cfg)
++{
++	struct dpni_cmd_set_tx_priorities *cmd_params;
++	struct fsl_mc_command cmd = { 0 };
++	int i;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_TX_PRIORITIES,
++					  cmd_flags,
++					  token);
++	cmd_params = (struct dpni_cmd_set_tx_priorities *)cmd.params;
++	dpni_set_field(cmd_params->flags,
++		       SEPARATE_GRP,
++		       cfg->separate_groups);
++	cmd_params->prio_group_A = cfg->prio_group_A;
++	cmd_params->prio_group_B = cfg->prio_group_B;
++
++	for (i = 0; i + 1 < DPNI_MAX_TC; i += 2) {
++		dpni_set_field(cmd_params->modes[i / 2],
++			       MODE_1,
++			       cfg->tc_sched[i].mode);
++		dpni_set_field(cmd_params->modes[i / 2],
++			       MODE_2,
++			       cfg->tc_sched[i + 1].mode);
++	}
++
++	for (i = 0; i < DPNI_MAX_TC; i++) {
++		cmd_params->delta_bandwidth[i] =
++				cpu_to_le16(cfg->tc_sched[i].delta_bandwidth);
++	}
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++
+ /**
+  * dpni_set_rx_tc_dist() - Set Rx traffic class distribution configuration
+  * @mc_io:	Pointer to MC portal's I/O object
+@@ -1358,7 +1417,7 @@ int dpni_set_rx_tc_dist(struct fsl_mc_io *mc_io,
+ 			u8 tc_id,
+ 			const struct dpni_rx_tc_dist_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_rx_tc_dist *cmd_params;
+ 
+ 	/* prepare command */
+@@ -1398,7 +1457,7 @@ int dpni_set_qos_table(struct fsl_mc_io *mc_io,
+ 		       const struct dpni_qos_tbl_cfg *cfg)
+ {
+ 	struct dpni_cmd_set_qos_table *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_QOS_TBL,
+@@ -1436,7 +1495,7 @@ int dpni_add_qos_entry(struct fsl_mc_io *mc_io,
+ 		       u16 index)
+ {
+ 	struct dpni_cmd_add_qos_entry *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ADD_QOS_ENT,
+@@ -1468,7 +1527,7 @@ int dpni_remove_qos_entry(struct fsl_mc_io *mc_io,
+ 			  const struct dpni_rule_cfg *cfg)
+ {
+ 	struct dpni_cmd_remove_qos_entry *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_REMOVE_QOS_ENT,
+@@ -1507,7 +1566,7 @@ int dpni_add_fs_entry(struct fsl_mc_io *mc_io,
+ 		      const struct dpni_fs_action_cfg *action)
+ {
+ 	struct dpni_cmd_add_fs_entry *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ADD_FS_ENT,
+@@ -1529,7 +1588,7 @@ int dpni_add_fs_entry(struct fsl_mc_io *mc_io,
+ 
+ /**
+  * dpni_remove_fs_entry() - Remove Flow Steering entry from a specific
+- *			traffic class
++ *			    traffic class
+  * @mc_io:	Pointer to MC portal's I/O object
+  * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
+  * @token:	Token of DPNI object
+@@ -1545,7 +1604,7 @@ int dpni_remove_fs_entry(struct fsl_mc_io *mc_io,
+ 			 const struct dpni_rule_cfg *cfg)
+ {
+ 	struct dpni_cmd_remove_fs_entry *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_REMOVE_FS_ENT,
+@@ -1563,17 +1622,18 @@ int dpni_remove_fs_entry(struct fsl_mc_io *mc_io,
+ 
+ /**
+  * dpni_set_congestion_notification() - Set traffic class congestion
+- *	notification configuration
++ *					notification configuration
+  * @mc_io:	Pointer to MC portal's I/O object
+  * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
+  * @token:	Token of DPNI object
+  * @qtype:	Type of queue - Rx, Tx and Tx confirm types are supported
+  * @tc_id:	Traffic class selection (0-7)
+- * @cfg:	congestion notification configuration
++ * @cfg:	Congestion notification configuration
+  *
+  * Return:	'0' on Success; error code otherwise.
+  */
+-int dpni_set_congestion_notification(struct fsl_mc_io *mc_io,
++int dpni_set_congestion_notification(
++			struct fsl_mc_io *mc_io,
+ 			u32 cmd_flags,
+ 			u16 token,
+ 			enum dpni_queue_type qtype,
+@@ -1581,7 +1641,7 @@ int dpni_set_congestion_notification(struct fsl_mc_io *mc_io,
+ 			const struct dpni_congestion_notification_cfg *cfg)
+ {
+ 	struct dpni_cmd_set_congestion_notification *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(
+@@ -1613,7 +1673,10 @@ int dpni_set_congestion_notification(struct fsl_mc_io *mc_io,
+  * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
+  * @token:	Token of DPNI object
+  * @qtype:	Type of queue - Rx, Tx and Tx confirm types are supported
+- * @tc_id:	Traffic class selection (0-7)
++ * @tc_id:	bits 7-4 contain ceetm channel index (valid only for TX);
++ *		bits 3-0 contain traffic class.
++ *		Use macro DPNI_BUILD_CH_TC() to build correct value for
++ *		tc_id parameter.
+  * @cfg:	congestion notification configuration
+  *
+  * Return:	'0' on Success; error code otherwise.
+@@ -1628,7 +1691,7 @@ int dpni_get_congestion_notification(
+ {
+ 	struct dpni_rsp_get_congestion_notification *rsp_params;
+ 	struct dpni_cmd_get_congestion_notification *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -1685,7 +1748,7 @@ int dpni_set_queue(struct fsl_mc_io *mc_io,
+ 		   u8 options,
+ 		   const struct dpni_queue *queue)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_queue *cmd_params;
+ 
+ 	/* prepare command */
+@@ -1733,7 +1796,7 @@ int dpni_get_queue(struct fsl_mc_io *mc_io,
+ 		   struct dpni_queue *queue,
+ 		   struct dpni_queue_id *qid)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_queue *cmd_params;
+ 	struct dpni_rsp_get_queue *rsp_params;
+ 	int err;
+@@ -1757,7 +1820,7 @@ int dpni_get_queue(struct fsl_mc_io *mc_io,
+ 	queue->destination.id = le32_to_cpu(rsp_params->dest_id);
+ 	queue->destination.priority = rsp_params->dest_prio;
+ 	queue->destination.type = dpni_get_field(rsp_params->flags,
+-						     DEST_TYPE);
++						 DEST_TYPE);
+ 	queue->flc.stash_control = dpni_get_field(rsp_params->flags,
+ 						  STASH_CTRL);
+ 	queue->destination.hold_active = dpni_get_field(rsp_params->flags,
+@@ -1777,6 +1840,8 @@ int dpni_get_queue(struct fsl_mc_io *mc_io,
+  * @token:	Token of DPNI object
+  * @page:	Selects the statistics page to retrieve, see
+  *		DPNI_GET_STATISTICS output. Pages are numbered 0 to 2.
++ * @param:	Custom parameter for some pages used to select a certain
++ *		statistic source, for example the TC.
+  * @stat:	Structure containing the statistics
+  *
+  * Return:	'0' on Success; Error code otherwise.
+@@ -1785,9 +1850,10 @@ int dpni_get_statistics(struct fsl_mc_io *mc_io,
+ 			u32 cmd_flags,
+ 			u16 token,
+ 			u8 page,
++			u8 param,
+ 			union dpni_statistics *stat)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_statistics *cmd_params;
+ 	struct dpni_rsp_get_statistics *rsp_params;
+ 	int i, err;
+@@ -1798,6 +1864,7 @@ int dpni_get_statistics(struct fsl_mc_io *mc_io,
+ 					  token);
+ 	cmd_params = (struct dpni_cmd_get_statistics *)cmd.params;
+ 	cmd_params->page_number = page;
++	cmd_params->param = param;
+ 
+ 	/* send command to mc */
+ 	err = mc_send_command(mc_io, &cmd);
+@@ -1824,7 +1891,7 @@ int dpni_reset_statistics(struct fsl_mc_io *mc_io,
+ 			  u32 cmd_flags,
+ 			  u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPNI_CMDID_RESET_STATISTICS,
+@@ -1843,7 +1910,10 @@ int dpni_reset_statistics(struct fsl_mc_io *mc_io,
+  * @cg_point:	Congestion point
+  * @q_type:	Queue type on which the taildrop is configured.
+  *		Only Rx queues are supported for now
+- * @tc:		Traffic class to apply this taildrop to
++ * @tc:		bits 7-4 contain ceetm channel index (valid only for TX);
++ *		bits 3-0 contain traffic class.
++ *		Use macro DPNI_BUILD_CH_TC() to build correct value for
++ *		tc parameter.
+  * @q_index:	Index of the queue if the DPNI supports multiple queues for
+  *		traffic distribution. Ignored if CONGESTION_POINT is not 0.
+  * @taildrop:	Taildrop structure
+@@ -1859,7 +1929,7 @@ int dpni_set_taildrop(struct fsl_mc_io *mc_io,
+ 		      u8 index,
+ 		      struct dpni_taildrop *taildrop)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_set_taildrop *cmd_params;
+ 
+ 	/* prepare command */
+@@ -1887,7 +1957,10 @@ int dpni_set_taildrop(struct fsl_mc_io *mc_io,
+  * @cg_point:	Congestion point
+  * @q_type:	Queue type on which the taildrop is configured.
+  *		Only Rx queues are supported for now
+- * @tc:		Traffic class to apply this taildrop to
++ * @tc:		bits 7-4 contain ceetm channel index (valid only for TX);
++ *		bits 3-0 contain traffic class.
++ *		Use macro DPNI_BUILD_CH_TC() to build correct value for
++ *		tc parameter.
+  * @q_index:	Index of the queue if the DPNI supports multiple queues for
+  *		traffic distribution. Ignored if CONGESTION_POINT is not 0.
+  * @taildrop:	Taildrop structure
+@@ -1903,7 +1976,7 @@ int dpni_get_taildrop(struct fsl_mc_io *mc_io,
+ 		      u8 index,
+ 		      struct dpni_taildrop *taildrop)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpni_cmd_get_taildrop *cmd_params;
+ 	struct dpni_rsp_get_taildrop *rsp_params;
+ 	int err;
+@@ -1931,3 +2004,109 @@ int dpni_get_taildrop(struct fsl_mc_io *mc_io,
+ 
+ 	return 0;
+ }
++
++/**
++ * dpni_get_api_version() - Get Data Path Network Interface API version
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @major_ver:	Major version of data path network interface API
++ * @minor_ver:	Minor version of data path network interface API
++ *
++ * Return:	'0' on Success; Error code otherwise.
++ */
++int dpni_get_api_version(struct fsl_mc_io *mc_io,
++			 u32 cmd_flags,
++			 u16 *major_ver,
++			 u16 *minor_ver)
++{
++	struct dpni_rsp_get_api_version *rsp_params;
++	struct fsl_mc_command cmd = { 0 };
++	int err;
++
++	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_API_VERSION,
++					  cmd_flags, 0);
++
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	rsp_params = (struct dpni_rsp_get_api_version *)cmd.params;
++	*major_ver = le16_to_cpu(rsp_params->major);
++	*minor_ver = le16_to_cpu(rsp_params->minor);
++
++	return 0;
++}
++
++/**
++ * dpni_set_rx_fs_dist() - Set Rx traffic class FS distribution
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPNI object
++ * @cfg: Distribution configuration
++ * If the FS is already enabled with a previous call the classification
++ * key will be changed but all the table rules are kept. If the
++ * existing rules do not match the key the results will not be
++ * predictable. It is the user responsibility to keep key integrity.
++ * If cfg.enable is set to 1 the command will create a flow steering table
++ * and will classify packets according to this table. The packets that
++ * miss all the table rules will be classified according to settings
++ * made in dpni_set_rx_hash_dist()
++ * If cfg.enable is set to 0 the command will clear flow steering table.
++ * The packets will be classified according to settings made in
++ * dpni_set_rx_hash_dist()
++ */
++int dpni_set_rx_fs_dist(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			const struct dpni_rx_dist_cfg *cfg)
++{
++	struct dpni_cmd_set_rx_fs_dist *cmd_params;
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_RX_FS_DIST,
++					  cmd_flags,
++					  token);
++	cmd_params = (struct dpni_cmd_set_rx_fs_dist *)cmd.params;
++	cmd_params->dist_size = le16_to_cpu(cfg->dist_size);
++	dpni_set_field(cmd_params->enable, RX_FS_DIST_ENABLE, cfg->enable);
++	cmd_params->tc = cfg->tc;
++	cmd_params->miss_flow_id = le16_to_cpu(cfg->fs_miss_flow_id);
++	cmd_params->key_cfg_iova = le64_to_cpu(cfg->key_cfg_iova);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
++
++/**
++ * dpni_set_rx_hash_dist() - Set Rx traffic class HASH distribution
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPNI object
++ * @cfg: Distribution configuration
++ * If cfg.enable is set to 1 the packets will be classified using a hash
++ * function based on the key received in cfg.key_cfg_iova parameter.
++ * If cfg.enable is set to 0 the packets will be sent to the queue configured
++ * in dpni_set_rx_dist_default_queue() call
++ */
++int dpni_set_rx_hash_dist(struct fsl_mc_io *mc_io,
++			  u32 cmd_flags,
++			  u16 token,
++			  const struct dpni_rx_dist_cfg *cfg)
++{
++	struct dpni_cmd_set_rx_hash_dist *cmd_params;
++	struct fsl_mc_command cmd = { 0 };
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_RX_HASH_DIST,
++					  cmd_flags,
++					  token);
++	cmd_params = (struct dpni_cmd_set_rx_hash_dist *)cmd.params;
++	cmd_params->dist_size = le16_to_cpu(cfg->dist_size);
++	dpni_set_field(cmd_params->enable, RX_FS_DIST_ENABLE, cfg->enable);
++	cmd_params->tc = cfg->tc;
++	cmd_params->key_cfg_iova = le64_to_cpu(cfg->key_cfg_iova);
++
++	/* send command to mc*/
++	return mc_send_command(mc_io, &cmd);
++}
+diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpni.h b/drivers/staging/fsl-dpaa2/ethernet/dpni.h
+index 40128ed1..0dcf1035 100644
+--- a/drivers/staging/fsl-dpaa2/ethernet/dpni.h
++++ b/drivers/staging/fsl-dpaa2/ethernet/dpni.h
+@@ -55,11 +55,11 @@ struct fsl_mc_io;
+ /**
+  * Maximum number of senders
+  */
+-#define DPNI_MAX_SENDERS			8
++#define DPNI_MAX_SENDERS			16
+ /**
+  * Maximum distribution size
+  */
+-#define DPNI_MAX_DIST_SIZE			8
++#define DPNI_MAX_DIST_SIZE			16
+ 
+ /**
+  * All traffic classes considered; see dpni_set_queue()
+@@ -485,6 +485,24 @@ union dpni_statistics {
+ 		u64 egress_discarded_frames;
+ 		u64 egress_confirmed_frames;
+ 	} page_2;
++	/**
++	 * struct page_3 - Page_3 statistics structure with values for the
++	 *		   selected TC
++	 * @ceetm_dequeue_bytes: Cumulative count of the number of bytes
++	 *			 dequeued
++	 * @ceetm_dequeue_frames: Cumulative count of the number of frames
++	 *			  dequeued
++	 * @ceetm_reject_bytes: Cumulative count of the number of bytes in all
++	 *			frames whose enqueue was rejected
++	 * @ceetm_reject_frames: Cumulative count of all frame enqueues
++	 *			 rejected
++	 */
++	struct {
++		u64 ceetm_dequeue_bytes;
++		u64 ceetm_dequeue_frames;
++		u64 ceetm_reject_bytes;
++		u64 ceetm_reject_frames;
++	} page_3;
+ 	/**
+ 	 * struct raw - raw statistics structure
+ 	 */
+@@ -497,6 +515,7 @@ int dpni_get_statistics(struct fsl_mc_io	*mc_io,
+ 			u32			cmd_flags,
+ 			u16			token,
+ 			u8			page,
++			u8			param,
+ 			union dpni_statistics	*stat);
+ 
+ int dpni_reset_statistics(struct fsl_mc_io *mc_io,
+@@ -562,14 +581,16 @@ int dpni_get_link_state(struct fsl_mc_io	*mc_io,
+  * @max_burst_size: burst size in bytes (up to 64KB)
+  */
+ struct dpni_tx_shaping_cfg {
+-	u32     rate_limit;
+-	u16     max_burst_size;
++	u32	rate_limit;
++	u16	max_burst_size;
+ };
+ 
+-int dpni_set_tx_shaping(struct fsl_mc_io                       *mc_io,
+-			u32                                     cmd_flags,
+-			u16                                     token,
+-			const struct dpni_tx_shaping_cfg        *tx_shaper);
++int dpni_set_tx_shaping(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			const struct dpni_tx_shaping_cfg *tx_cr_shaper,
++			const struct dpni_tx_shaping_cfg *tx_er_shaper,
++			int coupled);
+ 
+ int dpni_set_max_frame_length(struct fsl_mc_io	*mc_io,
+ 			      u32		cmd_flags,
+@@ -691,6 +712,50 @@ int dpni_set_qos_table(struct fsl_mc_io *mc_io,
+ 		       u16 token,
+ 		       const struct dpni_qos_tbl_cfg *cfg);
+ 
++/**
++ * enum dpni_tx_schedule_mode - DPNI Tx scheduling mode
++ * @DPNI_TX_SCHED_STRICT_PRIORITY: strict priority
++ * @DPNI_TX_SCHED_WEIGHTED_A: weighted based scheduling in group A
++ * @DPNI_TX_SCHED_WEIGHTED_B: weighted based scheduling in group B
++ */
++enum dpni_tx_schedule_mode {
++	DPNI_TX_SCHED_STRICT_PRIORITY = 0,
++	DPNI_TX_SCHED_WEIGHTED_A,
++	DPNI_TX_SCHED_WEIGHTED_B,
++};
++
++/**
++ * struct dpni_tx_schedule_cfg - Structure representing Tx scheduling conf
++ * @mode:		Scheduling mode
++ * @delta_bandwidth:	Bandwidth represented in weights from 100 to 10000;
++ *	not applicable for 'strict-priority' mode;
++ */
++struct dpni_tx_schedule_cfg {
++	enum dpni_tx_schedule_mode mode;
++	u16 delta_bandwidth;
++};
++
++/**
++ * struct dpni_tx_priorities_cfg - Structure representing transmission
++ *					priorities for DPNI TCs
++ * @tc_sched:	An array of traffic-classes
++ * @prio_group_A: Priority of group A
++ * @prio_group_B: Priority of group B
++ * @separate_groups: Treat A and B groups as separate
++ * @ceetm_ch_idx: ceetm channel index to apply the changes
++ */
++struct dpni_tx_priorities_cfg {
++	struct dpni_tx_schedule_cfg tc_sched[DPNI_MAX_TC];
++	u8 prio_group_A;
++	u8 prio_group_B;
++	u8 separate_groups;
++};
++
++int dpni_set_tx_priorities(struct fsl_mc_io *mc_io,
++			   u32 cmd_flags,
++			   u16 token,
++			   const struct dpni_tx_priorities_cfg *cfg);
++
+ /**
+  * struct dpni_rx_tc_dist_cfg - Rx traffic class distribution configuration
+  * @dist_size: Set the distribution size;
+@@ -838,11 +903,11 @@ enum dpni_congestion_point {
+ 
+ /**
+  * struct dpni_dest_cfg - Structure representing DPNI destination parameters
+- * @dest_type: Destination type
+- * @dest_id: Either DPIO ID or DPCON ID, depending on the destination type
+- * @priority: Priority selection within the DPIO or DPCON channel; valid values
+- *		are 0-1 or 0-7, depending on the number of priorities in that
+- *		channel; not relevant for 'DPNI_DEST_NONE' option
++ * @dest_type:	Destination type
++ * @dest_id:	Either DPIO ID or DPCON ID, depending on the destination type
++ * @priority:	Priority selection within the DPIO or DPCON channel; valid
++ *		values are 0-1 or 0-7, depending on the number of priorities
++ *		in that channel; not relevant for 'DPNI_DEST_NONE' option
+  */
+ struct dpni_dest_cfg {
+ 	enum dpni_dest dest_type;
+@@ -856,34 +921,34 @@ struct dpni_dest_cfg {
+  * CSCN message is written to message_iova once entering a
+  * congestion state (see 'threshold_entry')
+  */
+-#define DPNI_CONG_OPT_WRITE_MEM_ON_ENTER	0x00000001
++#define DPNI_CONG_OPT_WRITE_MEM_ON_ENTER        0x00000001
+ /**
+  * CSCN message is written to message_iova once exiting a
+  * congestion state (see 'threshold_exit')
+  */
+-#define DPNI_CONG_OPT_WRITE_MEM_ON_EXIT		0x00000002
++#define DPNI_CONG_OPT_WRITE_MEM_ON_EXIT         0x00000002
+ /**
+  * CSCN write will attempt to allocate into a cache (coherent write);
+  * valid only if 'DPNI_CONG_OPT_WRITE_MEM_<X>' is selected
+  */
+-#define DPNI_CONG_OPT_COHERENT_WRITE		0x00000004
++#define DPNI_CONG_OPT_COHERENT_WRITE            0x00000004
+ /**
+  * if 'dest_cfg.dest_type != DPNI_DEST_NONE' CSCN message is sent to
+  * DPIO/DPCON's WQ channel once entering a congestion state
+  * (see 'threshold_entry')
+  */
+-#define DPNI_CONG_OPT_NOTIFY_DEST_ON_ENTER	0x00000008
++#define DPNI_CONG_OPT_NOTIFY_DEST_ON_ENTER      0x00000008
+ /**
+  * if 'dest_cfg.dest_type != DPNI_DEST_NONE' CSCN message is sent to
+  * DPIO/DPCON's WQ channel once exiting a congestion state
+  * (see 'threshold_exit')
+  */
+-#define DPNI_CONG_OPT_NOTIFY_DEST_ON_EXIT	0x00000010
++#define DPNI_CONG_OPT_NOTIFY_DEST_ON_EXIT       0x00000010
+ /**
+  * if 'dest_cfg.dest_type != DPNI_DEST_NONE' when the CSCN is written to the
+  * sw-portal's DQRR, the DQRI interrupt is asserted immediately (if enabled)
+  */
+-#define DPNI_CONG_OPT_INTR_COALESCING_DISABLED	0x00000020
++#define DPNI_CONG_OPT_INTR_COALESCING_DISABLED  0x00000020
+ /**
+  * This congestion will trigger flow control or priority flow control.
+  * This will have effect only if flow control is enabled with
+@@ -893,15 +958,15 @@ struct dpni_dest_cfg {
+ 
+ /**
+  * struct dpni_congestion_notification_cfg - congestion notification
+- *		configuration
+- * @units: units type
+- * @threshold_entry: above this threshold we enter a congestion state.
+- *	set it to '0' to disable it
+- * @threshold_exit: below this threshold we exit the congestion state.
++ *					configuration
++ * @units: Units type
++ * @threshold_entry: Above this threshold we enter a congestion state.
++ *		set it to '0' to disable it
++ * @threshold_exit: Below this threshold we exit the congestion state.
+  * @message_ctx: The context that will be part of the CSCN message
+  * @message_iova: I/O virtual address (must be in DMA-able memory),
+- *	must be 16B aligned; valid only if 'DPNI_CONG_OPT_WRITE_MEM_<X>' is
+- *	contained in 'options'
++ *		must be 16B aligned; valid only if 'DPNI_CONG_OPT_WRITE_MEM_<X>'
++ *		is contained in 'options'
+  * @dest_cfg: CSCN can be send to either DPIO or DPCON WQ channel
+  * @notification_mode: Mask of available options; use 'DPNI_CONG_OPT_<X>' values
+  */
+@@ -916,7 +981,14 @@ struct dpni_congestion_notification_cfg {
+ 	u16 notification_mode;
+ };
+ 
+-int dpni_set_congestion_notification(struct fsl_mc_io *mc_io,
++/** Compose TC parameter for function dpni_set_congestion_notification()
++ * and dpni_get_congestion_notification().
++ */
++#define DPNI_BUILD_CH_TC(ceetm_ch_idx, tc) \
++	((((ceetm_ch_idx) & 0x0F) << 4) | ((tc) & 0x0F))
++
++int dpni_set_congestion_notification(
++			struct fsl_mc_io *mc_io,
+ 			u32 cmd_flags,
+ 			u16 token,
+ 			enum dpni_queue_type qtype,
+@@ -977,6 +1049,11 @@ struct dpni_rule_cfg {
+ 	u8	key_size;
+ };
+ 
++int dpni_get_api_version(struct fsl_mc_io *mc_io,
++			 u32 cmd_flags,
++			 u16 *major_ver,
++			 u16 *minor_ver);
++
+ int dpni_add_qos_entry(struct fsl_mc_io *mc_io,
+ 		       u32 cmd_flags,
+ 		       u16 token,
+@@ -994,13 +1071,13 @@ int dpni_clear_qos_table(struct fsl_mc_io *mc_io,
+ 			 u16 token);
+ 
+ /**
+- * Discard matching traffic.  If set, this takes precedence over any other
++ * Discard matching traffic. If set, this takes precedence over any other
+  * configuration and matching traffic is always discarded.
+  */
+  #define DPNI_FS_OPT_DISCARD            0x1
+ 
+ /**
+- * Set FLC value.  If set, flc member of truct dpni_fs_action_cfg is used to
++ * Set FLC value. If set, flc member of struct dpni_fs_action_cfg is used to
+  * override the FLC value set per queue.
+  * For more details check the Frame Descriptor section in the hardware
+  * documentation.
+@@ -1009,26 +1086,27 @@ int dpni_clear_qos_table(struct fsl_mc_io *mc_io,
+ 
+ /*
+  * Indicates whether the 6 lowest significant bits of FLC are used for stash
+- * control.  If set, the 6 least significant bits in value are interpreted as
++ * control. If set, the 6 least significant bits in value are interpreted as
+  * follows:
+  *     - bits 0-1: indicates the number of 64 byte units of context that are
+- *     stashed.  FLC value is interpreted as a memory address in this case,
++ *     stashed. FLC value is interpreted as a memory address in this case,
+  *     excluding the 6 LS bits.
+  *     - bits 2-3: indicates the number of 64 byte units of frame annotation
+- *     to be stashed.  Annotation is placed at FD[ADDR].
++ *     to be stashed. Annotation is placed at FD[ADDR].
+  *     - bits 4-5: indicates the number of 64 byte units of frame data to be
+- *     stashed.  Frame data is placed at FD[ADDR] + FD[OFFSET].
++ *     stashed. Frame data is placed at FD[ADDR] + FD[OFFSET].
+  * This flag is ignored if DPNI_FS_OPT_SET_FLC is not specified.
+  */
+ #define DPNI_FS_OPT_SET_STASH_CONTROL  0x4
+ 
+ /**
+  * struct dpni_fs_action_cfg - Action configuration for table look-up
+- * @flc: FLC value for traffic matching this rule.  Please check the Frame
+- * Descriptor section in the hardware documentation for more information.
+- * @flow_id: Identifies the Rx queue used for matching traffic.  Supported
+- *     values are in range 0 to num_queue-1.
+- * @options: Any combination of DPNI_FS_OPT_ values.
++ * @flc:	FLC value for traffic matching this rule. Please check the
++ *		Frame Descriptor section in the hardware documentation for
++ *		more information.
++ * @flow_id:	Identifies the Rx queue used for matching traffic. Supported
++ *		values are in range 0 to num_queue-1.
++ * @options:	Any combination of DPNI_FS_OPT_ values.
+  */
+ struct dpni_fs_action_cfg {
+ 	u64 flc;
+@@ -1050,4 +1128,45 @@ int dpni_remove_fs_entry(struct fsl_mc_io *mc_io,
+ 			 u8 tc_id,
+ 			 const struct dpni_rule_cfg *cfg);
+ 
++/**
++ * When used for queue_idx in function dpni_set_rx_dist_default_queue
++ * will signal to dpni to drop all unclassified frames
++ */
++#define DPNI_FS_MISS_DROP		((uint16_t)-1)
++
++/**
++ * struct dpni_rx_dist_cfg - distribution configuration
++ * @dist_size:	distribution size; supported values: 1,2,3,4,6,7,8,
++ *		12,14,16,24,28,32,48,56,64,96,112,128,192,224,256,384,448,
++ *		512,768,896,1024
++ * @key_cfg_iova: I/O virtual address of 256 bytes DMA-able memory filled with
++ *		the extractions to be used for the distribution key by calling
++ *		dpkg_prepare_key_cfg() relevant only when enable!=0 otherwise
++ *		it can be '0'
++ * @enable: enable/disable the distribution.
++ * @tc: TC id for which distribution is set
++ * @fs_miss_flow_id: when packet misses all rules from flow steering table and
++ *		hash is disabled it will be put into this queue id; use
++ *		DPNI_FS_MISS_DROP to drop frames. The value of this field is
++ *		used only when flow steering distribution is enabled and hash
++ *		distribution is disabled
++ */
++struct dpni_rx_dist_cfg {
++	u16 dist_size;
++	u64 key_cfg_iova;
++	u8 enable;
++	u8 tc;
++	u16 fs_miss_flow_id;
++};
++
++int dpni_set_rx_fs_dist(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			const struct dpni_rx_dist_cfg *cfg);
++
++int dpni_set_rx_hash_dist(struct fsl_mc_io *mc_io,
++			  u32 cmd_flags,
++			  u16 token,
++			  const struct dpni_rx_dist_cfg *cfg);
++
+ #endif /* __FSL_DPNI_H */
+diff --git a/drivers/staging/fsl-dpaa2/ethsw/Makefile b/drivers/staging/fsl-dpaa2/ethsw/Makefile
+index de92cd91..59f2c595 100644
+--- a/drivers/staging/fsl-dpaa2/ethsw/Makefile
++++ b/drivers/staging/fsl-dpaa2/ethsw/Makefile
+@@ -1,6 +1,9 @@
++# SPDX-License-Identifier: GPL-2.0
+ #
+ # Makefile for the Freescale DPAA2 Ethernet Switch
+ #
++# Copyright 2014-2017 Freescale Semiconductor, Inc.
++# Copyright 2017-2018 NXP
+ 
+ obj-$(CONFIG_FSL_DPAA2_ETHSW) += dpaa2-ethsw.o
+ 
+diff --git a/drivers/staging/fsl-dpaa2/ethsw/dpsw-cmd.h b/drivers/staging/fsl-dpaa2/ethsw/dpsw-cmd.h
+index 06b71122..55081ec9 100644
+--- a/drivers/staging/fsl-dpaa2/ethsw/dpsw-cmd.h
++++ b/drivers/staging/fsl-dpaa2/ethsw/dpsw-cmd.h
+@@ -1,35 +1,10 @@
+-/* Copyright 2013-2016 Freescale Semiconductor Inc.
+- * Copyright 2017 NXP
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Copyright 2013-2016 Freescale Semiconductor, Inc.
++ * Copyright 2017-2018 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *	 notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *	 notice, this list of conditions and the following disclaimer in the
+- *	 documentation and/or other materials provided with the distribution.
+- *     * Neither the name of the above-listed copyright holders nor the
+- *	 names of any contributors may be used to endorse or promote products
+- *	 derived from this software without specific prior written permission.
+- *
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+- * POSSIBILITY OF SUCH DAMAGE.
+  */
++
+ #ifndef __FSL_DPSW_CMD_H
+ #define __FSL_DPSW_CMD_H
+ 
+@@ -74,6 +49,8 @@
+ #define DPSW_CMDID_IF_SET_FLOODING          DPSW_CMD_ID(0x047)
+ #define DPSW_CMDID_IF_SET_BROADCAST         DPSW_CMD_ID(0x048)
+ 
++#define DPSW_CMDID_IF_GET_TCI               DPSW_CMD_ID(0x04A)
++
+ #define DPSW_CMDID_IF_SET_LINK_CFG          DPSW_CMD_ID(0x04C)
+ 
+ #define DPSW_CMDID_VLAN_ADD                 DPSW_CMD_ID(0x060)
+@@ -231,6 +208,17 @@ struct dpsw_cmd_if_set_tci {
+ 	__le16 conf;
+ };
+ 
++struct dpsw_cmd_if_get_tci {
++	__le16 if_id;
++};
++
++struct dpsw_rsp_if_get_tci {
++	__le16 pad;
++	__le16 vlan_id;
++	u8 dei;
++	u8 pcp;
++};
++
+ #define DPSW_STATE_SHIFT	0
+ #define DPSW_STATE_SIZE		4
+ 
+diff --git a/drivers/staging/fsl-dpaa2/ethsw/dpsw.c b/drivers/staging/fsl-dpaa2/ethsw/dpsw.c
+index 8d6455a0..11dc7485 100644
+--- a/drivers/staging/fsl-dpaa2/ethsw/dpsw.c
++++ b/drivers/staging/fsl-dpaa2/ethsw/dpsw.c
+@@ -1,37 +1,11 @@
+-/* Copyright 2013-2016 Freescale Semiconductor Inc.
+- * Copyright 2017 NXP
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Copyright 2013-2016 Freescale Semiconductor, Inc.
++ * Copyright 2017-2018 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *	 notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *	 notice, this list of conditions and the following disclaimer in the
+- *	 documentation and/or other materials provided with the distribution.
+- *     * Neither the name of the above-listed copyright holders nor the
+- *	 names of any contributors may be used to endorse or promote products
+- *	 derived from this software without specific prior written permission.
+- *
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+- * POSSIBILITY OF SUCH DAMAGE.
+  */
+-#include "../../fsl-mc/include/mc.h"
+-#include "../../fsl-mc/include/mc-sys.h"
++
++#include <linux/fsl/mc.h>
+ #include "dpsw.h"
+ #include "dpsw-cmd.h"
+ 
+@@ -43,7 +17,7 @@ static void build_if_id_bitmap(__le64 *bmap,
+ 
+ 	for (i = 0; (i < num_ifs) && (i < DPSW_MAX_IF); i++) {
+ 		if (id[i] < DPSW_MAX_IF)
+-			bmap[id[i] / 64] |= BIT_MASK(id[i] % 64);
++			bmap[id[i] / 64] |= cpu_to_le64(BIT_MASK(id[i] % 64));
+ 	}
+ }
+ 
+@@ -69,7 +43,7 @@ int dpsw_open(struct fsl_mc_io *mc_io,
+ 	      int dpsw_id,
+ 	      u16 *token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_open *cmd_params;
+ 	int err;
+ 
+@@ -106,7 +80,7 @@ int dpsw_close(struct fsl_mc_io *mc_io,
+ 	       u32 cmd_flags,
+ 	       u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPSW_CMDID_CLOSE,
+@@ -129,7 +103,7 @@ int dpsw_enable(struct fsl_mc_io *mc_io,
+ 		u32 cmd_flags,
+ 		u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPSW_CMDID_ENABLE,
+@@ -152,7 +126,7 @@ int dpsw_disable(struct fsl_mc_io *mc_io,
+ 		 u32 cmd_flags,
+ 		 u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPSW_CMDID_DISABLE,
+@@ -175,7 +149,7 @@ int dpsw_reset(struct fsl_mc_io *mc_io,
+ 	       u32 cmd_flags,
+ 	       u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPSW_CMDID_RESET,
+@@ -207,7 +181,7 @@ int dpsw_set_irq_enable(struct fsl_mc_io *mc_io,
+ 			u8 irq_index,
+ 			u8 en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_set_irq_enable *cmd_params;
+ 
+ 	/* prepare command */
+@@ -244,7 +218,7 @@ int dpsw_set_irq_mask(struct fsl_mc_io *mc_io,
+ 		      u8 irq_index,
+ 		      u32 mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_set_irq_mask *cmd_params;
+ 
+ 	/* prepare command */
+@@ -277,7 +251,7 @@ int dpsw_get_irq_status(struct fsl_mc_io *mc_io,
+ 			u8 irq_index,
+ 			u32 *status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_get_irq_status *cmd_params;
+ 	struct dpsw_rsp_get_irq_status *rsp_params;
+ 	int err;
+@@ -320,7 +294,7 @@ int dpsw_clear_irq_status(struct fsl_mc_io *mc_io,
+ 			  u8 irq_index,
+ 			  u32 status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_clear_irq_status *cmd_params;
+ 
+ 	/* prepare command */
+@@ -349,7 +323,7 @@ int dpsw_get_attributes(struct fsl_mc_io *mc_io,
+ 			u16 token,
+ 			struct dpsw_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_rsp_get_attr *rsp_params;
+ 	int err;
+ 
+@@ -399,7 +373,7 @@ int dpsw_if_set_link_cfg(struct fsl_mc_io *mc_io,
+ 			 u16 if_id,
+ 			 struct dpsw_link_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if_set_link_cfg *cmd_params;
+ 
+ 	/* prepare command */
+@@ -431,7 +405,7 @@ int dpsw_if_get_link_state(struct fsl_mc_io *mc_io,
+ 			   u16 if_id,
+ 			   struct dpsw_link_state *state)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if_get_link_state *cmd_params;
+ 	struct dpsw_rsp_if_get_link_state *rsp_params;
+ 	int err;
+@@ -473,7 +447,7 @@ int dpsw_if_set_flooding(struct fsl_mc_io *mc_io,
+ 			 u16 if_id,
+ 			 u8 en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if_set_flooding *cmd_params;
+ 
+ 	/* prepare command */
+@@ -504,7 +478,7 @@ int dpsw_if_set_broadcast(struct fsl_mc_io *mc_io,
+ 			  u16 if_id,
+ 			  u8 en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if_set_broadcast *cmd_params;
+ 
+ 	/* prepare command */
+@@ -535,8 +509,9 @@ int dpsw_if_set_tci(struct fsl_mc_io *mc_io,
+ 		    u16 if_id,
+ 		    const struct dpsw_tci_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if_set_tci *cmd_params;
++	u16 tmp_conf = 0;
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPSW_CMDID_IF_SET_TCI,
+@@ -544,15 +519,57 @@ int dpsw_if_set_tci(struct fsl_mc_io *mc_io,
+ 					  token);
+ 	cmd_params = (struct dpsw_cmd_if_set_tci *)cmd.params;
+ 	cmd_params->if_id = cpu_to_le16(if_id);
+-	dpsw_set_field(cmd_params->conf, VLAN_ID, cfg->vlan_id);
+-	dpsw_set_field(cmd_params->conf, DEI, cfg->dei);
+-	dpsw_set_field(cmd_params->conf, PCP, cfg->pcp);
+-	cmd_params->conf = cpu_to_le16(cmd_params->conf);
++	dpsw_set_field(tmp_conf, VLAN_ID, cfg->vlan_id);
++	dpsw_set_field(tmp_conf, DEI, cfg->dei);
++	dpsw_set_field(tmp_conf, PCP, cfg->pcp);
++	cmd_params->conf = cpu_to_le16(tmp_conf);
+ 
+ 	/* send command to mc*/
+ 	return mc_send_command(mc_io, &cmd);
+ }
+ 
++/**
++ * dpsw_if_get_tci() - Get default VLAN Tag Control Information (TCI)
++ * @mc_io:	Pointer to MC portal's I/O object
++ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
++ * @token:	Token of DPSW object
++ * @if_id:	Interface Identifier
++ * @cfg:	Tag Control Information Configuration
++ *
++ * Return:	Completion status. '0' on Success; Error code otherwise.
++ */
++int dpsw_if_get_tci(struct fsl_mc_io *mc_io,
++		    u32 cmd_flags,
++		    u16 token,
++		    u16 if_id,
++		    struct dpsw_tci_cfg *cfg)
++{
++	struct fsl_mc_command cmd = { 0 };
++	struct dpsw_cmd_if_get_tci *cmd_params;
++	struct dpsw_rsp_if_get_tci *rsp_params;
++	int err;
++
++	/* prepare command */
++	cmd.header = mc_encode_cmd_header(DPSW_CMDID_IF_GET_TCI,
++					  cmd_flags,
++					  token);
++	cmd_params = (struct dpsw_cmd_if_get_tci *)cmd.params;
++	cmd_params->if_id = cpu_to_le16(if_id);
++
++	/* send command to mc*/
++	err = mc_send_command(mc_io, &cmd);
++	if (err)
++		return err;
++
++	/* retrieve response parameters */
++	rsp_params = (struct dpsw_rsp_if_get_tci *)cmd.params;
++	cfg->pcp = rsp_params->pcp;
++	cfg->dei = rsp_params->dei;
++	cfg->vlan_id = le16_to_cpu(rsp_params->vlan_id);
++
++	return 0;
++}
++
+ /**
+  * dpsw_if_set_stp() - Function sets Spanning Tree Protocol (STP) state.
+  * @mc_io:	Pointer to MC portal's I/O object
+@@ -572,7 +589,7 @@ int dpsw_if_set_stp(struct fsl_mc_io *mc_io,
+ 		    u16 if_id,
+ 		    const struct dpsw_stp_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if_set_stp *cmd_params;
+ 
+ 	/* prepare command */
+@@ -606,7 +623,7 @@ int dpsw_if_get_counter(struct fsl_mc_io *mc_io,
+ 			enum dpsw_counter type,
+ 			u64 *counter)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if_get_counter *cmd_params;
+ 	struct dpsw_rsp_if_get_counter *rsp_params;
+ 	int err;
+@@ -645,7 +662,7 @@ int dpsw_if_enable(struct fsl_mc_io *mc_io,
+ 		   u16 token,
+ 		   u16 if_id)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if *cmd_params;
+ 
+ 	/* prepare command */
+@@ -673,7 +690,7 @@ int dpsw_if_disable(struct fsl_mc_io *mc_io,
+ 		    u16 token,
+ 		    u16 if_id)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if *cmd_params;
+ 
+ 	/* prepare command */
+@@ -703,7 +720,7 @@ int dpsw_if_set_max_frame_length(struct fsl_mc_io *mc_io,
+ 				 u16 if_id,
+ 				 u16 frame_length)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_if_set_max_frame_length *cmd_params;
+ 
+ 	/* prepare command */
+@@ -741,7 +758,7 @@ int dpsw_vlan_add(struct fsl_mc_io *mc_io,
+ 		  u16 vlan_id,
+ 		  const struct dpsw_vlan_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_vlan_add *cmd_params;
+ 
+ 	/* prepare command */
+@@ -777,7 +794,7 @@ int dpsw_vlan_add_if(struct fsl_mc_io *mc_io,
+ 		     u16 vlan_id,
+ 		     const struct dpsw_vlan_if_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_vlan_manage_if *cmd_params;
+ 
+ 	/* prepare command */
+@@ -815,7 +832,7 @@ int dpsw_vlan_add_if_untagged(struct fsl_mc_io *mc_io,
+ 			      u16 vlan_id,
+ 			      const struct dpsw_vlan_if_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_vlan_manage_if *cmd_params;
+ 
+ 	/* prepare command */
+@@ -849,7 +866,7 @@ int dpsw_vlan_remove_if(struct fsl_mc_io *mc_io,
+ 			u16 vlan_id,
+ 			const struct dpsw_vlan_if_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_vlan_manage_if *cmd_params;
+ 
+ 	/* prepare command */
+@@ -885,7 +902,7 @@ int dpsw_vlan_remove_if_untagged(struct fsl_mc_io *mc_io,
+ 				 u16 vlan_id,
+ 				 const struct dpsw_vlan_if_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_vlan_manage_if *cmd_params;
+ 
+ 	/* prepare command */
+@@ -914,7 +931,7 @@ int dpsw_vlan_remove(struct fsl_mc_io *mc_io,
+ 		     u16 token,
+ 		     u16 vlan_id)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_vlan_remove *cmd_params;
+ 
+ 	/* prepare command */
+@@ -944,7 +961,7 @@ int dpsw_fdb_add_unicast(struct fsl_mc_io *mc_io,
+ 			 u16 fdb_id,
+ 			 const struct dpsw_fdb_unicast_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_fdb_unicast_op *cmd_params;
+ 	int i;
+ 
+@@ -979,7 +996,7 @@ int dpsw_fdb_remove_unicast(struct fsl_mc_io *mc_io,
+ 			    u16 fdb_id,
+ 			    const struct dpsw_fdb_unicast_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_fdb_unicast_op *cmd_params;
+ 	int i;
+ 
+@@ -1021,7 +1038,7 @@ int dpsw_fdb_add_multicast(struct fsl_mc_io *mc_io,
+ 			   u16 fdb_id,
+ 			   const struct dpsw_fdb_multicast_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_fdb_multicast_op *cmd_params;
+ 	int i;
+ 
+@@ -1063,7 +1080,7 @@ int dpsw_fdb_remove_multicast(struct fsl_mc_io *mc_io,
+ 			      u16 fdb_id,
+ 			      const struct dpsw_fdb_multicast_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_fdb_multicast_op *cmd_params;
+ 	int i;
+ 
+@@ -1099,7 +1116,7 @@ int dpsw_fdb_set_learning_mode(struct fsl_mc_io *mc_io,
+ 			       u16 fdb_id,
+ 			       enum dpsw_fdb_learning_mode mode)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_cmd_fdb_set_learning_mode *cmd_params;
+ 
+ 	/* prepare command */
+@@ -1128,7 +1145,7 @@ int dpsw_get_api_version(struct fsl_mc_io *mc_io,
+ 			 u16 *major_ver,
+ 			 u16 *minor_ver)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpsw_rsp_get_api_version *rsp_params;
+ 	int err;
+ 
+diff --git a/drivers/staging/fsl-dpaa2/ethsw/dpsw.h b/drivers/staging/fsl-dpaa2/ethsw/dpsw.h
+index 87369a57..8ec199ac 100644
+--- a/drivers/staging/fsl-dpaa2/ethsw/dpsw.h
++++ b/drivers/staging/fsl-dpaa2/ethsw/dpsw.h
+@@ -1,35 +1,10 @@
+-/* Copyright 2013-2016 Freescale Semiconductor Inc.
+- * Copyright 2017 NXP
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Copyright 2013-2016 Freescale Semiconductor, Inc.
++ * Copyright 2017-2018 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *	 notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *	 notice, this list of conditions and the following disclaimer in the
+- *	 documentation and/or other materials provided with the distribution.
+- *     * Neither the name of the above-listed copyright holders nor the
+- *	 names of any contributors may be used to endorse or promote products
+- *	 derived from this software without specific prior written permission.
+- *
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+- * POSSIBILITY OF SUCH DAMAGE.
+  */
++
+ #ifndef __FSL_DPSW_H
+ #define __FSL_DPSW_H
+ 
+@@ -331,6 +306,12 @@ int dpsw_if_set_tci(struct fsl_mc_io *mc_io,
+ 		    u16 if_id,
+ 		    const struct dpsw_tci_cfg *cfg);
+ 
++int dpsw_if_get_tci(struct fsl_mc_io *mc_io,
++		    u32 cmd_flags,
++		    u16 token,
++		    u16 if_id,
++		    struct dpsw_tci_cfg *cfg);
++
+ /**
+  * enum dpsw_stp_state - Spanning Tree Protocol (STP) states
+  * @DPSW_STP_STATE_BLOCKING: Blocking state
+diff --git a/drivers/staging/fsl-dpaa2/ethsw/ethsw.c b/drivers/staging/fsl-dpaa2/ethsw/ethsw.c
+index f9dc627f..5694099a 100644
+--- a/drivers/staging/fsl-dpaa2/ethsw/ethsw.c
++++ b/drivers/staging/fsl-dpaa2/ethsw/ethsw.c
+@@ -38,7 +38,7 @@
+ #include <linux/kthread.h>
+ #include <linux/workqueue.h>
+ 
+-#include "../../fsl-mc/include/mc.h"
++#include <linux/fsl/mc.h>
+ 
+ #include "ethsw.h"
+ 
+@@ -74,14 +74,23 @@ static int ethsw_add_vlan(struct ethsw_core *ethsw, u16 vid)
+ 	return 0;
+ }
+ 
+-static int ethsw_port_set_tci(struct ethsw_port_priv *port_priv,
+-			      struct dpsw_tci_cfg *tci_cfg)
++static int ethsw_port_set_pvid(struct ethsw_port_priv *port_priv, u16 pvid)
+ {
+ 	struct ethsw_core *ethsw = port_priv->ethsw_data;
+ 	struct net_device *netdev = port_priv->netdev;
++	struct dpsw_tci_cfg tci_cfg = { 0 };
+ 	bool is_oper;
+ 	int err, ret;
+ 
++	err = dpsw_if_get_tci(ethsw->mc_io, 0, ethsw->dpsw_handle,
++			      port_priv->idx, &tci_cfg);
++	if (err) {
++		netdev_err(netdev, "dpsw_if_get_tci err %d\n", err);
++		return err;
++	}
++
++	tci_cfg.vlan_id = pvid;
++
+ 	/* Interface needs to be down to change PVID */
+ 	is_oper = netif_oper_up(netdev);
+ 	if (is_oper) {
+@@ -95,17 +104,16 @@ static int ethsw_port_set_tci(struct ethsw_port_priv *port_priv,
+ 	}
+ 
+ 	err = dpsw_if_set_tci(ethsw->mc_io, 0, ethsw->dpsw_handle,
+-			      port_priv->idx, tci_cfg);
++			      port_priv->idx, &tci_cfg);
+ 	if (err) {
+ 		netdev_err(netdev, "dpsw_if_set_tci err %d\n", err);
+ 		goto set_tci_error;
+ 	}
+ 
+ 	/* Delete previous PVID info and mark the new one */
+-	if (port_priv->pvid)
+-		port_priv->vlans[port_priv->pvid] &= ~ETHSW_VLAN_PVID;
+-	port_priv->vlans[tci_cfg->vlan_id] |= ETHSW_VLAN_PVID;
+-	port_priv->pvid = tci_cfg->vlan_id;
++	port_priv->vlans[port_priv->pvid] &= ~ETHSW_VLAN_PVID;
++	port_priv->vlans[pvid] |= ETHSW_VLAN_PVID;
++	port_priv->pvid = pvid;
+ 
+ set_tci_error:
+ 	if (is_oper) {
+@@ -157,13 +165,7 @@ static int ethsw_port_add_vlan(struct ethsw_port_priv *port_priv,
+ 	}
+ 
+ 	if (flags & BRIDGE_VLAN_INFO_PVID) {
+-		struct dpsw_tci_cfg tci_cfg = {
+-			.pcp = 0,
+-			.dei = 0,
+-			.vlan_id = vid,
+-		};
+-
+-		err = ethsw_port_set_tci(port_priv, &tci_cfg);
++		err = ethsw_port_set_pvid(port_priv, vid);
+ 		if (err)
+ 			return err;
+ 	}
+@@ -535,6 +537,8 @@ static const struct net_device_ops ethsw_port_ops = {
+ 	.ndo_change_mtu		= port_change_mtu,
+ 	.ndo_has_offload_stats	= port_has_offload_stats,
+ 	.ndo_get_offload_stats	= port_get_offload_stats,
++	.ndo_bridge_setlink     = switchdev_port_bridge_setlink,
++	.ndo_bridge_getlink     = switchdev_port_bridge_getlink,
+ 
+ 	.ndo_start_xmit		= port_dropframe,
+ };
+@@ -705,11 +709,11 @@ static int port_attr_br_flags_set(struct net_device *netdev,
+ 		return 0;
+ 
+ 	/* Learning is enabled per switch */
+-	err = ethsw_set_learning(port_priv->ethsw_data, flags & BR_LEARNING);
++	err = ethsw_set_learning(port_priv->ethsw_data, !!(flags & BR_LEARNING));
+ 	if (err)
+ 		goto exit;
+ 
+-	err = ethsw_port_set_flood(port_priv, flags & BR_FLOOD);
++	err = ethsw_port_set_flood(port_priv, !!(flags & BR_FLOOD));
+ 
+ exit:
+ 	return err;
+@@ -799,9 +803,7 @@ static int ethsw_port_del_vlan(struct ethsw_port_priv *port_priv, u16 vid)
+ 		return -ENOENT;
+ 
+ 	if (port_priv->vlans[vid] & ETHSW_VLAN_PVID) {
+-		struct dpsw_tci_cfg tci_cfg = { 0 };
+-
+-		err = ethsw_port_set_tci(port_priv, &tci_cfg);
++		err = ethsw_port_set_pvid(port_priv, 0);
+ 		if (err)
+ 			return err;
+ 	}
+@@ -1188,7 +1190,6 @@ static int ethsw_port_init(struct ethsw_port_priv *port_priv, u16 port)
+ 	const char def_mcast[ETH_ALEN] = {0x01, 0x00, 0x5e, 0x00, 0x00, 0x01};
+ 	struct net_device *netdev = port_priv->netdev;
+ 	struct ethsw_core *ethsw = port_priv->ethsw_data;
+-	struct dpsw_tci_cfg tci_cfg = {0};
+ 	struct dpsw_vlan_if_cfg vcfg;
+ 	int err;
+ 
+@@ -1206,7 +1207,7 @@ static int ethsw_port_init(struct ethsw_port_priv *port_priv, u16 port)
+ 		return err;
+ 	}
+ 
+-	err = ethsw_port_set_tci(port_priv, &tci_cfg);
++	err = ethsw_port_set_pvid(port_priv, 0);
+ 	if (err)
+ 		return err;
+ 
+diff --git a/drivers/staging/fsl-dpaa2/evb/dpdmux.c b/drivers/staging/fsl-dpaa2/evb/dpdmux.c
+index f7a87633..4c4a059e 100644
+--- a/drivers/staging/fsl-dpaa2/evb/dpdmux.c
++++ b/drivers/staging/fsl-dpaa2/evb/dpdmux.c
+@@ -29,8 +29,7 @@
+  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+  * POSSIBILITY OF SUCH DAMAGE.
+  */
+-#include "../../fsl-mc/include/mc-sys.h"
+-#include "../../fsl-mc/include/mc-cmd.h"
++#include <linux/fsl/mc.h>
+ #include "dpdmux.h"
+ #include "dpdmux-cmd.h"
+ 
+@@ -56,7 +55,7 @@ int dpdmux_open(struct fsl_mc_io *mc_io,
+ 		int dpdmux_id,
+ 		u16 *token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_open *cmd_params;
+ 	int err;
+ 
+@@ -93,7 +92,7 @@ int dpdmux_close(struct fsl_mc_io *mc_io,
+ 		 u32 cmd_flags,
+ 		 u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_CLOSE,
+@@ -132,7 +131,7 @@ int dpdmux_create(struct fsl_mc_io *mc_io,
+ 		  const struct dpdmux_cfg *cfg,
+ 		  u32 *obj_id)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_create *cmd_params;
+ 	int err;
+ 
+@@ -181,7 +180,7 @@ int dpdmux_destroy(struct fsl_mc_io *mc_io,
+ 		   u32 cmd_flags,
+ 		   u32 object_id)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_destroy *cmd_params;
+ 
+ 	/* prepare command */
+@@ -207,7 +206,7 @@ int dpdmux_enable(struct fsl_mc_io *mc_io,
+ 		  u32 cmd_flags,
+ 		  u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_ENABLE,
+@@ -230,7 +229,7 @@ int dpdmux_disable(struct fsl_mc_io *mc_io,
+ 		   u32 cmd_flags,
+ 		   u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_DISABLE,
+@@ -255,7 +254,7 @@ int dpdmux_is_enabled(struct fsl_mc_io *mc_io,
+ 		      u16 token,
+ 		      int *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_rsp_is_enabled *rsp_params;
+ 	int err;
+ 
+@@ -288,7 +287,7 @@ int dpdmux_reset(struct fsl_mc_io *mc_io,
+ 		 u32 cmd_flags,
+ 		 u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_RESET,
+@@ -320,7 +319,7 @@ int dpdmux_set_irq_enable(struct fsl_mc_io *mc_io,
+ 			  u8 irq_index,
+ 			  u8 en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_set_irq_enable *cmd_params;
+ 
+ 	/* prepare command */
+@@ -351,7 +350,7 @@ int dpdmux_get_irq_enable(struct fsl_mc_io *mc_io,
+ 			  u8 irq_index,
+ 			  u8 *en)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_get_irq_enable *cmd_params;
+ 	struct dpdmux_rsp_get_irq_enable *rsp_params;
+ 	int err;
+@@ -397,7 +396,7 @@ int dpdmux_set_irq_mask(struct fsl_mc_io *mc_io,
+ 			u8 irq_index,
+ 			u32 mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_set_irq_mask *cmd_params;
+ 
+ 	/* prepare command */
+@@ -431,7 +430,7 @@ int dpdmux_get_irq_mask(struct fsl_mc_io *mc_io,
+ 			u8 irq_index,
+ 			u32 *mask)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_get_irq_mask *cmd_params;
+ 	struct dpdmux_rsp_get_irq_mask *rsp_params;
+ 	int err;
+@@ -473,7 +472,7 @@ int dpdmux_get_irq_status(struct fsl_mc_io *mc_io,
+ 			  u8 irq_index,
+ 			  u32 *status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_get_irq_status *cmd_params;
+ 	struct dpdmux_rsp_get_irq_status *rsp_params;
+ 	int err;
+@@ -516,7 +515,7 @@ int dpdmux_clear_irq_status(struct fsl_mc_io *mc_io,
+ 			    u8 irq_index,
+ 			    u32 status)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_clear_irq_status *cmd_params;
+ 
+ 	/* prepare command */
+@@ -545,7 +544,7 @@ int dpdmux_get_attributes(struct fsl_mc_io *mc_io,
+ 			  u16 token,
+ 			  struct dpdmux_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_rsp_get_attr *rsp_params;
+ 	int err;
+ 
+@@ -586,7 +585,7 @@ int dpdmux_if_enable(struct fsl_mc_io *mc_io,
+ 		     u16 if_id)
+ {
+ 	struct dpdmux_cmd_if *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_IF_ENABLE,
+@@ -614,7 +613,7 @@ int dpdmux_if_disable(struct fsl_mc_io *mc_io,
+ 		      u16 if_id)
+ {
+ 	struct dpdmux_cmd_if *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_IF_DISABLE,
+@@ -646,7 +645,7 @@ int dpdmux_set_max_frame_length(struct fsl_mc_io *mc_io,
+ 				u16 token,
+ 				u16 max_frame_length)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_set_max_frame_length *cmd_params;
+ 
+ 	/* prepare command */
+@@ -672,7 +671,7 @@ int dpdmux_ul_reset_counters(struct fsl_mc_io *mc_io,
+ 			     u32 cmd_flags,
+ 			     u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_UL_RESET_COUNTERS,
+@@ -706,7 +705,7 @@ int dpdmux_if_set_accepted_frames(struct fsl_mc_io *mc_io,
+ 				  u16 if_id,
+ 				  const struct dpdmux_accepted_frames *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_if_set_accepted_frames *cmd_params;
+ 
+ 	/* prepare command */
+@@ -740,7 +739,7 @@ int dpdmux_if_get_attributes(struct fsl_mc_io *mc_io,
+ 			     u16 if_id,
+ 			     struct dpdmux_if_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_if *cmd_params;
+ 	struct dpdmux_rsp_if_get_attr *rsp_params;
+ 	int err;
+@@ -787,7 +786,7 @@ int dpdmux_if_remove_l2_rule(struct fsl_mc_io *mc_io,
+ 			     u16 if_id,
+ 			     const struct dpdmux_l2_rule *rule)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_if_l2_rule *cmd_params;
+ 
+ 	/* prepare command */
+@@ -827,7 +826,7 @@ int dpdmux_if_add_l2_rule(struct fsl_mc_io *mc_io,
+ 			  u16 if_id,
+ 			  const struct dpdmux_l2_rule *rule)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_if_l2_rule *cmd_params;
+ 
+ 	/* prepare command */
+@@ -866,7 +865,7 @@ int dpdmux_if_get_counter(struct fsl_mc_io *mc_io,
+ 			  enum dpdmux_counter_type counter_type,
+ 			  u64 *counter)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_if_get_counter *cmd_params;
+ 	struct dpdmux_rsp_if_get_counter *rsp_params;
+ 	int err;
+@@ -907,7 +906,7 @@ int dpdmux_if_set_link_cfg(struct fsl_mc_io *mc_io,
+ 			   u16 if_id,
+ 			   struct dpdmux_link_cfg *cfg)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_if_set_link_cfg *cmd_params;
+ 
+ 	/* prepare command */
+@@ -939,7 +938,7 @@ int dpdmux_if_get_link_state(struct fsl_mc_io *mc_io,
+ 			     u16 if_id,
+ 			     struct dpdmux_link_state *state)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_cmd_if_get_link_state *cmd_params;
+ 	struct dpdmux_rsp_if_get_link_state *rsp_params;
+ 	int err;
+@@ -990,7 +989,7 @@ int dpdmux_set_custom_key(struct fsl_mc_io *mc_io,
+ 			  u64 key_cfg_iova)
+ {
+ 	struct dpdmux_set_custom_key *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_SET_CUSTOM_KEY,
+@@ -1026,7 +1025,7 @@ int dpdmux_add_custom_cls_entry(struct fsl_mc_io *mc_io,
+ 				struct dpdmux_cls_action *action)
+ {
+ 	struct dpdmux_cmd_add_custom_cls_entry *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_ADD_CUSTOM_CLS_ENTRY,
+@@ -1063,7 +1062,7 @@ int dpdmux_remove_custom_cls_entry(struct fsl_mc_io *mc_io,
+ 				   struct dpdmux_rule_cfg *rule)
+ {
+ 	struct dpdmux_cmd_remove_custom_cls_entry *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPDMUX_CMDID_REMOVE_CUSTOM_CLS_ENTRY,
+@@ -1092,7 +1091,7 @@ int dpdmux_get_api_version(struct fsl_mc_io *mc_io,
+ 			   u16 *major_ver,
+ 			   u16 *minor_ver)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpdmux_rsp_get_api_version *rsp_params;
+ 	int err;
+ 
+diff --git a/drivers/staging/fsl-dpaa2/evb/evb.c b/drivers/staging/fsl-dpaa2/evb/evb.c
+index d9df1796..37612db8 100644
+--- a/drivers/staging/fsl-dpaa2/evb/evb.c
++++ b/drivers/staging/fsl-dpaa2/evb/evb.c
+@@ -38,7 +38,7 @@
+ #include <uapi/linux/if_bridge.h>
+ #include <net/netlink.h>
+ 
+-#include "../../fsl-mc/include/mc.h"
++#include <linux/fsl/mc.h>
+ 
+ #include "dpdmux.h"
+ #include "dpdmux-cmd.h"
+@@ -1202,11 +1202,16 @@ static int evb_probe(struct fsl_mc_device *evb_dev)
+ 
+ 	priv = netdev_priv(netdev);
+ 
+-	err = fsl_mc_portal_allocate(evb_dev, 0, &priv->mc_io);
+-	if (unlikely(err)) {
+-		dev_err(dev, "fsl_mc_portal_allocate err %d\n", err);
++	err = fsl_mc_portal_allocate(evb_dev, FSL_MC_IO_ATOMIC_CONTEXT_PORTAL,
++				     &priv->mc_io);
++	if (err) {
++		if (err == -ENXIO)
++			err = -EPROBE_DEFER;
++		else
++			dev_err(dev, "fsl_mc_portal_allocate err %d\n", err);
+ 		goto err_free_netdev;
+ 	}
++
+ 	if (!priv->mc_io) {
+ 		dev_err(dev, "fsl_mc_portal_allocate returned null handle but no error\n");
+ 		err = -EFAULT;
+diff --git a/drivers/staging/fsl-dpaa2/mac/dpmac.c b/drivers/staging/fsl-dpaa2/mac/dpmac.c
+index f7827423..9ca830dd 100644
+--- a/drivers/staging/fsl-dpaa2/mac/dpmac.c
++++ b/drivers/staging/fsl-dpaa2/mac/dpmac.c
+@@ -29,8 +29,7 @@
+  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+  * POSSIBILITY OF SUCH DAMAGE.
+  */
+-#include "../../fsl-mc/include/mc-sys.h"
+-#include "../../fsl-mc/include/mc-cmd.h"
++#include <linux/fsl/mc.h>
+ #include "dpmac.h"
+ #include "dpmac-cmd.h"
+ 
+@@ -57,7 +56,7 @@ int dpmac_open(struct fsl_mc_io *mc_io,
+ 	       u16 *token)
+ {
+ 	struct dpmac_cmd_open *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -93,7 +92,7 @@ int dpmac_close(struct fsl_mc_io *mc_io,
+ 		u32 cmd_flags,
+ 		u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPMAC_CMDID_CLOSE, cmd_flags,
+@@ -129,7 +128,7 @@ int dpmac_create(struct fsl_mc_io *mc_io,
+ 		 u32 *obj_id)
+ {
+ 	struct dpmac_cmd_create *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -171,7 +170,7 @@ int dpmac_destroy(struct fsl_mc_io *mc_io,
+ 		  u32 object_id)
+ {
+ 	struct dpmac_cmd_destroy *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPMAC_CMDID_DESTROY,
+@@ -206,7 +205,7 @@ int dpmac_set_irq_enable(struct fsl_mc_io *mc_io,
+ 			 u8 en)
+ {
+ 	struct dpmac_cmd_set_irq_enable *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPMAC_CMDID_SET_IRQ_ENABLE,
+@@ -238,7 +237,7 @@ int dpmac_get_irq_enable(struct fsl_mc_io *mc_io,
+ {
+ 	struct dpmac_cmd_get_irq_enable *cmd_params;
+ 	struct dpmac_rsp_get_irq_enable *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -283,7 +282,7 @@ int dpmac_set_irq_mask(struct fsl_mc_io *mc_io,
+ 		       u32 mask)
+ {
+ 	struct dpmac_cmd_set_irq_mask *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPMAC_CMDID_SET_IRQ_MASK,
+@@ -318,7 +317,7 @@ int dpmac_get_irq_mask(struct fsl_mc_io *mc_io,
+ {
+ 	struct dpmac_cmd_get_irq_mask *cmd_params;
+ 	struct dpmac_rsp_get_irq_mask *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -361,7 +360,7 @@ int dpmac_get_irq_status(struct fsl_mc_io *mc_io,
+ {
+ 	struct dpmac_cmd_get_irq_status *cmd_params;
+ 	struct dpmac_rsp_get_irq_status *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -404,7 +403,7 @@ int dpmac_clear_irq_status(struct fsl_mc_io *mc_io,
+ 			   u32 status)
+ {
+ 	struct dpmac_cmd_clear_irq_status *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPMAC_CMDID_CLEAR_IRQ_STATUS,
+@@ -434,7 +433,7 @@ int dpmac_get_attributes(struct fsl_mc_io *mc_io,
+ 			 struct dpmac_attr *attr)
+ {
+ 	struct dpmac_rsp_get_attributes *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -472,7 +471,7 @@ int dpmac_get_link_cfg(struct fsl_mc_io *mc_io,
+ 		       struct dpmac_link_cfg *cfg)
+ {
+ 	struct dpmac_rsp_get_link_cfg *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err = 0;
+ 
+ 	/* prepare command */
+@@ -507,7 +506,7 @@ int dpmac_set_link_state(struct fsl_mc_io *mc_io,
+ 			 struct dpmac_link_state *link_state)
+ {
+ 	struct dpmac_cmd_set_link_state *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPMAC_CMDID_SET_LINK_STATE,
+@@ -540,7 +539,7 @@ int dpmac_get_counter(struct fsl_mc_io *mc_io,
+ {
+ 	struct dpmac_cmd_get_counter *dpmac_cmd;
+ 	struct dpmac_rsp_get_counter *dpmac_rsp;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err = 0;
+ 
+ 	/* prepare command */
+@@ -568,7 +567,7 @@ int dpmac_set_port_mac_addr(struct fsl_mc_io *mc_io,
+ 			    const u8 addr[6])
+ {
+ 	struct dpmac_cmd_set_port_mac_addr *dpmac_cmd;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPMAC_CMDID_SET_PORT_MAC_ADDR,
+@@ -601,7 +600,7 @@ int dpmac_get_api_version(struct fsl_mc_io *mc_io,
+ 			  u16 *minor_ver)
+ {
+ 	struct dpmac_rsp_get_api_version *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	cmd.header = mc_encode_cmd_header(DPMAC_CMDID_GET_API_VERSION,
+diff --git a/drivers/staging/fsl-dpaa2/mac/mac.c b/drivers/staging/fsl-dpaa2/mac/mac.c
+index 3ddf3448..8d7e0439 100644
+--- a/drivers/staging/fsl-dpaa2/mac/mac.c
++++ b/drivers/staging/fsl-dpaa2/mac/mac.c
+@@ -46,8 +46,7 @@
+ #include <linux/phy.h>
+ #include <linux/phy_fixed.h>
+ 
+-#include "../../fsl-mc/include/mc.h"
+-#include "../../fsl-mc/include/mc-sys.h"
++#include <linux/fsl/mc.h>
+ 
+ #include "dpmac.h"
+ #include "dpmac-cmd.h"
+@@ -152,16 +151,18 @@ static netdev_tx_t dpaa2_mac_drop_frame(struct sk_buff *skb,
+ 	return NETDEV_TX_OK;
+ }
+ 
+-static int dpaa2_mac_get_settings(struct net_device *netdev,
+-				  struct ethtool_cmd *cmd)
++static int dpaa2_mac_get_link_ksettings(struct net_device *netdev,
++					struct ethtool_link_ksettings *ks)
+ {
+-	return phy_ethtool_gset(netdev->phydev, cmd);
++	phy_ethtool_ksettings_get(netdev->phydev, ks);
++
++	return 0;
+ }
+ 
+-static int dpaa2_mac_set_settings(struct net_device *netdev,
+-				  struct ethtool_cmd *cmd)
++static int dpaa2_mac_set_link_ksettings(struct net_device *netdev,
++					const struct ethtool_link_ksettings *ks)
+ {
+-	return phy_ethtool_sset(netdev->phydev, cmd);
++	return phy_ethtool_ksettings_set(netdev->phydev, ks);
+ }
+ 
+ static void dpaa2_mac_get_stats(struct net_device *netdev,
+@@ -319,8 +320,8 @@ static const struct net_device_ops dpaa2_mac_ndo_ops = {
+ };
+ 
+ static const struct ethtool_ops dpaa2_mac_ethtool_ops = {
+-	.get_settings		= &dpaa2_mac_get_settings,
+-	.set_settings		= &dpaa2_mac_set_settings,
++	.get_link_ksettings	= &dpaa2_mac_get_link_ksettings,
++	.set_link_ksettings	= &dpaa2_mac_set_link_ksettings,
+ 	.get_strings		= &dpaa2_mac_get_strings,
+ 	.get_ethtool_stats	= &dpaa2_mac_get_ethtool_stats,
+ 	.get_sset_count		= &dpaa2_mac_get_sset_count,
+@@ -489,10 +490,12 @@ static int dpaa2_mac_probe(struct fsl_mc_device *mc_dev)
+ 
+ 	dev_set_drvdata(dev, priv);
+ 
+-	err = fsl_mc_portal_allocate(mc_dev, 0, &mc_dev->mc_io);
++	/* We may need to issue MC commands while in atomic context */
++	err = fsl_mc_portal_allocate(mc_dev, FSL_MC_IO_ATOMIC_CONTEXT_PORTAL,
++				     &mc_dev->mc_io);
+ 	if (err || !mc_dev->mc_io) {
+-		dev_err(dev, "fsl_mc_portal_allocate error: %d\n", err);
+-		err = -ENODEV;
++		dev_dbg(dev, "fsl_mc_portal_allocate error: %d\n", err);
++		err = -EPROBE_DEFER;
+ 		goto err_free_netdev;
+ 	}
+ 
+diff --git a/drivers/staging/fsl-dpaa2/rtc/dprtc.c b/drivers/staging/fsl-dpaa2/rtc/dprtc.c
+index 399177e4..a787e647 100644
+--- a/drivers/staging/fsl-dpaa2/rtc/dprtc.c
++++ b/drivers/staging/fsl-dpaa2/rtc/dprtc.c
+@@ -29,8 +29,8 @@
+  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+  * POSSIBILITY OF SUCH DAMAGE.
+  */
+-#include "../../fsl-mc/include/mc-sys.h"
+-#include "../../fsl-mc/include/mc-cmd.h"
++#include <linux/fsl/mc.h>
++
+ #include "dprtc.h"
+ #include "dprtc-cmd.h"
+ 
+@@ -57,7 +57,7 @@ int dprtc_open(struct fsl_mc_io *mc_io,
+ 	       uint16_t *token)
+ {
+ 	struct dprtc_cmd_open *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -93,7 +93,7 @@ int dprtc_close(struct fsl_mc_io *mc_io,
+ 		uint32_t cmd_flags,
+ 		uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_CLOSE, cmd_flags,
+@@ -128,7 +128,7 @@ int dprtc_create(struct fsl_mc_io *mc_io,
+ 		 const struct dprtc_cfg *cfg,
+ 		 uint32_t *obj_id)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	(void)(cfg); /* unused */
+@@ -170,7 +170,7 @@ int dprtc_destroy(struct fsl_mc_io *mc_io,
+ 		  uint32_t object_id)
+ {
+ 	struct dprtc_cmd_destroy *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_DESTROY,
+@@ -187,7 +187,7 @@ int dprtc_enable(struct fsl_mc_io *mc_io,
+ 		 uint32_t cmd_flags,
+ 		 uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_ENABLE, cmd_flags,
+@@ -201,7 +201,7 @@ int dprtc_disable(struct fsl_mc_io *mc_io,
+ 		  uint32_t cmd_flags,
+ 		  uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_DISABLE,
+@@ -218,7 +218,7 @@ int dprtc_is_enabled(struct fsl_mc_io *mc_io,
+ 		     int *en)
+ {
+ 	struct dprtc_rsp_is_enabled *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -241,7 +241,7 @@ int dprtc_reset(struct fsl_mc_io *mc_io,
+ 		uint32_t cmd_flags,
+ 		uint16_t token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_RESET,
+@@ -274,7 +274,7 @@ int dprtc_set_irq_enable(struct fsl_mc_io *mc_io,
+ 			 uint8_t en)
+ {
+ 	struct dprtc_cmd_set_irq_enable *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_SET_IRQ_ENABLE,
+@@ -306,7 +306,7 @@ int dprtc_get_irq_enable(struct fsl_mc_io *mc_io,
+ {
+ 	struct dprtc_rsp_get_irq_enable *rsp_params;
+ 	struct dprtc_cmd_get_irq *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -351,7 +351,7 @@ int dprtc_set_irq_mask(struct fsl_mc_io *mc_io,
+ 		       uint32_t mask)
+ {
+ 	struct dprtc_cmd_set_irq_mask *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_SET_IRQ_MASK,
+@@ -386,7 +386,7 @@ int dprtc_get_irq_mask(struct fsl_mc_io *mc_io,
+ {
+ 	struct dprtc_rsp_get_irq_mask *rsp_params;
+ 	struct dprtc_cmd_get_irq *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -429,7 +429,7 @@ int dprtc_get_irq_status(struct fsl_mc_io *mc_io,
+ {
+ 	struct dprtc_cmd_get_irq_status *cmd_params;
+ 	struct dprtc_rsp_get_irq_status *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -472,7 +472,7 @@ int dprtc_clear_irq_status(struct fsl_mc_io *mc_io,
+ 			   uint32_t status)
+ {
+ 	struct dprtc_cmd_clear_irq_status *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_CLEAR_IRQ_STATUS,
+@@ -502,7 +502,7 @@ int dprtc_get_attributes(struct fsl_mc_io *mc_io,
+ 			 struct dprtc_attr *attr)
+ {
+ 	struct dprtc_rsp_get_attributes *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -539,7 +539,7 @@ int dprtc_set_clock_offset(struct fsl_mc_io *mc_io,
+ 			   int64_t offset)
+ {
+ 	struct dprtc_cmd_set_clock_offset *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_SET_CLOCK_OFFSET,
+@@ -568,7 +568,7 @@ int dprtc_set_freq_compensation(struct fsl_mc_io *mc_io,
+ 				uint32_t freq_compensation)
+ {
+ 	struct dprtc_get_freq_compensation *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_SET_FREQ_COMPENSATION,
+@@ -597,7 +597,7 @@ int dprtc_get_freq_compensation(struct fsl_mc_io *mc_io,
+ 				uint32_t *freq_compensation)
+ {
+ 	struct dprtc_get_freq_compensation *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -633,7 +633,7 @@ int dprtc_get_time(struct fsl_mc_io *mc_io,
+ 		   uint64_t *time)
+ {
+ 	struct dprtc_time *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -669,7 +669,7 @@ int dprtc_set_time(struct fsl_mc_io *mc_io,
+ 		   uint64_t time)
+ {
+ 	struct dprtc_time *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_SET_TIME,
+@@ -699,7 +699,7 @@ int dprtc_set_alarm(struct fsl_mc_io *mc_io,
+ 		    uint16_t token, uint64_t time)
+ {
+ 	struct dprtc_time *cmd_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_SET_ALARM,
+@@ -727,7 +727,7 @@ int dprtc_get_api_version(struct fsl_mc_io *mc_io,
+ 			  uint16_t *minor_ver)
+ {
+ 	struct dprtc_rsp_get_api_version *rsp_params;
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	cmd.header = mc_encode_cmd_header(DPRTC_CMDID_GET_API_VERSION,
+diff --git a/drivers/staging/fsl-dpaa2/rtc/rtc.c b/drivers/staging/fsl-dpaa2/rtc/rtc.c
+index 0afc6538..ab7f5c3d 100644
+--- a/drivers/staging/fsl-dpaa2/rtc/rtc.c
++++ b/drivers/staging/fsl-dpaa2/rtc/rtc.c
+@@ -33,8 +33,7 @@
+ #include <linux/module.h>
+ #include <linux/ptp_clock_kernel.h>
+ 
+-#include "../../fsl-mc/include/mc.h"
+-#include "../../fsl-mc/include/mc-sys.h"
++#include <linux/fsl/mc.h>
+ 
+ #include "dprtc.h"
+ #include "dprtc-cmd.h"
+diff --git a/drivers/staging/fsl-mc/Kconfig b/drivers/staging/fsl-mc/Kconfig
+index 32df07b1..3002229b 100644
+--- a/drivers/staging/fsl-mc/Kconfig
++++ b/drivers/staging/fsl-mc/Kconfig
+@@ -1 +1,2 @@
++# SPDX-License-Identifier: GPL-2.0
+ source "drivers/staging/fsl-mc/bus/Kconfig"
+diff --git a/drivers/staging/fsl-mc/Makefile b/drivers/staging/fsl-mc/Makefile
+index 9c6a0012..14683889 100644
+--- a/drivers/staging/fsl-mc/Makefile
++++ b/drivers/staging/fsl-mc/Makefile
+@@ -1,2 +1,3 @@
++# SPDX-License-Identifier: GPL-2.0
+ # Freescale Management Complex (MC) bus drivers
+ obj-$(CONFIG_FSL_MC_BUS)	+= bus/
+diff --git a/drivers/staging/fsl-mc/bus/Kconfig b/drivers/staging/fsl-mc/bus/Kconfig
+index 67847c0e..58fef92c 100644
+--- a/drivers/staging/fsl-mc/bus/Kconfig
++++ b/drivers/staging/fsl-mc/bus/Kconfig
+@@ -1,24 +1,13 @@
++# SPDX-License-Identifier: GPL-2.0
+ #
+ # DPAA2 fsl-mc bus
+ #
+ # Copyright (C) 2014-2016 Freescale Semiconductor, Inc.
+ #
+-# This file is released under the GPLv2
+-#
+-
+-config FSL_MC_BUS
+-	bool "QorIQ DPAA2 fsl-mc bus driver"
+-	depends on OF && ARCH_LAYERSCAPE
+-	select GENERIC_MSI_IRQ_DOMAIN
+-	help
+-	  Driver to enable the bus infrastructure for the QorIQ DPAA2
+-	  architecture.  The fsl-mc bus driver handles discovery of
+-	  DPAA2 objects (which are represented as Linux devices) and
+-	  binding objects to drivers.
+ 
+ config FSL_MC_DPIO
+         tristate "QorIQ DPAA2 DPIO driver"
+-        depends on FSL_MC_BUS
++	depends on FSL_MC_BUS
+         help
+ 	  Driver for the DPAA2 DPIO object.  A DPIO provides queue and
+ 	  buffer management facilities for software to interact with
+@@ -31,10 +20,3 @@ config FSL_QBMAN_DEBUG
+ 	depends on FSL_MC_DPIO
+ 	help
+ 	  QBMan debug assistant APIs.
+-
+-config FSL_MC_RESTOOL
+-        tristate "Freescale Management Complex (MC) restool driver"
+-        depends on FSL_MC_BUS
+-        help
+-          Driver that provides kernel support for the Freescale Management
+-          Complex resource manager user-space tool.
+diff --git a/drivers/staging/fsl-mc/bus/Makefile b/drivers/staging/fsl-mc/bus/Makefile
+index e7e2239c..21d8ebc8 100644
+--- a/drivers/staging/fsl-mc/bus/Makefile
++++ b/drivers/staging/fsl-mc/bus/Makefile
+@@ -1,28 +1,9 @@
++# SPDX-License-Identifier: GPL-2.0
+ #
+ # Freescale Management Complex (MC) bus drivers
+ #
+ # Copyright (C) 2014 Freescale Semiconductor, Inc.
+ #
+-# This file is released under the GPLv2
+-#
+-obj-$(CONFIG_FSL_MC_BUS) += mc-bus-driver.o
+-
+-mc-bus-driver-objs := fsl-mc-bus.o \
+-		      mc-sys.o \
+-		      mc-io.o \
+-		      dprc.o \
+-		      dpmng.o \
+-		      dprc-driver.o \
+-		      fsl-mc-allocator.o \
+-		      fsl-mc-msi.o \
+-		      irq-gic-v3-its-fsl-mc-msi.o \
+-		      dpmcp.o \
+-		      dpbp.o \
+-		      dpcon.o \
+-		      fsl-mc-iommu.o
+ 
+ # MC DPIO driver
+ obj-$(CONFIG_FSL_MC_DPIO) += dpio/
+-
+-# MC restool kernel support
+-obj-$(CONFIG_FSL_MC_RESTOOL) += mc-restool.o
+diff --git a/drivers/staging/fsl-mc/bus/dpio/Makefile b/drivers/staging/fsl-mc/bus/dpio/Makefile
+index 837d3303..b9ff24c7 100644
+--- a/drivers/staging/fsl-mc/bus/dpio/Makefile
++++ b/drivers/staging/fsl-mc/bus/dpio/Makefile
+@@ -1,9 +1,8 @@
++# SPDX-License-Identifier: GPL-2.0
+ #
+ # QorIQ DPAA2 DPIO driver
+ #
+ 
+-subdir-ccflags-y := -Werror
+-
+ obj-$(CONFIG_FSL_MC_DPIO) += fsl-mc-dpio.o
+ 
+ fsl-mc-dpio-objs := dpio.o qbman-portal.o dpio-service.o dpio-driver.o
+diff --git a/drivers/staging/fsl-mc/bus/dpio/dpio-cmd.h b/drivers/staging/fsl-mc/bus/dpio/dpio-cmd.h
+index 9d0bdeae..5814d2f3 100644
+--- a/drivers/staging/fsl-mc/bus/dpio/dpio-cmd.h
++++ b/drivers/staging/fsl-mc/bus/dpio/dpio-cmd.h
+@@ -1,34 +1,8 @@
++/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+ /*
+  * Copyright 2013-2016 Freescale Semiconductor Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- * * Redistributions of source code must retain the above copyright
+- * notice, this list of conditions and the following disclaimer.
+- * * Redistributions in binary form must reproduce the above copyright
+- * notice, this list of conditions and the following disclaimer in the
+- * documentation and/or other materials provided with the distribution.
+- * * Neither the name of the above-listed copyright holders nor the
+- * names of any contributors may be used to endorse or promote products
+- * derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+- * POSSIBILITY OF SUCH DAMAGE.
+  */
+ #ifndef _FSL_DPIO_CMD_H
+ #define _FSL_DPIO_CMD_H
+diff --git a/drivers/staging/fsl-mc/bus/dpio/dpio-driver.c b/drivers/staging/fsl-mc/bus/dpio/dpio-driver.c
+index 1c407473..a85bc5f6 100644
+--- a/drivers/staging/fsl-mc/bus/dpio/dpio-driver.c
++++ b/drivers/staging/fsl-mc/bus/dpio/dpio-driver.c
+@@ -1,33 +1,8 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * Copyright 2014-2016 Freescale Semiconductor Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *	 notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *	 notice, this list of conditions and the following disclaimer in the
+- *	 documentation and/or other materials provided with the distribution.
+- *     * Neither the name of Freescale Semiconductor nor the
+- *	 names of its contributors may be used to endorse or promote products
+- *	 derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+  */
+ 
+ #include <linux/types.h>
+@@ -39,7 +14,7 @@
+ #include <linux/dma-mapping.h>
+ #include <linux/delay.h>
+ 
+-#include "../../include/mc.h"
++#include <linux/fsl/mc.h>
+ #include "../../include/dpaa2-io.h"
+ 
+ #include "qbman-portal.h"
+@@ -114,6 +89,7 @@ static int dpaa2_dpio_probe(struct fsl_mc_device *dpio_dev)
+ 	int err = -ENOMEM;
+ 	struct device *dev = &dpio_dev->dev;
+ 	static int next_cpu = -1;
++	int possible_next_cpu;
+ 
+ 	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+ 	if (!priv)
+@@ -162,16 +138,16 @@ static int dpaa2_dpio_probe(struct fsl_mc_device *dpio_dev)
+ 
+ 	/* get the cpu to use for the affinity hint */
+ 	if (next_cpu == -1)
+-		next_cpu = cpumask_first(cpu_online_mask);
++		possible_next_cpu = cpumask_first(cpu_online_mask);
+ 	else
+-		next_cpu = cpumask_next(next_cpu, cpu_online_mask);
++		possible_next_cpu = cpumask_next(next_cpu, cpu_online_mask);
+ 
+-	if (!cpu_possible(next_cpu)) {
++	if (possible_next_cpu >= nr_cpu_ids) {
+ 		dev_err(dev, "probe failed. Number of DPIOs exceeds NR_CPUS.\n");
+ 		err = -ERANGE;
+ 		goto err_allocate_irqs;
+ 	}
+-	desc.cpu = next_cpu;
++	desc.cpu = next_cpu = possible_next_cpu;
+ 
+ 	/*
+ 	 * Set the CENA regs to be the cache enabled area of the portal to
+diff --git a/drivers/staging/fsl-mc/bus/dpio/dpio-service.c b/drivers/staging/fsl-mc/bus/dpio/dpio-service.c
+index 540887ce..860bca98 100644
+--- a/drivers/staging/fsl-mc/bus/dpio/dpio-service.c
++++ b/drivers/staging/fsl-mc/bus/dpio/dpio-service.c
+@@ -1,36 +1,11 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * Copyright 2014-2016 Freescale Semiconductor Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *	 notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *	 notice, this list of conditions and the following disclaimer in the
+- *	 documentation and/or other materials provided with the distribution.
+- *     * Neither the name of Freescale Semiconductor nor the
+- *	 names of its contributors may be used to endorse or promote products
+- *	 derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+  */
+ #include <linux/types.h>
+-#include "../../include/mc.h"
++#include <linux/fsl/mc.h>
+ #include "../../include/dpaa2-io.h"
+ #include <linux/init.h>
+ #include <linux/module.h>
+@@ -43,7 +18,6 @@
+ #include "qbman-portal.h"
+ 
+ struct dpaa2_io {
+-	atomic_t refs;
+ 	struct dpaa2_io_desc dpio_desc;
+ 	struct qbman_swp_desc swp_desc;
+ 	struct qbman_swp *swp;
+@@ -147,7 +121,6 @@ struct dpaa2_io *dpaa2_io_create(const struct dpaa2_io_desc *desc)
+ 		return NULL;
+ 	}
+ 
+-	atomic_set(&obj->refs, 1);
+ 	obj->dpio_desc = *desc;
+ 	obj->swp_desc.cena_bar = obj->dpio_desc.regs_cena;
+ 	obj->swp_desc.cinh_bar = obj->dpio_desc.regs_cinh;
+@@ -179,7 +152,6 @@ struct dpaa2_io *dpaa2_io_create(const struct dpaa2_io_desc *desc)
+ 
+ 	return obj;
+ }
+-EXPORT_SYMBOL(dpaa2_io_create);
+ 
+ /**
+  * dpaa2_io_down() - release the dpaa2_io object.
+@@ -192,11 +164,8 @@ EXPORT_SYMBOL(dpaa2_io_create);
+  */
+ void dpaa2_io_down(struct dpaa2_io *d)
+ {
+-	if (!atomic_dec_and_test(&d->refs))
+-		return;
+ 	kfree(d);
+ }
+-EXPORT_SYMBOL(dpaa2_io_down);
+ 
+ #define DPAA_POLL_MAX 32
+ 
+@@ -227,7 +196,7 @@ irqreturn_t dpaa2_io_irq(struct dpaa2_io *obj)
+ 			u64 q64;
+ 
+ 			q64 = qbman_result_SCN_ctx(dq);
+-			ctx = (void *)q64;
++			ctx = (void *)(uintptr_t)q64;
+ 			ctx->cb(ctx);
+ 		} else {
+ 			pr_crit("fsl-mc-dpio: Unrecognised/ignored DQRR entry\n");
+@@ -243,7 +212,6 @@ irqreturn_t dpaa2_io_irq(struct dpaa2_io *obj)
+ 	qbman_swp_interrupt_set_inhibit(swp, 0);
+ 	return IRQ_HANDLED;
+ }
+-EXPORT_SYMBOL(dpaa2_io_irq);
+ 
+ /**
+  * dpaa2_io_service_register() - Prepare for servicing of FQDAN or CDAN
+@@ -273,7 +241,7 @@ int dpaa2_io_service_register(struct dpaa2_io *d,
+ 		return -ENODEV;
+ 
+ 	ctx->dpio_id = d->dpio_desc.dpio_id;
+-	ctx->qman64 = (u64)ctx;
++	ctx->qman64 = (u64)(uintptr_t)ctx;
+ 	ctx->dpio_private = d;
+ 	spin_lock_irqsave(&d->lock_notifications, irqflags);
+ 	list_add(&ctx->node, &d->notifications);
+@@ -281,12 +249,12 @@ int dpaa2_io_service_register(struct dpaa2_io *d,
+ 
+ 	/* Enable the generation of CDAN notifications */
+ 	if (ctx->is_cdan)
+-		qbman_swp_CDAN_set_context_enable(d->swp,
+-						  (u16)ctx->id,
+-						  ctx->qman64);
++		return qbman_swp_CDAN_set_context_enable(d->swp,
++							 (u16)ctx->id,
++							 ctx->qman64);
+ 	return 0;
+ }
+-EXPORT_SYMBOL(dpaa2_io_service_register);
++EXPORT_SYMBOL_GPL(dpaa2_io_service_register);
+ 
+ /**
+  * dpaa2_io_service_deregister - The opposite of 'register'.
+@@ -309,7 +277,7 @@ void dpaa2_io_service_deregister(struct dpaa2_io *service,
+ 	list_del(&ctx->node);
+ 	spin_unlock_irqrestore(&d->lock_notifications, irqflags);
+ }
+-EXPORT_SYMBOL(dpaa2_io_service_deregister);
++EXPORT_SYMBOL_GPL(dpaa2_io_service_deregister);
+ 
+ /**
+  * dpaa2_io_service_rearm() - Rearm the notification for the given DPIO service.
+@@ -343,7 +311,7 @@ int dpaa2_io_service_rearm(struct dpaa2_io *d,
+ 
+ 	return err;
+ }
+-EXPORT_SYMBOL(dpaa2_io_service_rearm);
++EXPORT_SYMBOL_GPL(dpaa2_io_service_rearm);
+ 
+ /**
+  * dpaa2_io_service_pull_fq() - pull dequeue functions from a fq.
+@@ -406,7 +374,7 @@ int dpaa2_io_service_pull_channel(struct dpaa2_io *d, u32 channelid,
+ 
+ 	return err;
+ }
+-EXPORT_SYMBOL(dpaa2_io_service_pull_channel);
++EXPORT_SYMBOL_GPL(dpaa2_io_service_pull_channel);
+ 
+ /**
+  * dpaa2_io_service_enqueue_fq() - Enqueue a frame to a frame queue.
+@@ -462,7 +430,7 @@ int dpaa2_io_service_enqueue_qd(struct dpaa2_io *d,
+ 
+ 	return qbman_swp_enqueue(d->swp, &ed, fd);
+ }
+-EXPORT_SYMBOL(dpaa2_io_service_enqueue_qd);
++EXPORT_SYMBOL_GPL(dpaa2_io_service_enqueue_qd);
+ 
+ /**
+  * dpaa2_io_service_release() - Release buffers to a buffer pool.
+@@ -489,7 +457,7 @@ int dpaa2_io_service_release(struct dpaa2_io *d,
+ 
+ 	return qbman_swp_release(d->swp, &rd, buffers, num_buffers);
+ }
+-EXPORT_SYMBOL(dpaa2_io_service_release);
++EXPORT_SYMBOL_GPL(dpaa2_io_service_release);
+ 
+ /**
+  * dpaa2_io_service_acquire() - Acquire buffers from a buffer pool.
+@@ -520,7 +488,7 @@ int dpaa2_io_service_acquire(struct dpaa2_io *d,
+ 
+ 	return err;
+ }
+-EXPORT_SYMBOL(dpaa2_io_service_acquire);
++EXPORT_SYMBOL_GPL(dpaa2_io_service_acquire);
+ 
+ /*
+  * 'Stores' are reusable memory blocks for holding dequeue results, and to
+@@ -574,7 +542,7 @@ struct dpaa2_io_store *dpaa2_io_store_create(unsigned int max_frames,
+ 
+ 	return ret;
+ }
+-EXPORT_SYMBOL(dpaa2_io_store_create);
++EXPORT_SYMBOL_GPL(dpaa2_io_store_create);
+ 
+ /**
+  * dpaa2_io_store_destroy() - Frees the dma memory storage for dequeue
+@@ -588,7 +556,7 @@ void dpaa2_io_store_destroy(struct dpaa2_io_store *s)
+ 	kfree(s->alloced_addr);
+ 	kfree(s);
+ }
+-EXPORT_SYMBOL(dpaa2_io_store_destroy);
++EXPORT_SYMBOL_GPL(dpaa2_io_store_destroy);
+ 
+ /**
+  * dpaa2_io_store_next() - Determine when the next dequeue result is available.
+@@ -636,7 +604,7 @@ struct dpaa2_dq *dpaa2_io_store_next(struct dpaa2_io_store *s, int *is_last)
+ 
+ 	return ret;
+ }
+-EXPORT_SYMBOL(dpaa2_io_store_next);
++EXPORT_SYMBOL_GPL(dpaa2_io_store_next);
+ 
+ /**
+  * dpaa2_io_query_fq_count() - Get the frame and byte count for a given fq.
+@@ -705,3 +673,108 @@ int dpaa2_io_query_bp_count(struct dpaa2_io *d, u32 bpid, u32 *num)
+ 	return 0;
+ }
+ EXPORT_SYMBOL(dpaa2_io_query_bp_count);
++
++/**
++ * dpaa2_io_service_enqueue_orp_fq() - Enqueue a frame to a frame queue with
++ * order restoration
++ * @d: the given DPIO service.
++ * @fqid: the given frame queue id.
++ * @fd: the frame descriptor which is enqueued.
++ * @orpid: the order restoration point ID
++ * @seqnum: the order sequence number
++ * @last: must be set for the final frame if seqnum is shared (spilt frame)
++ *
++ * Performs an enqueue to a frame queue using the specified order restoration
++ * point. The QMan device will ensure the order of frames placed on the
++ * queue will be ordered as per the sequence number.
++ *
++ * In the case a frame is split it is possible to enqueue using the same
++ * sequence number more than once. The final frame in a shared sequence number
++ * most be indicated by setting last = 1. For non shared sequence numbers
++ * last = 1 must always be set.
++ *
++ * Return 0 for successful enqueue, or -EBUSY if the enqueue ring is not ready,
++ * or -ENODEV if there is no dpio service.
++ */
++int dpaa2_io_service_enqueue_orp_fq(struct dpaa2_io *d, u32 fqid,
++				    const struct dpaa2_fd *fd, u16 orpid,
++				    u16 seqnum, int last)
++{
++	struct qbman_eq_desc ed;
++
++	d = service_select(d);
++	if (!d)
++		return -ENODEV;
++	qbman_eq_desc_clear(&ed);
++	qbman_eq_desc_set_orp(&ed, 0, orpid, seqnum, !last);
++	qbman_eq_desc_set_fq(&ed, fqid);
++	return qbman_swp_enqueue(d->swp, &ed, fd);
++}
++EXPORT_SYMBOL(dpaa2_io_service_enqueue_orp_fq);
++
++/**
++ * dpaa2_io_service_enqueue_orp_qd() - Enqueue a frame to a queueing destination
++ * with order restoration
++ * @d: the given DPIO service.
++ * @qdid: the given queuing destination id.
++ * @fd: the frame descriptor which is enqueued.
++ * @orpid: the order restoration point ID
++ * @seqnum: the order sequence number
++ * @last: must be set for the final frame if seqnum is shared (spilt frame)
++ *
++ * Performs an enqueue to a frame queue using the specified order restoration
++ * point. The QMan device will ensure the order of frames placed on the
++ * queue will be ordered as per the sequence number.
++ *
++ * In the case a frame is split it is possible to enqueue using the same
++ * sequence number more than once. The final frame in a shared sequence number
++ * most be indicated by setting last = 1. For non shared sequence numbers
++ * last = 1 must always be set.
++ *
++ * Return 0 for successful enqueue, or -EBUSY if the enqueue ring is not ready,
++ * or -ENODEV if there is no dpio service.
++ */
++int dpaa2_io_service_enqueue_orp_qd(struct dpaa2_io *d, u32 qdid, u8 prio,
++				    u16 qdbin, const struct dpaa2_fd *fd,
++				    u16 orpid, u16 seqnum, int last)
++{
++	struct qbman_eq_desc ed;
++
++	d = service_select(d);
++	if (!d)
++		return -ENODEV;
++	qbman_eq_desc_clear(&ed);
++	qbman_eq_desc_set_orp(&ed, 0, orpid, seqnum, !last);
++	qbman_eq_desc_set_qd(&ed, qdid, qdbin, prio);
++	return qbman_swp_enqueue(d->swp, &ed, fd);
++}
++EXPORT_SYMBOL_GPL(dpaa2_io_service_enqueue_orp_qd);
++
++/**
++ * dpaa2_io_service_orp_seqnum_drop() - Remove a sequence number from
++ * an order restoration list
++ * @d: the given DPIO service.
++ * @orpid: Order restoration point to remove a sequence number from
++ * @seqnum: Sequence number to remove
++ *
++ * Removes a frames sequence number from an order restoration point without
++ * enqueing the frame. Used to indicate that the order restoration hardware
++ * should not expect to see this sequence number. Typically used to indicate
++ * a frame was terminated or dropped from a flow.
++ *
++ * Return 0 for successful enqueue, or -EBUSY if the enqueue ring is not ready,
++ * or -ENODEV if there is no dpio service.
++ */
++int dpaa2_io_service_orp_seqnum_drop(struct dpaa2_io *d, u16 orpid, u16 seqnum)
++{
++	struct qbman_eq_desc ed;
++	struct dpaa2_fd fd;
++
++	d = service_select(d);
++	if (!d)
++		return -ENODEV;
++	qbman_eq_desc_clear(&ed);
++	qbman_eq_desc_set_orp_hole(&ed, orpid, seqnum);
++	return qbman_swp_enqueue(d->swp, &ed, &fd);
++}
++EXPORT_SYMBOL_GPL(dpaa2_io_service_orp_seqnum_drop);
+diff --git a/drivers/staging/fsl-mc/bus/dpio/dpio.c b/drivers/staging/fsl-mc/bus/dpio/dpio.c
+index 60fc935c..521bc694 100644
+--- a/drivers/staging/fsl-mc/bus/dpio/dpio.c
++++ b/drivers/staging/fsl-mc/bus/dpio/dpio.c
+@@ -1,37 +1,11 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * Copyright 2013-2016 Freescale Semiconductor Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- * * Redistributions of source code must retain the above copyright
+- * notice, this list of conditions and the following disclaimer.
+- * * Redistributions in binary form must reproduce the above copyright
+- * notice, this list of conditions and the following disclaimer in the
+- * documentation and/or other materials provided with the distribution.
+- * * Neither the name of the above-listed copyright holders nor the
+- * names of any contributors may be used to endorse or promote products
+- * derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+- * POSSIBILITY OF SUCH DAMAGE.
+  */
+-#include "../../include/mc-sys.h"
+-#include "../../include/mc-cmd.h"
++#include <linux/kernel.h>
++#include <linux/fsl/mc.h>
+ 
+ #include "dpio.h"
+ #include "dpio-cmd.h"
+@@ -63,7 +37,7 @@ int dpio_open(struct fsl_mc_io *mc_io,
+ 	      int dpio_id,
+ 	      u16 *token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpio_cmd_open *dpio_cmd;
+ 	int err;
+ 
+@@ -96,7 +70,7 @@ int dpio_close(struct fsl_mc_io *mc_io,
+ 	       u32 cmd_flags,
+ 	       u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPIO_CMDID_CLOSE,
+@@ -118,7 +92,7 @@ int dpio_enable(struct fsl_mc_io *mc_io,
+ 		u32 cmd_flags,
+ 		u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPIO_CMDID_ENABLE,
+@@ -140,7 +114,7 @@ int dpio_disable(struct fsl_mc_io *mc_io,
+ 		 u32 cmd_flags,
+ 		 u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPIO_CMDID_DISABLE,
+@@ -164,7 +138,7 @@ int dpio_get_attributes(struct fsl_mc_io *mc_io,
+ 			u16 token,
+ 			struct dpio_attr *attr)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	struct dpio_rsp_get_attr *dpio_rsp;
+ 	int err;
+ 
+@@ -206,7 +180,7 @@ int dpio_get_api_version(struct fsl_mc_io *mc_io,
+ 			 u16 *major_ver,
+ 			 u16 *minor_ver)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 	int err;
+ 
+ 	/* prepare command */
+@@ -235,7 +209,7 @@ int dpio_reset(struct fsl_mc_io *mc_io,
+ 	       u32 cmd_flags,
+ 	       u16 token)
+ {
+-	struct mc_command cmd = { 0 };
++	struct fsl_mc_command cmd = { 0 };
+ 
+ 	/* prepare command */
+ 	cmd.header = mc_encode_cmd_header(DPIO_CMDID_RESET,
+diff --git a/drivers/staging/fsl-mc/bus/dpio/dpio.h b/drivers/staging/fsl-mc/bus/dpio/dpio.h
+index d19c8acf..b2ac4ba4 100644
+--- a/drivers/staging/fsl-mc/bus/dpio/dpio.h
++++ b/drivers/staging/fsl-mc/bus/dpio/dpio.h
+@@ -1,34 +1,8 @@
++/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+ /*
+  * Copyright 2013-2016 Freescale Semiconductor Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- * * Redistributions of source code must retain the above copyright
+- * notice, this list of conditions and the following disclaimer.
+- * * Redistributions in binary form must reproduce the above copyright
+- * notice, this list of conditions and the following disclaimer in the
+- * documentation and/or other materials provided with the distribution.
+- * * Neither the name of the above-listed copyright holders nor the
+- * names of any contributors may be used to endorse or promote products
+- * derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+- * POSSIBILITY OF SUCH DAMAGE.
+  */
+ #ifndef __FSL_DPIO_H
+ #define __FSL_DPIO_H
+diff --git a/drivers/staging/fsl-mc/bus/dpio/qbman-portal.c b/drivers/staging/fsl-mc/bus/dpio/qbman-portal.c
+index 97fd3561..f57956c5 100644
+--- a/drivers/staging/fsl-mc/bus/dpio/qbman-portal.c
++++ b/drivers/staging/fsl-mc/bus/dpio/qbman-portal.c
+@@ -1,33 +1,8 @@
++// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+ /*
+  * Copyright (C) 2014-2016 Freescale Semiconductor, Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *       notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *       notice, this list of conditions and the following disclaimer in the
+- *       documentation and/or other materials provided with the distribution.
+- *     * Neither the name of Freescale Semiconductor nor the
+- *       names of its contributors may be used to endorse or promote products
+- *       derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+  */
+ 
+ #include <asm/cacheflush.h>
+@@ -144,18 +119,18 @@ static inline u32 qbman_set_swp_cfg(u8 max_fill, u8 wn,	u8 est, u8 rpm, u8 dcm,
+ 				    u8 epm, int sd, int sp, int se,
+ 				    int dp, int de, int ep)
+ {
+-	return cpu_to_le32 (max_fill << SWP_CFG_DQRR_MF_SHIFT |
+-			    est << SWP_CFG_EST_SHIFT |
+-			    wn << SWP_CFG_WN_SHIFT |
+-			    rpm << SWP_CFG_RPM_SHIFT |
+-			    dcm << SWP_CFG_DCM_SHIFT |
+-			    epm << SWP_CFG_EPM_SHIFT |
+-			    sd << SWP_CFG_SD_SHIFT |
+-			    sp << SWP_CFG_SP_SHIFT |
+-			    se << SWP_CFG_SE_SHIFT |
+-			    dp << SWP_CFG_DP_SHIFT |
+-			    de << SWP_CFG_DE_SHIFT |
+-			    ep << SWP_CFG_EP_SHIFT);
++	return (max_fill << SWP_CFG_DQRR_MF_SHIFT |
++		est << SWP_CFG_EST_SHIFT |
++		wn << SWP_CFG_WN_SHIFT |
++		rpm << SWP_CFG_RPM_SHIFT |
++		dcm << SWP_CFG_DCM_SHIFT |
++		epm << SWP_CFG_EPM_SHIFT |
++		sd << SWP_CFG_SD_SHIFT |
++		sp << SWP_CFG_SP_SHIFT |
++		se << SWP_CFG_SE_SHIFT |
++		dp << SWP_CFG_DP_SHIFT |
++		de << SWP_CFG_DE_SHIFT |
++		ep << SWP_CFG_EP_SHIFT);
+ }
+ 
+ /**
+@@ -380,6 +355,43 @@ void qbman_eq_desc_set_no_orp(struct qbman_eq_desc *d, int respond_success)
+ 		d->verb |= enqueue_rejects_to_fq;
+ }
+ 
++/**
++ * qbman_eq_desc_set_orp() - Set order-restoration in the enqueue descriptor
++ * @d: the enqueue descriptor.
++ * @response_success: 1 = enqueue with response always; 0 = enqueue with
++ * rejections returned on a FQ.
++ * @oprid: the order point record id.
++ * @seqnum: the order restoration sequence number.
++ * @incomplete: indicates whether this is the last fragments using the same
++ * sequence number.
++ */
++void qbman_eq_desc_set_orp(struct qbman_eq_desc *d, int respond_success,
++			   u16 oprid, u16 seqnum, int incomplete)
++{
++	d->verb |= (1 << QB_ENQUEUE_CMD_ORP_ENABLE_SHIFT);
++	if (respond_success)
++		d->verb |= enqueue_response_always;
++	else
++		d->verb |= enqueue_rejects_to_fq;
++	d->orpid = cpu_to_le16(oprid);
++	d->seqnum = cpu_to_le16((!!incomplete << 14) | seqnum);
++}
++
++/**
++ * qbman_eq_desc_set_orp_hole() - fill a hole in the order-restoration sequence
++ * without any enqueue
++ * @d: the enqueue descriptor.
++ * @oprid: the order point record id.
++ * @seqnum: the order restoration sequence number.
++ */
++void qbman_eq_desc_set_orp_hole(struct qbman_eq_desc *d, u16 oprid,
++				u16 seqnum)
++{
++	d->verb |= (1 << QB_ENQUEUE_CMD_ORP_ENABLE_SHIFT) | enqueue_empty;
++	d->orpid = cpu_to_le16(oprid);
++	d->seqnum = cpu_to_le16(seqnum);
++}
++
+ /*
+  * Exactly one of the following descriptor "targets" should be set. (Calling any
+  * one of these will replace the effect of any prior call to one of these.)
+@@ -543,7 +555,7 @@ void qbman_pull_desc_set_storage(struct qbman_pull_desc *d,
+ 				 int stash)
+ {
+ 	/* save the virtual address */
+-	d->rsp_addr_virt = (u64)storage;
++	d->rsp_addr_virt = (u64)(uintptr_t)storage;
+ 
+ 	if (!storage) {
+ 		d->verb &= ~(1 << QB_VDQCR_VERB_RLS_SHIFT);
+@@ -636,7 +648,7 @@ int qbman_swp_pull(struct qbman_swp *s, struct qbman_pull_desc *d)
+ 		atomic_inc(&s->vdq.available);
+ 		return -EBUSY;
+ 	}
+-	s->vdq.storage = (void *)d->rsp_addr_virt;
++	s->vdq.storage = (void *)(uintptr_t)d->rsp_addr_virt;
+ 	p = qbman_get_cmd(s, QBMAN_CENA_SWP_VDQCR);
+ 	p->numf = d->numf;
+ 	p->tok = QMAN_DQ_TOKEN_VALID;
+@@ -876,7 +888,7 @@ int qbman_swp_release(struct qbman_swp *s, const struct qbman_release_desc *d,
+ struct qbman_acquire_desc {
+ 	u8 verb;
+ 	u8 reserved;
+-	u16 bpid;
++	__le16 bpid;
+ 	u8 num;
+ 	u8 reserved2[59];
+ };
+@@ -884,10 +896,10 @@ struct qbman_acquire_desc {
+ struct qbman_acquire_rslt {
+ 	u8 verb;
+ 	u8 rslt;
+-	u16 reserved;
++	__le16 reserved;
+ 	u8 num;
+ 	u8 reserved2[3];
+-	u64 buf[7];
++	__le64 buf[7];
+ };
+ 
+ /**
+@@ -950,7 +962,7 @@ int qbman_swp_acquire(struct qbman_swp *s, u16 bpid, u64 *buffers,
+ struct qbman_alt_fq_state_desc {
+ 	u8 verb;
+ 	u8 reserved[3];
+-	u32 fqid;
++	__le32 fqid;
+ 	u8 reserved2[56];
+ };
+ 
+@@ -973,7 +985,7 @@ int qbman_swp_alt_fq_state(struct qbman_swp *s, u32 fqid,
+ 	if (!p)
+ 		return -EBUSY;
+ 
+-	p->fqid = cpu_to_le32(fqid) & ALT_FQ_FQID_MASK;
++	p->fqid = cpu_to_le32(fqid & ALT_FQ_FQID_MASK);
+ 
+ 	/* Complete the management command */
+ 	r = qbman_swp_mc_complete(s, p, alt_fq_verb);
+@@ -999,11 +1011,11 @@ int qbman_swp_alt_fq_state(struct qbman_swp *s, u32 fqid,
+ struct qbman_cdan_ctrl_desc {
+ 	u8 verb;
+ 	u8 reserved;
+-	u16 ch;
++	__le16 ch;
+ 	u8 we;
+ 	u8 ctrl;
+-	u16 reserved2;
+-	u64 cdan_ctx;
++	__le16 reserved2;
++	__le64 cdan_ctx;
+ 	u8 reserved3[48];
+ 
+ };
+@@ -1011,7 +1023,7 @@ struct qbman_cdan_ctrl_desc {
+ struct qbman_cdan_ctrl_rslt {
+ 	u8 verb;
+ 	u8 rslt;
+-	u16 ch;
++	__le16 ch;
+ 	u8 reserved[60];
+ };
+ 
+diff --git a/drivers/staging/fsl-mc/bus/dpio/qbman-portal.h b/drivers/staging/fsl-mc/bus/dpio/qbman-portal.h
+index 985f1976..ff851dd3 100644
+--- a/drivers/staging/fsl-mc/bus/dpio/qbman-portal.h
++++ b/drivers/staging/fsl-mc/bus/dpio/qbman-portal.h
+@@ -1,33 +1,8 @@
++/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+ /*
+  * Copyright (C) 2014-2016 Freescale Semiconductor, Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *       notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *       notice, this list of conditions and the following disclaimer in the
+- *       documentation and/or other materials provided with the distribution.
+- *     * Neither the name of Freescale Semiconductor nor the
+- *       names of its contributors may be used to endorse or promote products
+- *       derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+  */
+ #ifndef __FSL_QBMAN_PORTAL_H
+ #define __FSL_QBMAN_PORTAL_H
+@@ -57,8 +32,8 @@ struct qbman_pull_desc {
+ 	u8 numf;
+ 	u8 tok;
+ 	u8 reserved;
+-	u32 dq_src;
+-	u64 rsp_addr;
++	__le32 dq_src;
++	__le64 rsp_addr;
+ 	u64 rsp_addr_virt;
+ 	u8 padding[40];
+ };
+@@ -95,17 +70,17 @@ enum qbman_pull_type_e {
+ struct qbman_eq_desc {
+ 	u8 verb;
+ 	u8 dca;
+-	u16 seqnum;
+-	u16 orpid;
+-	u16 reserved1;
+-	u32 tgtid;
+-	u32 tag;
+-	u16 qdbin;
++	__le16 seqnum;
++	__le16 orpid;
++	__le16 reserved1;
++	__le32 tgtid;
++	__le32 tag;
++	__le16 qdbin;
+ 	u8 qpri;
+ 	u8 reserved[3];
+ 	u8 wae;
+ 	u8 rspid;
+-	u64 rsp_addr;
++	__le64 rsp_addr;
+ 	u8 fd[32];
+ };
+ 
+@@ -113,9 +88,9 @@ struct qbman_eq_desc {
+ struct qbman_release_desc {
+ 	u8 verb;
+ 	u8 reserved;
+-	u16 bpid;
+-	u32 reserved2;
+-	u64 buf[7];
++	__le16 bpid;
++	__le32 reserved2;
++	__le64 buf[7];
+ };
+ 
+ /* Management command result codes */
+@@ -187,6 +162,9 @@ int qbman_result_has_new_result(struct qbman_swp *p, const struct dpaa2_dq *dq);
+ 
+ void qbman_eq_desc_clear(struct qbman_eq_desc *d);
+ void qbman_eq_desc_set_no_orp(struct qbman_eq_desc *d, int respond_success);
++void qbman_eq_desc_set_orp(struct qbman_eq_desc *d, int respond_success,
++			   u16 oprid, u16 seqnum, int incomplete);
++void qbman_eq_desc_set_orp_hole(struct qbman_eq_desc *d, u16 oprid, u16 seqnum);
+ void qbman_eq_desc_set_token(struct qbman_eq_desc *d, u8 token);
+ void qbman_eq_desc_set_fq(struct qbman_eq_desc *d, u32 fqid);
+ void qbman_eq_desc_set_qd(struct qbman_eq_desc *d, u32 qdid,
+diff --git a/drivers/staging/fsl-mc/include/dpaa2-fd.h b/drivers/staging/fsl-mc/include/dpaa2-fd.h
+index 72328415..457c3853 100644
+--- a/drivers/staging/fsl-mc/include/dpaa2-fd.h
++++ b/drivers/staging/fsl-mc/include/dpaa2-fd.h
+@@ -1,33 +1,8 @@
++/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+ /*
+  * Copyright 2014-2016 Freescale Semiconductor Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *       notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *       notice, this list of conditions and the following disclaimer in the
+- *       documentation and/or other materials provided with the distribution.
+- *     * Neither the name of Freescale Semiconductor nor the
+- *       names of its contributors may be used to endorse or promote products
+- *       derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+  */
+ #ifndef __FSL_DPAA2_FD_H
+ #define __FSL_DPAA2_FD_H
+@@ -333,7 +308,7 @@ enum dpaa2_sg_format {
+  */
+ static inline dma_addr_t dpaa2_sg_get_addr(const struct dpaa2_sg_entry *sg)
+ {
+-	return le64_to_cpu((dma_addr_t)sg->addr);
++	return (dma_addr_t)le64_to_cpu(sg->addr);
+ }
+ 
+ /**
+@@ -464,8 +439,8 @@ static inline bool dpaa2_sg_is_final(const struct dpaa2_sg_entry *sg)
+  */
+ static inline void dpaa2_sg_set_final(struct dpaa2_sg_entry *sg, bool final)
+ {
+-	sg->format_offset &= cpu_to_le16(~(SG_FINAL_FLAG_MASK
+-					 << SG_FINAL_FLAG_SHIFT));
++	sg->format_offset &= cpu_to_le16((~(SG_FINAL_FLAG_MASK
++					 << SG_FINAL_FLAG_SHIFT)) & 0xFFFF);
+ 	sg->format_offset |= cpu_to_le16(final << SG_FINAL_FLAG_SHIFT);
+ }
+ 
+diff --git a/drivers/staging/fsl-mc/include/dpaa2-global.h b/drivers/staging/fsl-mc/include/dpaa2-global.h
+index 0326447f..9bc07133 100644
+--- a/drivers/staging/fsl-mc/include/dpaa2-global.h
++++ b/drivers/staging/fsl-mc/include/dpaa2-global.h
+@@ -1,33 +1,8 @@
++/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+ /*
+  * Copyright 2014-2016 Freescale Semiconductor Inc.
+  * Copyright 2016 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *       notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *       notice, this list of conditions and the following disclaimer in the
+- *       documentation and/or other materials provided with the distribution.
+- *     * Neither the name of Freescale Semiconductor nor the
+- *       names of its contributors may be used to endorse or promote products
+- *       derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+  */
+ #ifndef __FSL_DPAA2_GLOBAL_H
+ #define __FSL_DPAA2_GLOBAL_H
+diff --git a/drivers/staging/fsl-mc/include/dpaa2-io.h b/drivers/staging/fsl-mc/include/dpaa2-io.h
+index b5458f88..9f51afaa 100644
+--- a/drivers/staging/fsl-mc/include/dpaa2-io.h
++++ b/drivers/staging/fsl-mc/include/dpaa2-io.h
+@@ -1,33 +1,8 @@
++/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+ /*
+  * Copyright 2014-2016 Freescale Semiconductor Inc.
+  * Copyright 2017 NXP
+  *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions are met:
+- *     * Redistributions of source code must retain the above copyright
+- *       notice, this list of conditions and the following disclaimer.
+- *     * Redistributions in binary form must reproduce the above copyright
+- *       notice, this list of conditions and the following disclaimer in the
+- *       documentation and/or other materials provided with the distribution.
+- *     * Neither the name of Freescale Semiconductor nor the
+- *       names of its contributors may be used to endorse or promote products
+- *       derived from this software without specific prior written permission.
+- *
+- * ALTERNATIVELY, this software may be distributed under the terms of the
+- * GNU General Public License ("GPL") as published by the Free Software
+- * Foundation, either version 2 of that License or (at your option) any
+- * later version.
+- *
+- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+  */
+ #ifndef __FSL_DPAA2_IO_H
+ #define __FSL_DPAA2_IO_H
+@@ -104,7 +79,7 @@ struct dpaa2_io *dpaa2_io_service_select(int cpu);
+  * Used when a FQDAN/CDAN registration is made by drivers.
+  */
+ struct dpaa2_io_notification_ctx {
+-	void (*cb)(struct dpaa2_io_notification_ctx *);
++	void (*cb)(struct dpaa2_io_notification_ctx *ctx);
+ 	int is_cdan;
+ 	u32 id;
+ 	int desired_cpu;
+@@ -140,6 +115,18 @@ struct dpaa2_io_store *dpaa2_io_store_create(unsigned int max_frames,
+ void dpaa2_io_store_destroy(struct dpaa2_io_store *s);
+ struct dpaa2_dq *dpaa2_io_store_next(struct dpaa2_io_store *s, int *is_last);
+ 
++/* Order Restoration Support */
++int dpaa2_io_service_enqueue_orp_fq(struct dpaa2_io *d, u32 fqid,
++				    const struct dpaa2_fd *fd, u16 orpid,
++				    u16 seqnum, int last);
++
++int dpaa2_io_service_enqueue_orp_qd(struct dpaa2_io *d, u32 qdid, u8 prio,
++				    u16 qdbin, const struct dpaa2_fd *fd,
++				    u16 orpid, u16 seqnum, int last);
++
++int dpaa2_io_service_orp_seqnum_drop(struct dpaa2_io *d, u16 orpid,
++				     u16 seqnum);
++
+ /***************/
+ /* CSCN        */
+ /***************/
+diff --git a/drivers/staging/fsl-mc/include/dpopr.h b/drivers/staging/fsl-mc/include/dpopr.h
+index e1110af2..c84d3ee3 100644
+--- a/drivers/staging/fsl-mc/include/dpopr.h
++++ b/drivers/staging/fsl-mc/include/dpopr.h
+@@ -33,6 +33,8 @@
+ #ifndef __FSL_DPOPR_H_
+ #define __FSL_DPOPR_H_
+ 
++#include <linux/types.h>
++
+ /* Data Path Order Restoration API
+  * Contains initialization APIs and runtime APIs for the Order Restoration
+  */
+diff --git a/drivers/staging/fsl_qbman/fsl_usdpaa_irq.c b/drivers/staging/fsl_qbman/fsl_usdpaa_irq.c
+index 914c7471..d01b9582 100644
+--- a/drivers/staging/fsl_qbman/fsl_usdpaa_irq.c
++++ b/drivers/staging/fsl_qbman/fsl_usdpaa_irq.c
+@@ -102,7 +102,7 @@ static irqreturn_t usdpaa_irq_handler(int irq, void *_ctx)
+ 	/* Set the inhibit register.  This will be reenabled
+ 	   once the USDPAA code handles the IRQ */
+ 	out_be32(ctx->inhibit_addr, 0x1);
+-	pr_info("Inhibit at %p count %d", ctx->inhibit_addr, ctx->irq_count);
++	pr_debug("Inhibit at %p count %d", ctx->inhibit_addr, ctx->irq_count);
+ 	return IRQ_HANDLED;
+ }
+ 
+diff --git a/drivers/staging/fsl_qbman/qman_high.c b/drivers/staging/fsl_qbman/qman_high.c
+index 1651e62c..f80dadf6 100644
+--- a/drivers/staging/fsl_qbman/qman_high.c
++++ b/drivers/staging/fsl_qbman/qman_high.c
+@@ -3075,36 +3075,19 @@ struct cgr_comp {
+ 	struct completion completion;
+ };
+ 
+-static int qman_delete_cgr_thread(void *p)
++static void qman_delete_cgr_smp_call(void *p)
+ {
+-	struct cgr_comp *cgr_comp = (struct cgr_comp *)p;
+-	int res;
+-
+-	res = qman_delete_cgr((struct qman_cgr *)cgr_comp->cgr);
+-	complete(&cgr_comp->completion);
+-
+-	return res;
++	qman_delete_cgr((struct qman_cgr *)p);
+ }
+ 
+ void qman_delete_cgr_safe(struct qman_cgr *cgr)
+ {
+-	struct task_struct *thread;
+-	struct cgr_comp cgr_comp;
+-
+ 	preempt_disable();
+ 	if (qman_cgr_cpus[cgr->cgrid] != smp_processor_id()) {
+-		init_completion(&cgr_comp.completion);
+-		cgr_comp.cgr = cgr;
+-		thread = kthread_create(qman_delete_cgr_thread, &cgr_comp,
+-					"cgr_del");
+-
+-		if (likely(!IS_ERR(thread))) {
+-			kthread_bind(thread, qman_cgr_cpus[cgr->cgrid]);
+-			wake_up_process(thread);
+-			wait_for_completion(&cgr_comp.completion);
+-			preempt_enable();
+-			return;
+-		}
++		smp_call_function_single(qman_cgr_cpus[cgr->cgrid],
++					 qman_delete_cgr_smp_call, cgr, true);
++		preempt_enable();
++		return;
+ 	}
+ 	qman_delete_cgr(cgr);
+ 	preempt_enable();
+diff --git a/drivers/staging/fsl_qbman/qman_low.h b/drivers/staging/fsl_qbman/qman_low.h
+index e43f9ea0..d58af1b5 100644
+--- a/drivers/staging/fsl_qbman/qman_low.h
++++ b/drivers/staging/fsl_qbman/qman_low.h
+@@ -658,7 +658,7 @@ static inline int qm_dqrr_init(struct qm_portal *portal,
+ 			qm_dqrr_cce_consume(portal, dqrr->fill);
+ 			break;
+ 		case qm_dqrr_cdc:
+-			qm_dqrr_cdc_consume_n(portal, (QM_DQRR_SIZE - 1));
++			qm_dqrr_cdc_consume_n(portal, (1<<QM_DQRR_SIZE) - 1);
+ 			break;
+ 		default:
+ 			DPA_ASSERT(0);
+@@ -682,6 +682,9 @@ static inline int qm_dqrr_init(struct qm_portal *portal,
+ 		(0 ? 0x10 : 0);				/* Ignore SP */
+ 	qm_out(CFG, cfg);
+ 	qm_dqrr_set_maxfill(portal, max_fill);
++
++	/* Recalculate cursor as we may have consumed frames */
++	dqrr->cursor = dqrr->ring + dqrr->ci;
+ 	return 0;
+ }
+ 
+diff --git a/drivers/usb/dwc3/dwc3-pci.c b/drivers/usb/dwc3/dwc3-pci.c
+index 427291a1..d0f6cb4d 100644
+--- a/drivers/usb/dwc3/dwc3-pci.c
++++ b/drivers/usb/dwc3/dwc3-pci.c
+@@ -72,6 +72,7 @@ static int dwc3_pci_quirks(struct pci_dev *pdev, struct platform_device *dwc3)
+ 			PROPERTY_ENTRY_BOOL("snps,disable_scramble_quirk"),
+ 			PROPERTY_ENTRY_BOOL("snps,dis_u3_susphy_quirk"),
+ 			PROPERTY_ENTRY_BOOL("snps,dis_u2_susphy_quirk"),
++			PROPERTY_ENTRY_BOOL("linux,sysdev_is_parent"),
+ 			{ },
+ 		};
+ 
+@@ -83,6 +84,7 @@ static int dwc3_pci_quirks(struct pci_dev *pdev, struct platform_device *dwc3)
+ 
+ 		struct property_entry properties[] = {
+ 			PROPERTY_ENTRY_STRING("dr_mode", "peripheral"),
++			PROPERTY_ENTRY_BOOL("linux,sysdev_is_parent"),
+ 			{ }
+ 		};
+ 
+@@ -128,6 +130,7 @@ static int dwc3_pci_quirks(struct pci_dev *pdev, struct platform_device *dwc3)
+ 			PROPERTY_ENTRY_BOOL("snps,usb3_lpm_capable"),
+ 			PROPERTY_ENTRY_BOOL("snps,has-lpm-erratum"),
+ 			PROPERTY_ENTRY_BOOL("snps,dis_enblslpm_quirk"),
++			PROPERTY_ENTRY_BOOL("linux,sysdev_is_parent"),
+ 			{ },
+ 		};
+ 
+@@ -173,7 +176,7 @@ static int dwc3_pci_probe(struct pci_dev *pci,
+ 	ret = platform_device_add_resources(dwc3, res, ARRAY_SIZE(res));
+ 	if (ret) {
+ 		dev_err(dev, "couldn't add resources to dwc3 device\n");
+-		return ret;
++		goto err;
+ 	}
+ 
+ 	dwc3->dev.parent = dev;
+diff --git a/drivers/vfio/fsl-mc/vfio_fsl_mc.c b/drivers/vfio/fsl-mc/vfio_fsl_mc.c
+index 9dc32d27..9a356fcd 100644
+--- a/drivers/vfio/fsl-mc/vfio_fsl_mc.c
++++ b/drivers/vfio/fsl-mc/vfio_fsl_mc.c
+@@ -19,10 +19,7 @@
+ #include <linux/vfio.h>
+ #include <linux/delay.h>
+ 
+-#include "../../staging/fsl-mc/include/mc.h"
+-#include "../../staging/fsl-mc/include/mc-bus.h"
+-#include "../../staging/fsl-mc/include/mc-sys.h"
+-#include "../../staging/fsl-mc/bus/dprc-cmd.h"
++#include <linux/fsl/mc.h>
+ 
+ #include "vfio_fsl_mc_private.h"
+ 
+@@ -289,8 +286,9 @@ static ssize_t vfio_fsl_mc_read(void *device_data, char __user *buf,
+ 	uint64_t data[8];
+ 	int i;
+ 
+-	/* Read ioctl supported only for DPRC device */
+-	if (strcmp(vdev->mc_dev->obj_desc.type, "dprc"))
++	/* Read ioctl supported only for DPRC and DPMCP device */
++	if (strcmp(vdev->mc_dev->obj_desc.type, "dprc") &&
++	    strcmp(vdev->mc_dev->obj_desc.type, "dpmcp"))
+ 		return -EINVAL;
+ 
+ 	if (index >= vdev->num_regions)
+@@ -391,8 +389,9 @@ static ssize_t vfio_fsl_mc_write(void *device_data, const char __user *buf,
+ 	uint64_t data[8];
+ 	int ret;
+ 
+-	/* Write ioctl supported only for DPRC device */
+-	if (strcmp(vdev->mc_dev->obj_desc.type, "dprc"))
++	/* Write ioctl supported only for DPRC and DPMCP device */
++	if (strcmp(vdev->mc_dev->obj_desc.type, "dprc") &&
++	    strcmp(vdev->mc_dev->obj_desc.type, "dpmcp"))
+ 		return -EINVAL;
+ 
+ 	if (index >= vdev->num_regions)
+diff --git a/drivers/vfio/fsl-mc/vfio_fsl_mc_intr.c b/drivers/vfio/fsl-mc/vfio_fsl_mc_intr.c
+index eb244bb0..b3ce07df 100644
+--- a/drivers/vfio/fsl-mc/vfio_fsl_mc_intr.c
++++ b/drivers/vfio/fsl-mc/vfio_fsl_mc_intr.c
+@@ -15,7 +15,7 @@
+ #include <linux/eventfd.h>
+ #include <linux/msi.h>
+ 
+-#include "../../staging/fsl-mc/include/mc.h"
++#include "linux/fsl/mc.h"
+ #include "vfio_fsl_mc_private.h"
+ 
+ static irqreturn_t vfio_fsl_mc_irq_handler(int irq_num, void *arg)
+diff --git a/include/keys/secure-type.h b/include/keys/secure-type.h
+new file mode 100644
+index 00000000..08788d9b
+--- /dev/null
++++ b/include/keys/secure-type.h
+@@ -0,0 +1,33 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++/*
++ * Copyright 2018 NXP
++ *
++ */
++
++#ifndef _KEYS_SECURE_TYPE_H
++#define _KEYS_SECURE_TYPE_H
++
++#include <linux/key.h>
++#include <linux/rcupdate.h>
++
++/* Minimum key size to be used is 32 bytes and maximum key size fixed
++ * is 128 bytes.
++ * Blob size to be kept is Maximum key size + blob header added by CAAM.
++ */
++
++#define MIN_KEY_SIZE                    32
++#define MAX_KEY_SIZE                    128
++#define BLOB_HEADER_SIZE		48
++
++#define MAX_BLOB_SIZE                   (MAX_KEY_SIZE + BLOB_HEADER_SIZE)
++
++struct secure_key_payload {
++	struct rcu_head rcu;
++	unsigned int key_len;
++	unsigned int blob_len;
++	unsigned char key[MAX_KEY_SIZE + 1];
++	unsigned char blob[MAX_BLOB_SIZE];
++};
++
++extern struct key_type key_type_secure;
++#endif
+diff --git a/include/kvm/arm_psci.h b/include/kvm/arm_psci.h
+new file mode 100644
+index 00000000..4b154812
+--- /dev/null
++++ b/include/kvm/arm_psci.h
+@@ -0,0 +1,63 @@
++/*
++ * Copyright (C) 2012,2013 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#ifndef __KVM_ARM_PSCI_H__
++#define __KVM_ARM_PSCI_H__
++
++#include <linux/kvm_host.h>
++#include <uapi/linux/psci.h>
++
++#define KVM_ARM_PSCI_0_1	PSCI_VERSION(0, 1)
++#define KVM_ARM_PSCI_0_2	PSCI_VERSION(0, 2)
++#define KVM_ARM_PSCI_1_0	PSCI_VERSION(1, 0)
++
++#define KVM_ARM_PSCI_LATEST	KVM_ARM_PSCI_1_0
++
++/*
++ * We need the KVM pointer independently from the vcpu as we can call
++ * this from HYP, and need to apply kern_hyp_va on it...
++ */
++static inline int kvm_psci_version(struct kvm_vcpu *vcpu, struct kvm *kvm)
++{
++	/*
++	 * Our PSCI implementation stays the same across versions from
++	 * v0.2 onward, only adding the few mandatory functions (such
++	 * as FEATURES with 1.0) that are required by newer
++	 * revisions. It is thus safe to return the latest, unless
++	 * userspace has instructed us otherwise.
++	 */
++	if (test_bit(KVM_ARM_VCPU_PSCI_0_2, vcpu->arch.features)) {
++		if (vcpu->kvm->arch.psci_version)
++			return vcpu->kvm->arch.psci_version;
++
++		return KVM_ARM_PSCI_LATEST;
++	}
++
++	return KVM_ARM_PSCI_0_1;
++}
++
++
++int kvm_hvc_call_handler(struct kvm_vcpu *vcpu);
++
++struct kvm_one_reg;
++
++int kvm_arm_get_fw_num_regs(struct kvm_vcpu *vcpu);
++int kvm_arm_copy_fw_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices);
++int kvm_arm_get_fw_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
++int kvm_arm_set_fw_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
++
++#endif /* __KVM_ARM_PSCI_H__ */
+diff --git a/include/linux/fsl/guts.h b/include/linux/fsl/guts.h
+index 649e9171..941b1181 100644
+--- a/include/linux/fsl/guts.h
++++ b/include/linux/fsl/guts.h
+@@ -16,6 +16,7 @@
+ #define __FSL_GUTS_H__
+ 
+ #include <linux/types.h>
++#include <linux/io.h>
+ 
+ /**
+  * Global Utility Registers.
+@@ -29,83 +30,112 @@
+  * #ifdefs.
+  */
+ struct ccsr_guts {
+-	__be32	porpllsr;	/* 0x.0000 - POR PLL Ratio Status Register */
+-	__be32	porbmsr;	/* 0x.0004 - POR Boot Mode Status Register */
+-	__be32	porimpscr;	/* 0x.0008 - POR I/O Impedance Status and Control Register */
+-	__be32	pordevsr;	/* 0x.000c - POR I/O Device Status Register */
+-	__be32	pordbgmsr;	/* 0x.0010 - POR Debug Mode Status Register */
+-	__be32	pordevsr2;	/* 0x.0014 - POR device status register 2 */
++	u32	porpllsr;	/* 0x.0000 - POR PLL Ratio Status Register */
++	u32	porbmsr;	/* 0x.0004 - POR Boot Mode Status Register */
++	u32	porimpscr;	/* 0x.0008 - POR I/O Impedance Status and
++				 *           Control Register
++				 */
++	u32	pordevsr;	/* 0x.000c - POR I/O Device Status Register */
++	u32	pordbgmsr;	/* 0x.0010 - POR Debug Mode Status Register */
++	u32	pordevsr2;	/* 0x.0014 - POR device status register 2 */
+ 	u8	res018[0x20 - 0x18];
+-	__be32	porcir;		/* 0x.0020 - POR Configuration Information Register */
++	u32	porcir;		/* 0x.0020 - POR Configuration Information
++				 *           Register
++				 */
+ 	u8	res024[0x30 - 0x24];
+-	__be32	gpiocr;		/* 0x.0030 - GPIO Control Register */
++	u32	gpiocr;		/* 0x.0030 - GPIO Control Register */
+ 	u8	res034[0x40 - 0x34];
+-	__be32	gpoutdr;	/* 0x.0040 - General-Purpose Output Data Register */
++	u32	gpoutdr;	/* 0x.0040 - General-Purpose Output Data
++				 *           Register
++				 */
+ 	u8	res044[0x50 - 0x44];
+-	__be32	gpindr;		/* 0x.0050 - General-Purpose Input Data Register */
++	u32	gpindr;		/* 0x.0050 - General-Purpose Input Data
++				 *           Register
++				 */
+ 	u8	res054[0x60 - 0x54];
+-	__be32	pmuxcr;		/* 0x.0060 - Alternate Function Signal Multiplex Control */
+-        __be32  pmuxcr2;	/* 0x.0064 - Alternate function signal multiplex control 2 */
+-        __be32  dmuxcr;		/* 0x.0068 - DMA Mux Control Register */
++	u32	pmuxcr;		/* 0x.0060 - Alternate Function Signal
++				 *           Multiplex Control
++				 */
++	u32	pmuxcr2;	/* 0x.0064 - Alternate function signal
++				 *           multiplex control 2
++				 */
++	u32	dmuxcr;		/* 0x.0068 - DMA Mux Control Register */
+         u8	res06c[0x70 - 0x6c];
+-	__be32	devdisr;	/* 0x.0070 - Device Disable Control */
++	u32	devdisr;	/* 0x.0070 - Device Disable Control */
+ #define CCSR_GUTS_DEVDISR_TB1	0x00001000
+ #define CCSR_GUTS_DEVDISR_TB0	0x00004000
+-	__be32	devdisr2;	/* 0x.0074 - Device Disable Control 2 */
++	u32	devdisr2;	/* 0x.0074 - Device Disable Control 2 */
+ 	u8	res078[0x7c - 0x78];
+-	__be32  pmjcr;		/* 0x.007c - 4 Power Management Jog Control Register */
+-	__be32	powmgtcsr;	/* 0x.0080 - Power Management Status and Control Register */
+-	__be32  pmrccr;		/* 0x.0084 - Power Management Reset Counter Configuration Register */
+-	__be32  pmpdccr;	/* 0x.0088 - Power Management Power Down Counter Configuration Register */
+-	__be32  pmcdr;		/* 0x.008c - 4Power management clock disable register */
+-	__be32	mcpsumr;	/* 0x.0090 - Machine Check Summary Register */
+-	__be32	rstrscr;	/* 0x.0094 - Reset Request Status and Control Register */
+-	__be32  ectrstcr;	/* 0x.0098 - Exception reset control register */
+-	__be32  autorstsr;	/* 0x.009c - Automatic reset status register */
+-	__be32	pvr;		/* 0x.00a0 - Processor Version Register */
+-	__be32	svr;		/* 0x.00a4 - System Version Register */
++	u32	pmjcr;		/* 0x.007c - 4 Power Management Jog Control
++				 *           Register
++				 */
++	u32	powmgtcsr;	/* 0x.0080 - Power Management Status and
++				 *           Control Register
++				 */
++	u32	pmrccr;		/* 0x.0084 - Power Management Reset Counter
++				 *           Configuration Register
++				 */
++	u32	pmpdccr;	/* 0x.0088 - Power Management Power Down Counter
++				 *           Configuration Register
++				 */
++	u32	pmcdr;		/* 0x.008c - 4Power management clock disable
++				 *           register
++				 */
++	u32	mcpsumr;	/* 0x.0090 - Machine Check Summary Register */
++	u32	rstrscr;	/* 0x.0094 - Reset Request Status and
++				 *           Control Register
++				 */
++	u32	ectrstcr;	/* 0x.0098 - Exception reset control register */
++	u32	autorstsr;	/* 0x.009c - Automatic reset status register */
++	u32	pvr;		/* 0x.00a0 - Processor Version Register */
++	u32	svr;		/* 0x.00a4 - System Version Register */
+ 	u8	res0a8[0xb0 - 0xa8];
+-	__be32	rstcr;		/* 0x.00b0 - Reset Control Register */
++	u32	rstcr;		/* 0x.00b0 - Reset Control Register */
+ 	u8	res0b4[0xc0 - 0xb4];
+-	__be32  iovselsr;	/* 0x.00c0 - I/O voltage select status register
++	u32	iovselsr;	/* 0x.00c0 - I/O voltage select status register
+ 					     Called 'elbcvselcr' on 86xx SOCs */
+ 	u8	res0c4[0x100 - 0xc4];
+-	__be32	rcwsr[16];	/* 0x.0100 - Reset Control Word Status registers
++	u32	rcwsr[16];	/* 0x.0100 - Reset Control Word Status registers
+ 					     There are 16 registers */
+ 	u8	res140[0x224 - 0x140];
+-	__be32  iodelay1;	/* 0x.0224 - IO delay control register 1 */
+-	__be32  iodelay2;	/* 0x.0228 - IO delay control register 2 */
++	u32	iodelay1;	/* 0x.0224 - IO delay control register 1 */
++	u32	iodelay2;	/* 0x.0228 - IO delay control register 2 */
+ 	u8	res22c[0x604 - 0x22c];
+-	__be32	pamubypenr; 	/* 0x.604 - PAMU bypass enable register */
++	u32	pamubypenr;	/* 0x.604 - PAMU bypass enable register */
+ 	u8	res608[0x800 - 0x608];
+-	__be32	clkdvdr;	/* 0x.0800 - Clock Divide Register */
++	u32	clkdvdr;	/* 0x.0800 - Clock Divide Register */
+ 	u8	res804[0x900 - 0x804];
+-	__be32	ircr;		/* 0x.0900 - Infrared Control Register */
++	u32	ircr;		/* 0x.0900 - Infrared Control Register */
+ 	u8	res904[0x908 - 0x904];
+-	__be32	dmacr;		/* 0x.0908 - DMA Control Register */
++	u32	dmacr;		/* 0x.0908 - DMA Control Register */
+ 	u8	res90c[0x914 - 0x90c];
+-	__be32	elbccr;		/* 0x.0914 - eLBC Control Register */
++	u32	elbccr;		/* 0x.0914 - eLBC Control Register */
+ 	u8	res918[0xb20 - 0x918];
+-	__be32	ddr1clkdr;	/* 0x.0b20 - DDR1 Clock Disable Register */
+-	__be32	ddr2clkdr;	/* 0x.0b24 - DDR2 Clock Disable Register */
+-	__be32	ddrclkdr;	/* 0x.0b28 - DDR Clock Disable Register */
++	u32	ddr1clkdr;	/* 0x.0b20 - DDR1 Clock Disable Register */
++	u32	ddr2clkdr;	/* 0x.0b24 - DDR2 Clock Disable Register */
++	u32	ddrclkdr;	/* 0x.0b28 - DDR Clock Disable Register */
+ 	u8	resb2c[0xe00 - 0xb2c];
+-	__be32	clkocr;		/* 0x.0e00 - Clock Out Select Register */
++	u32	clkocr;		/* 0x.0e00 - Clock Out Select Register */
+ 	u8	rese04[0xe10 - 0xe04];
+-	__be32	ddrdllcr;	/* 0x.0e10 - DDR DLL Control Register */
++	u32	ddrdllcr;	/* 0x.0e10 - DDR DLL Control Register */
+ 	u8	rese14[0xe20 - 0xe14];
+-	__be32	lbcdllcr;	/* 0x.0e20 - LBC DLL Control Register */
+-	__be32  cpfor;		/* 0x.0e24 - L2 charge pump fuse override register */
++	u32	lbcdllcr;	/* 0x.0e20 - LBC DLL Control Register */
++	u32	cpfor;		/* 0x.0e24 - L2 charge pump fuse override
++				 *           register
++				 */
+ 	u8	rese28[0xf04 - 0xe28];
+-	__be32	srds1cr0;	/* 0x.0f04 - SerDes1 Control Register 0 */
+-	__be32	srds1cr1;	/* 0x.0f08 - SerDes1 Control Register 0 */
++	u32	srds1cr0;	/* 0x.0f04 - SerDes1 Control Register 0 */
++	u32	srds1cr1;	/* 0x.0f08 - SerDes1 Control Register 0 */
+ 	u8	resf0c[0xf2c - 0xf0c];
+-	__be32  itcr;		/* 0x.0f2c - Internal transaction control register */
++	u32	itcr;		/* 0x.0f2c - Internal transaction control
++				 *           register
++				 */
+ 	u8	resf30[0xf40 - 0xf30];
+-	__be32	srds2cr0;	/* 0x.0f40 - SerDes2 Control Register 0 */
+-	__be32	srds2cr1;	/* 0x.0f44 - SerDes2 Control Register 0 */
++	u32	srds2cr0;	/* 0x.0f40 - SerDes2 Control Register 0 */
++	u32	srds2cr1;	/* 0x.0f44 - SerDes2 Control Register 0 */
+ } __attribute__ ((packed));
+ 
++u32 fsl_guts_get_svr(void);
+ 
+ /* Alternate function signal multiplex control */
+ #define MPC85xx_PMUXCR_QE(x) (0x8000 >> (x))
+diff --git a/include/linux/fsl/mc.h b/include/linux/fsl/mc.h
+new file mode 100644
+index 00000000..4ce9a93b
+--- /dev/null
++++ b/include/linux/fsl/mc.h
+@@ -0,0 +1,1025 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++/*
++ * Freescale Management Complex (MC) bus public interface
++ *
++ * Copyright (C) 2014-2016 Freescale Semiconductor, Inc.
++ * Author: German Rivera <German.Rivera@freescale.com>
++ *
++ */
++#ifndef _FSL_MC_H_
++#define _FSL_MC_H_
++
++#include <linux/device.h>
++#include <linux/mod_devicetable.h>
++#include <linux/interrupt.h>
++#include <linux/cdev.h>
++#include <uapi/linux/fsl_mc.h>
++
++#define FSL_MC_VENDOR_FREESCALE	0x1957
++
++struct irq_domain;
++struct msi_domain_info;
++
++struct fsl_mc_device;
++struct fsl_mc_io;
++
++/**
++ * struct fsl_mc_driver - MC object device driver object
++ * @driver: Generic device driver
++ * @match_id_table: table of supported device matching Ids
++ * @probe: Function called when a device is added
++ * @remove: Function called when a device is removed
++ * @shutdown: Function called at shutdown time to quiesce the device
++ * @suspend: Function called when a device is stopped
++ * @resume: Function called when a device is resumed
++ *
++ * Generic DPAA device driver object for device drivers that are registered
++ * with a DPRC bus. This structure is to be embedded in each device-specific
++ * driver structure.
++ */
++struct fsl_mc_driver {
++	struct device_driver driver;
++	const struct fsl_mc_device_id *match_id_table;
++	int (*probe)(struct fsl_mc_device *dev);
++	int (*remove)(struct fsl_mc_device *dev);
++	void (*shutdown)(struct fsl_mc_device *dev);
++	int (*suspend)(struct fsl_mc_device *dev, pm_message_t state);
++	int (*resume)(struct fsl_mc_device *dev);
++};
++
++#define to_fsl_mc_driver(_drv) \
++	container_of(_drv, struct fsl_mc_driver, driver)
++
++#define to_fsl_mc_bus(_mc_dev) \
++	container_of(_mc_dev, struct fsl_mc_bus, mc_dev)
++
++/**
++ * enum fsl_mc_pool_type - Types of allocatable MC bus resources
++ *
++ * Entries in these enum are used as indices in the array of resource
++ * pools of an fsl_mc_bus object.
++ */
++enum fsl_mc_pool_type {
++	FSL_MC_POOL_DPMCP = 0x0,    /* corresponds to "dpmcp" in the MC */
++	FSL_MC_POOL_DPBP,	    /* corresponds to "dpbp" in the MC */
++	FSL_MC_POOL_DPCON,	    /* corresponds to "dpcon" in the MC */
++	FSL_MC_POOL_IRQ,
++
++	/*
++	 * NOTE: New resource pool types must be added before this entry
++	 */
++	FSL_MC_NUM_POOL_TYPES
++};
++
++/**
++ * struct fsl_mc_resource - MC generic resource
++ * @type: type of resource
++ * @id: unique MC resource Id within the resources of the same type
++ * @data: pointer to resource-specific data if the resource is currently
++ * allocated, or NULL if the resource is not currently allocated.
++ * @parent_pool: pointer to the parent resource pool from which this
++ * resource is allocated from.
++ * @node: Node in the free list of the corresponding resource pool
++ *
++ * NOTE: This structure is to be embedded as a field of specific
++ * MC resource structures.
++ */
++struct fsl_mc_resource {
++	enum fsl_mc_pool_type type;
++	s32 id;
++	void *data;
++	struct fsl_mc_resource_pool *parent_pool;
++	struct list_head node;
++};
++
++/**
++ * struct fsl_mc_device_irq - MC object device message-based interrupt
++ * @msi_desc: pointer to MSI descriptor allocated by fsl_mc_msi_alloc_descs()
++ * @mc_dev: MC object device that owns this interrupt
++ * @dev_irq_index: device-relative IRQ index
++ * @resource: MC generic resource associated with the interrupt
++ */
++struct fsl_mc_device_irq {
++	struct msi_desc *msi_desc;
++	struct fsl_mc_device *mc_dev;
++	u8 dev_irq_index;
++	struct fsl_mc_resource resource;
++};
++
++#define to_fsl_mc_irq(_mc_resource) \
++	container_of(_mc_resource, struct fsl_mc_device_irq, resource)
++
++/* Opened state - Indicates that an object is open by at least one owner */
++#define FSL_MC_OBJ_STATE_OPEN		0x00000001
++/* Plugged state - Indicates that the object is plugged */
++#define FSL_MC_OBJ_STATE_PLUGGED	0x00000002
++
++/**
++ * Shareability flag - Object flag indicating no memory shareability.
++ * the object generates memory accesses that are non coherent with other
++ * masters;
++ * user is responsible for proper memory handling through IOMMU configuration.
++ */
++#define FSL_MC_OBJ_FLAG_NO_MEM_SHAREABILITY	0x0001
++
++/**
++ * struct fsl_mc_obj_desc - Object descriptor
++ * @type: Type of object: NULL terminated string
++ * @id: ID of logical object resource
++ * @vendor: Object vendor identifier
++ * @ver_major: Major version number
++ * @ver_minor:  Minor version number
++ * @irq_count: Number of interrupts supported by the object
++ * @region_count: Number of mappable regions supported by the object
++ * @state: Object state: combination of FSL_MC_OBJ_STATE_ states
++ * @label: Object label: NULL terminated string
++ * @flags: Object's flags
++ */
++struct fsl_mc_obj_desc {
++	char type[16];
++	int id;
++	u16 vendor;
++	u16 ver_major;
++	u16 ver_minor;
++	u8 irq_count;
++	u8 region_count;
++	u32 state;
++	char label[16];
++	u16 flags;
++};
++
++/**
++ * Bit masks for a MC object device (struct fsl_mc_device) flags
++ */
++#define FSL_MC_IS_DPRC	0x0001
++
++/**
++ * struct fsl_mc_device - MC object device object
++ * @dev: Linux driver model device object
++ * @dma_mask: Default DMA mask
++ * @flags: MC object device flags
++ * @icid: Isolation context ID for the device
++ * @mc_handle: MC handle for the corresponding MC object opened
++ * @mc_io: Pointer to MC IO object assigned to this device or
++ * NULL if none.
++ * @obj_desc: MC description of the DPAA device
++ * @regions: pointer to array of MMIO region entries
++ * @irqs: pointer to array of pointers to interrupts allocated to this device
++ * @resource: generic resource associated with this MC object device, if any.
++ * @driver_override: Driver name to force a match
++ *
++ * Generic device object for MC object devices that are "attached" to a
++ * MC bus.
++ *
++ * NOTES:
++ * - For a non-DPRC object its icid is the same as its parent DPRC's icid.
++ * - The SMMU notifier callback gets invoked after device_add() has been
++ *   called for an MC object device, but before the device-specific probe
++ *   callback gets called.
++ * - DP_OBJ_DPRC objects are the only MC objects that have built-in MC
++ *   portals. For all other MC objects, their device drivers are responsible for
++ *   allocating MC portals for them by calling fsl_mc_portal_allocate().
++ * - Some types of MC objects (e.g., DP_OBJ_DPBP, DP_OBJ_DPCON) are
++ *   treated as resources that can be allocated/deallocated from the
++ *   corresponding resource pool in the object's parent DPRC, using the
++ *   fsl_mc_object_allocate()/fsl_mc_object_free() functions. These MC objects
++ *   are known as "allocatable" objects. For them, the corresponding
++ *   fsl_mc_device's 'resource' points to the associated resource object.
++ *   For MC objects that are not allocatable (e.g., DP_OBJ_DPRC, DP_OBJ_DPNI),
++ *   'resource' is NULL.
++ */
++struct fsl_mc_device {
++	struct device dev;
++	u64 dma_mask;
++	u16 flags;
++	u32 icid;
++	u16 mc_handle;
++	struct fsl_mc_io *mc_io;
++	struct fsl_mc_obj_desc obj_desc;
++	struct resource *regions;
++	struct fsl_mc_device_irq **irqs;
++	struct fsl_mc_resource *resource;
++	const char *driver_override;
++};
++
++#define to_fsl_mc_device(_dev) \
++	container_of(_dev, struct fsl_mc_device, dev)
++
++struct mc_cmd_header {
++	u8 src_id;
++	u8 flags_hw;
++	u8 status;
++	u8 flags_sw;
++	__le16 token;
++	__le16 cmd_id;
++};
++
++enum mc_cmd_status {
++	MC_CMD_STATUS_OK = 0x0, /* Completed successfully */
++	MC_CMD_STATUS_READY = 0x1, /* Ready to be processed */
++	MC_CMD_STATUS_AUTH_ERR = 0x3, /* Authentication error */
++	MC_CMD_STATUS_NO_PRIVILEGE = 0x4, /* No privilege */
++	MC_CMD_STATUS_DMA_ERR = 0x5, /* DMA or I/O error */
++	MC_CMD_STATUS_CONFIG_ERR = 0x6, /* Configuration error */
++	MC_CMD_STATUS_TIMEOUT = 0x7, /* Operation timed out */
++	MC_CMD_STATUS_NO_RESOURCE = 0x8, /* No resources */
++	MC_CMD_STATUS_NO_MEMORY = 0x9, /* No memory available */
++	MC_CMD_STATUS_BUSY = 0xA, /* Device is busy */
++	MC_CMD_STATUS_UNSUPPORTED_OP = 0xB, /* Unsupported operation */
++	MC_CMD_STATUS_INVALID_STATE = 0xC /* Invalid state */
++};
++
++/*
++ * MC command flags
++ */
++
++/* High priority flag */
++#define MC_CMD_FLAG_PRI		0x80
++/* Command completion flag */
++#define MC_CMD_FLAG_INTR_DIS	0x01
++
++static inline u64 mc_encode_cmd_header(u16 cmd_id,
++				       u32 cmd_flags,
++				       u16 token)
++{
++	u64 header = 0;
++	struct mc_cmd_header *hdr = (struct mc_cmd_header *)&header;
++
++	hdr->cmd_id = cpu_to_le16(cmd_id);
++	hdr->token  = cpu_to_le16(token);
++	hdr->status = MC_CMD_STATUS_READY;
++	if (cmd_flags & MC_CMD_FLAG_PRI)
++		hdr->flags_hw = MC_CMD_FLAG_PRI;
++	if (cmd_flags & MC_CMD_FLAG_INTR_DIS)
++		hdr->flags_sw = MC_CMD_FLAG_INTR_DIS;
++
++	return header;
++}
++
++static inline u16 mc_cmd_hdr_read_token(struct fsl_mc_command *cmd)
++{
++	struct mc_cmd_header *hdr = (struct mc_cmd_header *)&cmd->header;
++	u16 token = le16_to_cpu(hdr->token);
++
++	return token;
++}
++
++struct mc_rsp_create {
++	__le32 object_id;
++};
++
++struct mc_rsp_api_ver {
++	__le16 major_ver;
++	__le16 minor_ver;
++};
++
++static inline u32 mc_cmd_read_object_id(struct fsl_mc_command *cmd)
++{
++	struct mc_rsp_create *rsp_params;
++
++	rsp_params = (struct mc_rsp_create *)cmd->params;
++	return le32_to_cpu(rsp_params->object_id);
++}
++
++static inline void mc_cmd_read_api_version(struct fsl_mc_command *cmd,
++					   u16 *major_ver,
++					   u16 *minor_ver)
++{
++	struct mc_rsp_api_ver *rsp_params;
++
++	rsp_params = (struct mc_rsp_api_ver *)cmd->params;
++	*major_ver = le16_to_cpu(rsp_params->major_ver);
++	*minor_ver = le16_to_cpu(rsp_params->minor_ver);
++}
++
++/**
++ * Bit masks for a MC I/O object (struct fsl_mc_io) flags
++ */
++#define FSL_MC_IO_ATOMIC_CONTEXT_PORTAL	0x0001
++
++/**
++ * struct fsl_mc_io - MC I/O object to be passed-in to mc_send_command()
++ * @dev: device associated with this Mc I/O object
++ * @flags: flags for mc_send_command()
++ * @portal_size: MC command portal size in bytes
++ * @portal_phys_addr: MC command portal physical address
++ * @portal_virt_addr: MC command portal virtual address
++ * @dpmcp_dev: pointer to the DPMCP device associated with the MC portal.
++ *
++ * Fields are only meaningful if the FSL_MC_IO_ATOMIC_CONTEXT_PORTAL flag is not
++ * set:
++ * @mutex: Mutex to serialize mc_send_command() calls that use the same MC
++ * portal, if the fsl_mc_io object was created with the
++ * FSL_MC_IO_ATOMIC_CONTEXT_PORTAL flag off. mc_send_command() calls for this
++ * fsl_mc_io object must be made only from non-atomic context.
++ *
++ * Fields are only meaningful if the FSL_MC_IO_ATOMIC_CONTEXT_PORTAL flag is
++ * set:
++ * @spinlock: Spinlock to serialize mc_send_command() calls that use the same MC
++ * portal, if the fsl_mc_io object was created with the
++ * FSL_MC_IO_ATOMIC_CONTEXT_PORTAL flag on. mc_send_command() calls for this
++ * fsl_mc_io object can be made from atomic or non-atomic context.
++ */
++struct fsl_mc_io {
++	struct device *dev;
++	u16 flags;
++	u32 portal_size;
++	phys_addr_t portal_phys_addr;
++	void __iomem *portal_virt_addr;
++	struct fsl_mc_device *dpmcp_dev;
++	union {
++		/*
++		 * This field is only meaningful if the
++		 * FSL_MC_IO_ATOMIC_CONTEXT_PORTAL flag is not set
++		 */
++		struct mutex mutex; /* serializes mc_send_command() */
++
++		/*
++		 * This field is only meaningful if the
++		 * FSL_MC_IO_ATOMIC_CONTEXT_PORTAL flag is set
++		 */
++		spinlock_t spinlock;	/* serializes mc_send_command() */
++	};
++};
++
++int mc_send_command(struct fsl_mc_io *mc_io, struct fsl_mc_command *cmd);
++
++#ifdef CONFIG_FSL_MC_BUS
++#define dev_is_fsl_mc(_dev) ((_dev)->bus == &fsl_mc_bus_type)
++#else
++/* If fsl-mc bus is not present device cannot belong to fsl-mc bus */
++#define dev_is_fsl_mc(_dev) (0)
++#endif
++
++/* Macro to check if a device is a container device */
++#define fsl_mc_is_cont_dev(_dev) (to_fsl_mc_device(_dev)->flags & \
++	FSL_MC_IS_DPRC)
++
++/* Macro to get the container device of a MC device */
++#define fsl_mc_cont_dev(_dev) (fsl_mc_is_cont_dev(_dev) ? \
++	(_dev) : (_dev)->parent)
++
++#define fsl_mc_is_dev_coherent(_dev) \
++	(!((to_fsl_mc_device(_dev))->obj_desc.flags & \
++	FSL_MC_OBJ_FLAG_NO_MEM_SHAREABILITY))
++
++/*
++ * module_fsl_mc_driver() - Helper macro for drivers that don't do
++ * anything special in module init/exit.  This eliminates a lot of
++ * boilerplate.  Each module may only use this macro once, and
++ * calling it replaces module_init() and module_exit()
++ */
++#define module_fsl_mc_driver(__fsl_mc_driver) \
++	module_driver(__fsl_mc_driver, fsl_mc_driver_register, \
++		      fsl_mc_driver_unregister)
++
++void fsl_mc_device_remove(struct fsl_mc_device *mc_dev);
++
++/*
++ * Macro to avoid include chaining to get THIS_MODULE
++ */
++#define fsl_mc_driver_register(drv) \
++	__fsl_mc_driver_register(drv, THIS_MODULE)
++
++int __must_check __fsl_mc_driver_register(struct fsl_mc_driver *fsl_mc_driver,
++					  struct module *owner);
++
++void fsl_mc_driver_unregister(struct fsl_mc_driver *driver);
++
++int __must_check fsl_mc_portal_allocate(struct fsl_mc_device *mc_dev,
++					u16 mc_io_flags,
++					struct fsl_mc_io **new_mc_io);
++
++void fsl_mc_portal_free(struct fsl_mc_io *mc_io);
++
++int fsl_mc_portal_reset(struct fsl_mc_io *mc_io);
++
++int __must_check fsl_mc_object_allocate(struct fsl_mc_device *mc_dev,
++					enum fsl_mc_pool_type pool_type,
++					struct fsl_mc_device **new_mc_adev);
++
++void fsl_mc_object_free(struct fsl_mc_device *mc_adev);
++
++struct irq_domain *fsl_mc_msi_create_irq_domain(struct fwnode_handle *fwnode,
++						struct msi_domain_info *info,
++						struct irq_domain *parent);
++
++int __must_check fsl_mc_allocate_irqs(struct fsl_mc_device *mc_dev);
++
++void fsl_mc_free_irqs(struct fsl_mc_device *mc_dev);
++
++void fsl_mc_dma_configure(struct fsl_mc_device *mc_dev,
++	struct device_node *fsl_mc_platform_node, int coherent);
++
++extern struct bus_type fsl_mc_bus_type;
++
++extern struct device_type fsl_mc_bus_dprc_type;
++extern struct device_type fsl_mc_bus_dpni_type;
++extern struct device_type fsl_mc_bus_dpio_type;
++extern struct device_type fsl_mc_bus_dpsw_type;
++extern struct device_type fsl_mc_bus_dpdmux_type;
++extern struct device_type fsl_mc_bus_dpbp_type;
++extern struct device_type fsl_mc_bus_dpcon_type;
++extern struct device_type fsl_mc_bus_dpmcp_type;
++extern struct device_type fsl_mc_bus_dpmac_type;
++extern struct device_type fsl_mc_bus_dprtc_type;
++extern struct device_type fsl_mc_bus_dpseci_type;
++extern struct device_type fsl_mc_bus_dpdcei_type;
++extern struct device_type fsl_mc_bus_dpaiop_type;
++extern struct device_type fsl_mc_bus_dpci_type;
++extern struct device_type fsl_mc_bus_dpdmai_type;
++
++static inline bool is_fsl_mc_bus_dprc(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dprc_type;
++}
++
++static inline bool is_fsl_mc_bus_dpni(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpni_type;
++}
++
++static inline bool is_fsl_mc_bus_dpio(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpio_type;
++}
++
++static inline bool is_fsl_mc_bus_dpsw(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpsw_type;
++}
++
++static inline bool is_fsl_mc_bus_dpdmux(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpdmux_type;
++}
++
++static inline bool is_fsl_mc_bus_dpbp(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpbp_type;
++}
++
++static inline bool is_fsl_mc_bus_dpcon(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpcon_type;
++}
++
++static inline bool is_fsl_mc_bus_dpmcp(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpmcp_type;
++}
++
++static inline bool is_fsl_mc_bus_dpmac(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpmac_type;
++}
++
++static inline bool is_fsl_mc_bus_dprtc(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dprtc_type;
++}
++
++static inline bool is_fsl_mc_bus_dpseci(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpseci_type;
++}
++
++static inline bool is_fsl_mc_bus_dpdcei(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpdcei_type;
++}
++
++static inline bool is_fsl_mc_bus_dpaiop(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpaiop_type;
++}
++
++static inline bool is_fsl_mc_bus_dpci(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpci_type;
++}
++
++static inline bool is_fsl_mc_bus_dpdmai(const struct fsl_mc_device *mc_dev)
++{
++	return mc_dev->dev.type == &fsl_mc_bus_dpdmai_type;
++}
++
++/*
++ * Data Path Resource Container (DPRC) API
++ */
++
++/* Minimal supported DPRC Version */
++#define DPRC_MIN_VER_MAJOR			6
++#define DPRC_MIN_VER_MINOR			0
++
++/* DPRC command versioning */
++#define DPRC_CMD_BASE_VERSION			1
++#define DPRC_CMD_ID_OFFSET			4
++
++#define DPRC_CMD(id)	(((id) << DPRC_CMD_ID_OFFSET) | DPRC_CMD_BASE_VERSION)
++
++/* DPRC command IDs */
++#define DPRC_CMDID_CLOSE                        DPRC_CMD(0x800)
++#define DPRC_CMDID_OPEN                         DPRC_CMD(0x805)
++#define DPRC_CMDID_GET_API_VERSION              DPRC_CMD(0xa05)
++
++#define DPRC_CMDID_GET_ATTR                     DPRC_CMD(0x004)
++#define DPRC_CMDID_RESET_CONT                   DPRC_CMD(0x005)
++
++#define DPRC_CMDID_SET_IRQ                      DPRC_CMD(0x010)
++#define DPRC_CMDID_SET_IRQ_ENABLE               DPRC_CMD(0x012)
++#define DPRC_CMDID_SET_IRQ_MASK                 DPRC_CMD(0x014)
++#define DPRC_CMDID_GET_IRQ_STATUS               DPRC_CMD(0x016)
++#define DPRC_CMDID_CLEAR_IRQ_STATUS             DPRC_CMD(0x017)
++
++#define DPRC_CMDID_GET_CONT_ID                  DPRC_CMD(0x830)
++#define DPRC_CMDID_GET_OBJ_COUNT                DPRC_CMD(0x159)
++#define DPRC_CMDID_GET_OBJ                      DPRC_CMD(0x15A)
++#define DPRC_CMDID_GET_OBJ_REG                  DPRC_CMD(0x15E)
++#define DPRC_CMDID_SET_OBJ_IRQ                  DPRC_CMD(0x15F)
++
++struct dprc_cmd_open {
++	__le32 container_id;
++};
++
++struct dprc_cmd_reset_container {
++	__le32 child_container_id;
++};
++
++struct dprc_cmd_set_irq {
++	/* cmd word 0 */
++	__le32 irq_val;
++	u8 irq_index;
++	u8 pad[3];
++	/* cmd word 1 */
++	__le64 irq_addr;
++	/* cmd word 2 */
++	__le32 irq_num;
++};
++
++#define DPRC_ENABLE		0x1
++
++struct dprc_cmd_set_irq_enable {
++	u8 enable;
++	u8 pad[3];
++	u8 irq_index;
++};
++
++struct dprc_cmd_set_irq_mask {
++	__le32 mask;
++	u8 irq_index;
++};
++
++struct dprc_cmd_get_irq_status {
++	__le32 status;
++	u8 irq_index;
++};
++
++struct dprc_rsp_get_irq_status {
++	__le32 status;
++};
++
++struct dprc_cmd_clear_irq_status {
++	__le32 status;
++	u8 irq_index;
++};
++
++struct dprc_rsp_get_attributes {
++	/* response word 0 */
++	__le32 container_id;
++	__le32 icid;
++	/* response word 1 */
++	__le32 options;
++	__le32 portal_id;
++};
++
++struct dprc_rsp_get_obj_count {
++	__le32 pad;
++	__le32 obj_count;
++};
++
++struct dprc_cmd_get_obj {
++	__le32 obj_index;
++};
++
++struct dprc_rsp_get_obj {
++	/* response word 0 */
++	__le32 pad0;
++	__le32 id;
++	/* response word 1 */
++	__le16 vendor;
++	u8 irq_count;
++	u8 region_count;
++	__le32 state;
++	/* response word 2 */
++	__le16 version_major;
++	__le16 version_minor;
++	__le16 flags;
++	__le16 pad1;
++	/* response word 3-4 */
++	u8 type[16];
++	/* response word 5-6 */
++	u8 label[16];
++};
++
++struct dprc_cmd_get_obj_region {
++	/* cmd word 0 */
++	__le32 obj_id;
++	__le16 pad0;
++	u8 region_index;
++	u8 pad1;
++	/* cmd word 1-2 */
++	__le64 pad2[2];
++	/* cmd word 3-4 */
++	u8 obj_type[16];
++};
++
++struct dprc_rsp_get_obj_region {
++	/* response word 0 */
++	__le64 pad0;
++	/* response word 1 */
++	__le32 base_addr;
++	__le32 pad1;
++	/* response word 2 */
++	__le32 size;
++	u8 type;
++	u8 pad2[3];
++	/* response word 3 */
++	__le32 flags;
++};
++
++struct dprc_cmd_set_obj_irq {
++	/* cmd word 0 */
++	__le32 irq_val;
++	u8 irq_index;
++	u8 pad[3];
++	/* cmd word 1 */
++	__le64 irq_addr;
++	/* cmd word 2 */
++	__le32 irq_num;
++	__le32 obj_id;
++	/* cmd word 3-4 */
++	u8 obj_type[16];
++};
++
++/*
++ * DPRC API for managing and querying DPAA resources
++ */
++int dprc_open(struct fsl_mc_io *mc_io,
++	      u32 cmd_flags,
++	      int container_id,
++	      u16 *token);
++
++int dprc_close(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       u16 token);
++
++/* DPRC IRQ events */
++
++/* IRQ event - Indicates that a new object added to the container */
++#define DPRC_IRQ_EVENT_OBJ_ADDED		0x00000001
++/* IRQ event - Indicates that an object was removed from the container */
++#define DPRC_IRQ_EVENT_OBJ_REMOVED		0x00000002
++/*
++ * IRQ event - Indicates that one of the descendant containers that opened by
++ * this container is destroyed
++ */
++#define DPRC_IRQ_EVENT_CONTAINER_DESTROYED	0x00000010
++
++/*
++ * IRQ event - Indicates that on one of the container's opened object is
++ * destroyed
++ */
++#define DPRC_IRQ_EVENT_OBJ_DESTROYED		0x00000020
++
++/* Irq event - Indicates that object is created at the container */
++#define DPRC_IRQ_EVENT_OBJ_CREATED		0x00000040
++
++/**
++ * struct dprc_irq_cfg - IRQ configuration
++ * @paddr:	Address that must be written to signal a message-based interrupt
++ * @val:	Value to write into irq_addr address
++ * @irq_num:	A user defined number associated with this IRQ
++ */
++struct dprc_irq_cfg {
++	     phys_addr_t paddr;
++	     u32 val;
++	     int irq_num;
++};
++
++int dprc_set_irq(struct fsl_mc_io *mc_io,
++		 u32 cmd_flags,
++		 u16 token,
++		 u8 irq_index,
++		 struct dprc_irq_cfg *irq_cfg);
++
++int dprc_set_irq_enable(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			u8 irq_index,
++			u8 en);
++
++int dprc_set_irq_mask(struct fsl_mc_io *mc_io,
++		      u32 cmd_flags,
++		      u16 token,
++		      u8 irq_index,
++		      u32 mask);
++
++int dprc_get_irq_status(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			u8 irq_index,
++			u32 *status);
++
++int dprc_clear_irq_status(struct fsl_mc_io *mc_io,
++			  u32 cmd_flags,
++			  u16 token,
++			  u8 irq_index,
++			  u32 status);
++
++/**
++ * struct dprc_attributes - Container attributes
++ * @container_id: Container's ID
++ * @icid: Container's ICID
++ * @portal_id: Container's portal ID
++ * @options: Container's options as set at container's creation
++ */
++struct dprc_attributes {
++	int container_id;
++	u32 icid;
++	int portal_id;
++	u64 options;
++};
++
++int dprc_get_attributes(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			struct dprc_attributes *attributes);
++
++int dprc_get_obj_count(struct fsl_mc_io *mc_io,
++		       u32 cmd_flags,
++		       u16 token,
++		       int *obj_count);
++
++int dprc_get_obj(struct fsl_mc_io *mc_io,
++		 u32 cmd_flags,
++		 u16 token,
++		 int obj_index,
++		 struct fsl_mc_obj_desc *obj_desc);
++
++int dprc_set_obj_irq(struct fsl_mc_io *mc_io,
++		     u32 cmd_flags,
++		     u16 token,
++		     char *obj_type,
++		     int obj_id,
++		     u8 irq_index,
++		     struct dprc_irq_cfg *irq_cfg);
++
++/* Region flags */
++/* Cacheable - Indicates that region should be mapped as cacheable */
++#define DPRC_REGION_CACHEABLE	0x00000001
++
++/**
++ * enum dprc_region_type - Region type
++ * @DPRC_REGION_TYPE_MC_PORTAL: MC portal region
++ * @DPRC_REGION_TYPE_QBMAN_PORTAL: Qbman portal region
++ */
++enum dprc_region_type {
++	DPRC_REGION_TYPE_MC_PORTAL,
++	DPRC_REGION_TYPE_QBMAN_PORTAL
++};
++
++#define DPRC_OBJ_FLAG_NO_MEM_SHAREABILITY       0x0001
++
++/**
++ * struct dprc_region_desc - Mappable region descriptor
++ * @base_offset: Region offset from region's base address.
++ *	For DPMCP and DPRC objects, region base is offset from SoC MC portals
++ *	base address; For DPIO, region base is offset from SoC QMan portals
++ *	base address
++ * @size: Region size (in bytes)
++ * @flags: Region attributes
++ * @type: Portal region type
++ */
++struct dprc_region_desc {
++	u32 base_offset;
++	u32 size;
++	u32 flags;
++	enum dprc_region_type type;
++};
++
++int dprc_get_obj_region(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			char *obj_type,
++			int obj_id,
++			u8 region_index,
++			struct dprc_region_desc *region_desc);
++
++int dprc_get_api_version(struct fsl_mc_io *mc_io,
++			 u32 cmd_flags,
++			 u16 *major_ver,
++			 u16 *minor_ver);
++
++int dprc_get_container_id(struct fsl_mc_io *mc_io,
++			  u32 cmd_flags,
++			  int *container_id);
++
++int dprc_reset_container(struct fsl_mc_io *mc_io,
++			 u32 cmd_flags,
++			 u16 token,
++			 int child_container_id);
++
++/*
++ * Data Path Buffer Pool (DPBP) API
++ * Contains initialization APIs and runtime control APIs for DPBP
++ */
++
++int dpbp_open(struct fsl_mc_io *mc_io,
++	      u32 cmd_flags,
++	      int dpbp_id,
++	      u16 *token);
++
++int dpbp_close(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       u16 token);
++
++int dpbp_enable(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token);
++
++int dpbp_disable(struct fsl_mc_io *mc_io,
++		 u32 cmd_flags,
++		 u16 token);
++
++int dpbp_reset(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       u16 token);
++
++/**
++ * struct dpbp_attr - Structure representing DPBP attributes
++ * @id:		DPBP object ID
++ * @bpid:	Hardware buffer pool ID; should be used as an argument in
++ *		acquire/release operations on buffers
++ */
++struct dpbp_attr {
++	int id;
++	u16 bpid;
++};
++
++int dpbp_get_attributes(struct fsl_mc_io *mc_io,
++			u32 cmd_flags,
++			u16 token,
++			struct dpbp_attr *attr);
++
++/* Data Path Concentrator (DPCON) API
++ * Contains initialization APIs and runtime control APIs for DPCON
++ */
++
++/**
++ * Use it to disable notifications; see dpcon_set_notification()
++ */
++#define DPCON_INVALID_DPIO_ID		(int)(-1)
++
++int dpcon_open(struct fsl_mc_io *mc_io,
++	       u32 cmd_flags,
++	       int dpcon_id,
++	       u16 *token);
++
++int dpcon_close(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token);
++
++int dpcon_enable(struct fsl_mc_io *mc_io,
++		 u32 cmd_flags,
++		 u16 token);
++
++int dpcon_disable(struct fsl_mc_io *mc_io,
++		  u32 cmd_flags,
++		  u16 token);
++
++int dpcon_reset(struct fsl_mc_io *mc_io,
++		u32 cmd_flags,
++		u16 token);
++
++/**
++ * struct dpcon_attr - Structure representing DPCON attributes
++ * @id: DPCON object ID
++ * @qbman_ch_id: Channel ID to be used by dequeue operation
++ * @num_priorities: Number of priorities for the DPCON channel (1-8)
++ */
++struct dpcon_attr {
++	int id;
++	u16 qbman_ch_id;
++	u8 num_priorities;
++};
++
++int dpcon_get_attributes(struct fsl_mc_io *mc_io,
++			 u32 cmd_flags,
++			 u16 token,
++			 struct dpcon_attr *attr);
++
++/**
++ * struct dpcon_notification_cfg - Structure representing notification params
++ * @dpio_id:	DPIO object ID; must be configured with a notification channel;
++ *	to disable notifications set it to 'DPCON_INVALID_DPIO_ID';
++ * @priority:	Priority selection within the DPIO channel; valid values
++ *		are 0-7, depending on the number of priorities in that channel
++ * @user_ctx:	User context value provided with each CDAN message
++ */
++struct dpcon_notification_cfg {
++	int dpio_id;
++	u8 priority;
++	u64 user_ctx;
++};
++
++int dpcon_set_notification(struct fsl_mc_io *mc_io,
++			   u32 cmd_flags,
++			   u16 token,
++			   struct dpcon_notification_cfg *cfg);
++
++struct irq_domain;
++struct msi_domain_info;
++
++/**
++ * Maximum number of total IRQs that can be pre-allocated for an MC bus'
++ * IRQ pool
++ */
++#define FSL_MC_IRQ_POOL_MAX_TOTAL_IRQS	256
++
++/**
++ * struct fsl_mc_resource_pool - Pool of MC resources of a given
++ * type
++ * @type: type of resources in the pool
++ * @max_count: maximum number of resources in the pool
++ * @free_count: number of free resources in the pool
++ * @mutex: mutex to serialize access to the pool's free list
++ * @free_list: anchor node of list of free resources in the pool
++ * @mc_bus: pointer to the MC bus that owns this resource pool
++ */
++struct fsl_mc_resource_pool {
++	enum fsl_mc_pool_type type;
++	int max_count;
++	int free_count;
++	struct mutex mutex;	/* serializes access to free_list */
++	struct list_head free_list;
++	struct fsl_mc_bus *mc_bus;
++};
++
++/**
++ * struct fsl_mc_restool - information associated with a restool device file
++ * @cdev: struct char device linked to the root dprc
++ * @dev: dev_t for the char device to be added
++ * @device: newly created device in /dev
++ * @mutex: mutex lock to serialize the open/release operations
++ * @local_instance_in_use: local MC I/O instance in use or not
++ * @dynamic_instance_count: number of dynamically created MC I/O instances
++ */
++struct fsl_mc_restool {
++	struct cdev cdev;
++	dev_t dev;
++	struct device *device;
++	struct mutex mutex; /* serialize open/release operations */
++	bool local_instance_in_use;
++	u32 dynamic_instance_count;
++};
++
++/**
++ * struct fsl_mc_bus - logical bus that corresponds to a physical DPRC
++ * @mc_dev: fsl-mc device for the bus device itself.
++ * @resource_pools: array of resource pools (one pool per resource type)
++ * for this MC bus. These resources represent allocatable entities
++ * from the physical DPRC.
++ * @irq_resources: Pointer to array of IRQ objects for the IRQ pool
++ * @scan_mutex: Serializes bus scanning
++ * @dprc_attr: DPRC attributes
++ * @restool_misc: struct that abstracts the interaction with userspace restool
++ */
++struct fsl_mc_bus {
++	struct fsl_mc_device mc_dev;
++	struct fsl_mc_resource_pool resource_pools[FSL_MC_NUM_POOL_TYPES];
++	struct fsl_mc_device_irq *irq_resources;
++	struct mutex scan_mutex;    /* serializes bus scanning */
++	struct dprc_attributes dprc_attr;
++	struct fsl_mc_restool restool_misc;
++};
++
++int dprc_scan_objects(struct fsl_mc_device *mc_bus_dev,
++		      const char *driver_override,
++		      unsigned int *total_irq_count);
++
++int fsl_mc_find_msi_domain(struct device *mc_platform_dev,
++			   struct irq_domain **mc_msi_domain);
++
++int fsl_mc_populate_irq_pool(struct fsl_mc_bus *mc_bus,
++			     unsigned int irq_count);
++
++void fsl_mc_cleanup_irq_pool(struct fsl_mc_bus *mc_bus);
++
++void fsl_mc_init_all_resource_pools(struct fsl_mc_device *mc_bus_dev);
++
++void fsl_mc_cleanup_all_resource_pools(struct fsl_mc_device *mc_bus_dev);
++
++void fsl_mc_get_root_dprc(struct device *dev, struct device **root_dprc_dev);
++
++#endif /* _FSL_MC_H_ */
+diff --git a/include/linux/fsl/svr.h b/include/linux/fsl/svr.h
+new file mode 100644
+index 00000000..e95c8f43
+--- /dev/null
++++ b/include/linux/fsl/svr.h
+@@ -0,0 +1,97 @@
++/*
++ * MPC85xx cpu type detection
++ *
++ * Copyright 2011-2012 Freescale Semiconductor, Inc.
++ *
++ * This is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License as published by
++ * the Free Software Foundation; either version 2 of the License, or
++ * (at your option) any later version.
++ */
++
++#ifndef FSL_SVR_H
++#define FSL_SVR_H
++
++#define SVR_REV(svr)	((svr) & 0xFF)		/* SOC design resision */
++#define SVR_MAJ(svr)	(((svr) >>  4) & 0xF)	/* Major revision field*/
++#define SVR_MIN(svr)	(((svr) >>  0) & 0xF)	/* Minor revision field*/
++
++/* Some parts define SVR[0:23] as the SOC version */
++#define SVR_SOC_VER(svr) (((svr) >> 8) & 0xFFF7FF)	/* SOC Version fields */
++
++#define SVR_8533	0x803400
++#define SVR_8535	0x803701
++#define SVR_8536	0x803700
++#define SVR_8540	0x803000
++#define SVR_8541	0x807200
++#define SVR_8543	0x803200
++#define SVR_8544	0x803401
++#define SVR_8545	0x803102
++#define SVR_8547	0x803101
++#define SVR_8548	0x803100
++#define SVR_8555	0x807100
++#define SVR_8560	0x807000
++#define SVR_8567	0x807501
++#define SVR_8568	0x807500
++#define SVR_8569	0x808000
++#define SVR_8572	0x80E000
++#define SVR_P1010	0x80F100
++#define SVR_P1011	0x80E500
++#define SVR_P1012	0x80E501
++#define SVR_P1013	0x80E700
++#define SVR_P1014	0x80F101
++#define SVR_P1017	0x80F700
++#define SVR_P1020	0x80E400
++#define SVR_P1021	0x80E401
++#define SVR_P1022	0x80E600
++#define SVR_P1023	0x80F600
++#define SVR_P1024	0x80E402
++#define SVR_P1025	0x80E403
++#define SVR_P2010	0x80E300
++#define SVR_P2020	0x80E200
++#define SVR_P2040	0x821000
++#define SVR_P2041	0x821001
++#define SVR_P3041	0x821103
++#define SVR_P4040	0x820100
++#define SVR_P4080	0x820000
++#define SVR_P5010	0x822100
++#define SVR_P5020	0x822000
++#define SVR_P5021	0X820500
++#define SVR_P5040	0x820400
++#define SVR_T4240	0x824000
++#define SVR_T4120	0x824001
++#define SVR_T4160	0x824100
++#define SVR_T4080	0x824102
++#define SVR_C291	0x850000
++#define SVR_C292	0x850020
++#define SVR_C293	0x850030
++#define SVR_B4860	0X868000
++#define SVR_G4860	0x868001
++#define SVR_G4060	0x868003
++#define SVR_B4440	0x868100
++#define SVR_G4440	0x868101
++#define SVR_B4420	0x868102
++#define SVR_B4220	0x868103
++#define SVR_T1040	0x852000
++#define SVR_T1041	0x852001
++#define SVR_T1042	0x852002
++#define SVR_T1020	0x852100
++#define SVR_T1021	0x852101
++#define SVR_T1022	0x852102
++#define SVR_T1023	0x854100
++#define SVR_T1024	0x854000
++#define SVR_T2080	0x853000
++#define SVR_T2081	0x853100
++
++#define SVR_8610	0x80A000
++#define SVR_8641	0x809000
++#define SVR_8641D	0x809001
++
++#define SVR_9130	0x860001
++#define SVR_9131	0x860000
++#define SVR_9132	0x861000
++#define SVR_9232	0x861400
++
++#define SVR_Unknown	0xFFFFFF
++
++#endif
+diff --git a/include/linux/iommu.h b/include/linux/iommu.h
+index 188599f5..7838e92d 100644
+--- a/include/linux/iommu.h
++++ b/include/linux/iommu.h
+@@ -351,6 +351,8 @@ static inline size_t iommu_map_sg(struct iommu_domain *domain,
+ extern struct iommu_group *pci_device_group(struct device *dev);
+ /* Generic device grouping function */
+ extern struct iommu_group *generic_device_group(struct device *dev);
++/* FSL-MC device grouping function */
++struct iommu_group *fsl_mc_device_group(struct device *dev);
+ 
+ /**
+  * struct iommu_fwspec - per-device IOMMU instance data
+diff --git a/include/linux/irqdomain.h b/include/linux/irqdomain.h
+index 188eced6..266d0129 100644
+--- a/include/linux/irqdomain.h
++++ b/include/linux/irqdomain.h
+@@ -138,6 +138,7 @@ struct irq_domain_chip_generic;
+  *      setting up one or more generic chips for interrupt controllers
+  *      drivers using the generic chip library which uses this pointer.
+  * @parent: Pointer to parent irq_domain to support hierarchy irq_domains
++ * @debugfs_file: dentry for the domain debugfs file
+  *
+  * Revmap data, used internally by irq_domain
+  * @revmap_direct_max_irq: The largest hwirq that can be set for controllers that
+@@ -160,6 +161,9 @@ struct irq_domain {
+ #ifdef	CONFIG_IRQ_DOMAIN_HIERARCHY
+ 	struct irq_domain *parent;
+ #endif
++#ifdef CONFIG_GENERIC_IRQ_DEBUGFS
++	struct dentry		*debugfs_file;
++#endif
+ 
+ 	/* reverse map data. The linear map gets appended to the irq_domain */
+ 	irq_hw_number_t hwirq_max;
+@@ -174,8 +178,8 @@ enum {
+ 	/* Irq domain is hierarchical */
+ 	IRQ_DOMAIN_FLAG_HIERARCHY	= (1 << 0),
+ 
+-	/* Core calls alloc/free recursive through the domain hierarchy. */
+-	IRQ_DOMAIN_FLAG_AUTO_RECURSIVE	= (1 << 1),
++	/* Irq domain name was allocated in __irq_domain_add() */
++	IRQ_DOMAIN_NAME_ALLOCATED	= (1 << 6),
+ 
+ 	/* Irq domain is an IPI domain with virq per cpu */
+ 	IRQ_DOMAIN_FLAG_IPI_PER_CPU	= (1 << 2),
+@@ -238,6 +242,9 @@ static inline bool is_fwnode_irqchip(struct fwnode_handle *fwnode)
+ 	return fwnode && fwnode->type == FWNODE_IRQCHIP;
+ }
+ 
++extern void irq_domain_update_bus_token(struct irq_domain *domain,
++					enum irq_domain_bus_token bus_token);
++
+ static inline
+ struct irq_domain *irq_find_matching_fwnode(struct fwnode_handle *fwnode,
+ 					    enum irq_domain_bus_token bus_token)
+@@ -410,7 +417,7 @@ static inline int irq_domain_alloc_irqs(struct irq_domain *domain,
+ 				       NULL);
+ }
+ 
+-extern int irq_domain_alloc_irqs_recursive(struct irq_domain *domain,
++extern int irq_domain_alloc_irqs_hierarchy(struct irq_domain *domain,
+ 					   unsigned int irq_base,
+ 					   unsigned int nr_irqs, void *arg);
+ extern int irq_domain_set_hwirq_and_chip(struct irq_domain *domain,
+diff --git a/include/net/switchdev.h b/include/net/switchdev.h
+index eba80c4f..5d899ecd 100644
+--- a/include/net/switchdev.h
++++ b/include/net/switchdev.h
+@@ -46,6 +46,7 @@ enum switchdev_attr_id {
+ 	SWITCHDEV_ATTR_ID_PORT_PARENT_ID,
+ 	SWITCHDEV_ATTR_ID_PORT_STP_STATE,
+ 	SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS,
++	SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS_SUPPORT,
+ 	SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME,
+ 	SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING,
+ };
+@@ -60,6 +61,7 @@ struct switchdev_attr {
+ 		struct netdev_phys_item_id ppid;	/* PORT_PARENT_ID */
+ 		u8 stp_state;				/* PORT_STP_STATE */
+ 		unsigned long brport_flags;		/* PORT_BRIDGE_FLAGS */
++		unsigned long brport_flags_support;	/* PORT_BRIDGE_FLAGS_SUPPORT */
+ 		clock_t ageing_time;			/* BRIDGE_AGEING_TIME */
+ 		bool vlan_filtering;			/* BRIDGE_VLAN_FILTERING */
+ 	} u;
+@@ -149,8 +151,10 @@ struct switchdev_ops {
+ };
+ 
+ enum switchdev_notifier_type {
+-	SWITCHDEV_FDB_ADD = 1,
+-	SWITCHDEV_FDB_DEL,
++	SWITCHDEV_FDB_ADD_TO_BRIDGE = 1,
++	SWITCHDEV_FDB_DEL_TO_BRIDGE,
++	SWITCHDEV_FDB_ADD_TO_DEVICE,
++	SWITCHDEV_FDB_DEL_TO_DEVICE,
+ };
+ 
+ struct switchdev_notifier_info {
+diff --git a/include/uapi/linux/fsl_mc.h b/include/uapi/linux/fsl_mc.h
+new file mode 100644
+index 00000000..e4c8dd01
+--- /dev/null
++++ b/include/uapi/linux/fsl_mc.h
+@@ -0,0 +1,31 @@
++/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
++/*
++ * Management Complex (MC) userspace public interface
++ *
++ * Copyright 2018 NXP
++ *
++ */
++#ifndef _UAPI_FSL_MC_H_
++#define _UAPI_FSL_MC_H_
++
++#define MC_CMD_NUM_OF_PARAMS	7
++
++/**
++ * struct fsl_mc_command - Management Complex (MC) command structure
++ * @header: MC command header
++ * @params: MC command parameters
++ *
++ * Used by RESTOOL_SEND_MC_COMMAND
++ */
++struct fsl_mc_command {
++	__u64 header;
++	__u64 params[MC_CMD_NUM_OF_PARAMS];
++};
++
++#define RESTOOL_IOCTL_TYPE	'R'
++#define RESTOOL_IOCTL_SEQ	0xE0
++
++#define RESTOOL_SEND_MC_COMMAND \
++	_IOWR(RESTOOL_IOCTL_TYPE, RESTOOL_IOCTL_SEQ, struct fsl_mc_command)
++
++#endif /* _UAPI_FSL_MC_H_ */
+diff --git a/kernel/irq/irqdomain.c b/kernel/irq/irqdomain.c
+index 31805f23..e6d5ed94 100644
+--- a/kernel/irq/irqdomain.c
++++ b/kernel/irq/irqdomain.c
+@@ -31,6 +31,14 @@ struct irqchip_fwid {
+ 	void *data;
+ };
+ 
++#ifdef CONFIG_GENERIC_IRQ_DEBUGFS
++static void debugfs_add_domain_dir(struct irq_domain *d);
++static void debugfs_remove_domain_dir(struct irq_domain *d);
++#else
++static inline void debugfs_add_domain_dir(struct irq_domain *d) { }
++static inline void debugfs_remove_domain_dir(struct irq_domain *d) { }
++#endif
++
+ /**
+  * irq_domain_alloc_fwnode - Allocate a fwnode_handle suitable for
+  *                           identifying an irq domain
+@@ -117,6 +125,7 @@ struct irq_domain *__irq_domain_add(struct fwnode_handle *fwnode, int size,
+ 	irq_domain_check_hierarchy(domain);
+ 
+ 	mutex_lock(&irq_domain_mutex);
++	debugfs_add_domain_dir(domain);
+ 	list_add(&domain->link, &irq_domain_list);
+ 	mutex_unlock(&irq_domain_mutex);
+ 
+@@ -136,6 +145,7 @@ EXPORT_SYMBOL_GPL(__irq_domain_add);
+ void irq_domain_remove(struct irq_domain *domain)
+ {
+ 	mutex_lock(&irq_domain_mutex);
++	debugfs_remove_domain_dir(domain);
+ 
+ 	WARN_ON(!radix_tree_empty(&domain->revmap_tree));
+ 
+@@ -156,6 +166,37 @@ void irq_domain_remove(struct irq_domain *domain)
+ }
+ EXPORT_SYMBOL_GPL(irq_domain_remove);
+ 
++void irq_domain_update_bus_token(struct irq_domain *domain,
++				 enum irq_domain_bus_token bus_token)
++{
++	char *name;
++
++	if (domain->bus_token == bus_token)
++		return;
++
++	mutex_lock(&irq_domain_mutex);
++
++	domain->bus_token = bus_token;
++
++	name = kasprintf(GFP_KERNEL, "%s-%d", domain->name, bus_token);
++	if (!name) {
++		mutex_unlock(&irq_domain_mutex);
++		return;
++	}
++
++	debugfs_remove_domain_dir(domain);
++
++	if (domain->flags & IRQ_DOMAIN_NAME_ALLOCATED)
++		kfree(domain->name);
++	else
++		domain->flags |= IRQ_DOMAIN_NAME_ALLOCATED;
++
++	domain->name = name;
++	debugfs_add_domain_dir(domain);
++
++	mutex_unlock(&irq_domain_mutex);
++}
++
+ /**
+  * irq_domain_add_simple() - Register an irq_domain and optionally map a range of irqs
+  * @of_node: pointer to interrupt controller's device tree node.
+@@ -1189,43 +1230,18 @@ void irq_domain_free_irqs_top(struct irq_domain *domain, unsigned int virq,
+ 	irq_domain_free_irqs_common(domain, virq, nr_irqs);
+ }
+ 
+-static bool irq_domain_is_auto_recursive(struct irq_domain *domain)
+-{
+-	return domain->flags & IRQ_DOMAIN_FLAG_AUTO_RECURSIVE;
+-}
+-
+-static void irq_domain_free_irqs_recursive(struct irq_domain *domain,
++static void irq_domain_free_irqs_hierarchy(struct irq_domain *domain,
+ 					   unsigned int irq_base,
+ 					   unsigned int nr_irqs)
+ {
+ 	domain->ops->free(domain, irq_base, nr_irqs);
+-	if (irq_domain_is_auto_recursive(domain)) {
+-		BUG_ON(!domain->parent);
+-		irq_domain_free_irqs_recursive(domain->parent, irq_base,
+-					       nr_irqs);
+-	}
+ }
+ 
+-int irq_domain_alloc_irqs_recursive(struct irq_domain *domain,
++int irq_domain_alloc_irqs_hierarchy(struct irq_domain *domain,
+ 				    unsigned int irq_base,
+ 				    unsigned int nr_irqs, void *arg)
+ {
+-	int ret = 0;
+-	struct irq_domain *parent = domain->parent;
+-	bool recursive = irq_domain_is_auto_recursive(domain);
+-
+-	BUG_ON(recursive && !parent);
+-	if (recursive)
+-		ret = irq_domain_alloc_irqs_recursive(parent, irq_base,
+-						      nr_irqs, arg);
+-	if (ret < 0)
+-		return ret;
+-
+-	ret = domain->ops->alloc(domain, irq_base, nr_irqs, arg);
+-	if (ret < 0 && recursive)
+-		irq_domain_free_irqs_recursive(parent, irq_base, nr_irqs);
+-
+-	return ret;
++	return domain->ops->alloc(domain, irq_base, nr_irqs, arg);
+ }
+ 
+ /**
+@@ -1286,7 +1302,7 @@ int __irq_domain_alloc_irqs(struct irq_domain *domain, int irq_base,
+ 	}
+ 
+ 	mutex_lock(&irq_domain_mutex);
+-	ret = irq_domain_alloc_irqs_recursive(domain, virq, nr_irqs, arg);
++	ret = irq_domain_alloc_irqs_hierarchy(domain, virq, nr_irqs, arg);
+ 	if (ret < 0) {
+ 		mutex_unlock(&irq_domain_mutex);
+ 		goto out_free_irq_data;
+@@ -1321,7 +1337,7 @@ void irq_domain_free_irqs(unsigned int virq, unsigned int nr_irqs)
+ 	mutex_lock(&irq_domain_mutex);
+ 	for (i = 0; i < nr_irqs; i++)
+ 		irq_domain_remove_irq(virq + i);
+-	irq_domain_free_irqs_recursive(data->domain, virq, nr_irqs);
++	irq_domain_free_irqs_hierarchy(data->domain, virq, nr_irqs);
+ 	mutex_unlock(&irq_domain_mutex);
+ 
+ 	irq_domain_free_irq_data(virq, nr_irqs);
+@@ -1341,15 +1357,11 @@ int irq_domain_alloc_irqs_parent(struct irq_domain *domain,
+ 				 unsigned int irq_base, unsigned int nr_irqs,
+ 				 void *arg)
+ {
+-	/* irq_domain_alloc_irqs_recursive() has called parent's alloc() */
+-	if (irq_domain_is_auto_recursive(domain))
+-		return 0;
++	if (!domain->parent)
++		return -ENOSYS;
+ 
+-	domain = domain->parent;
+-	if (domain)
+-		return irq_domain_alloc_irqs_recursive(domain, irq_base,
+-						       nr_irqs, arg);
+-	return -ENOSYS;
++	return irq_domain_alloc_irqs_hierarchy(domain->parent, irq_base,
++					       nr_irqs, arg);
+ }
+ EXPORT_SYMBOL_GPL(irq_domain_alloc_irqs_parent);
+ 
+@@ -1364,10 +1376,10 @@ EXPORT_SYMBOL_GPL(irq_domain_alloc_irqs_parent);
+ void irq_domain_free_irqs_parent(struct irq_domain *domain,
+ 				 unsigned int irq_base, unsigned int nr_irqs)
+ {
+-	/* irq_domain_free_irqs_recursive() will call parent's free */
+-	if (!irq_domain_is_auto_recursive(domain) && domain->parent)
+-		irq_domain_free_irqs_recursive(domain->parent, irq_base,
+-					       nr_irqs);
++	if (!domain->parent)
++		return;
++
++	irq_domain_free_irqs_hierarchy(domain->parent, irq_base, nr_irqs);
+ }
+ EXPORT_SYMBOL_GPL(irq_domain_free_irqs_parent);
+ 
+@@ -1487,3 +1499,78 @@ static void irq_domain_check_hierarchy(struct irq_domain *domain)
+ {
+ }
+ #endif	/* CONFIG_IRQ_DOMAIN_HIERARCHY */
++
++#ifdef CONFIG_GENERIC_IRQ_DEBUGFS
++static struct dentry *domain_dir;
++
++static void
++irq_domain_debug_show_one(struct seq_file *m, struct irq_domain *d, int ind)
++{
++	seq_printf(m, "%*sname:   %s\n", ind, "", d->name);
++	seq_printf(m, "%*ssize:   %u\n", ind + 1, "",
++		   d->revmap_size + d->revmap_direct_max_irq);
++	seq_printf(m, "%*smapped: %u\n", ind + 1, "", d->mapcount);
++	seq_printf(m, "%*sflags:  0x%08x\n", ind +1 , "", d->flags);
++#ifdef	CONFIG_IRQ_DOMAIN_HIERARCHY
++	if (!d->parent)
++		return;
++	seq_printf(m, "%*sparent: %s\n", ind + 1, "", d->parent->name);
++	irq_domain_debug_show_one(m, d->parent, ind + 4);
++#endif
++}
++
++static int irq_domain_debug_show(struct seq_file *m, void *p)
++{
++	struct irq_domain *d = m->private;
++
++	/* Default domain? Might be NULL */
++	if (!d) {
++		if (!irq_default_domain)
++			return 0;
++		d = irq_default_domain;
++	}
++	irq_domain_debug_show_one(m, d, 0);
++	return 0;
++}
++
++static int irq_domain_debug_open(struct inode *inode, struct file *file)
++{
++	return single_open(file, irq_domain_debug_show, inode->i_private);
++}
++
++static const struct file_operations dfs_domain_ops = {
++	.open		= irq_domain_debug_open,
++	.read		= seq_read,
++	.llseek		= seq_lseek,
++	.release	= single_release,
++};
++
++static void debugfs_add_domain_dir(struct irq_domain *d)
++{
++	if (!d->name || !domain_dir || d->debugfs_file)
++		return;
++	d->debugfs_file = debugfs_create_file(d->name, 0444, domain_dir, d,
++					      &dfs_domain_ops);
++}
++
++static void debugfs_remove_domain_dir(struct irq_domain *d)
++{
++	if (d->debugfs_file)
++		debugfs_remove(d->debugfs_file);
++}
++
++void __init irq_domain_debugfs_init(struct dentry *root)
++{
++	struct irq_domain *d;
++
++	domain_dir = debugfs_create_dir("domains", root);
++	if (!domain_dir)
++		return;
++
++	debugfs_create_file("default", 0444, domain_dir, NULL, &dfs_domain_ops);
++	mutex_lock(&irq_domain_mutex);
++	list_for_each_entry(d, &irq_domain_list, link)
++		debugfs_add_domain_dir(d);
++	mutex_unlock(&irq_domain_mutex);
++}
++#endif
+diff --git a/kernel/irq/msi.c b/kernel/irq/msi.c
+index 8a3e8727..6fa21a82 100644
+--- a/kernel/irq/msi.c
++++ b/kernel/irq/msi.c
+@@ -272,8 +272,8 @@ struct irq_domain *msi_create_irq_domain(struct fwnode_handle *fwnode,
+ 	if (info->flags & MSI_FLAG_USE_DEF_CHIP_OPS)
+ 		msi_domain_update_chip_ops(info);
+ 
+-	return irq_domain_create_hierarchy(parent, 0, 0, fwnode,
+-					   &msi_domain_ops, info);
++	return irq_domain_create_hierarchy(parent, IRQ_DOMAIN_FLAG_MSI, 0,
++					   fwnode, &msi_domain_ops, info);
+ }
+ 
+ int msi_domain_prepare_irqs(struct irq_domain *domain, struct device *dev,
+@@ -310,7 +310,7 @@ int msi_domain_populate_irqs(struct irq_domain *domain, struct device *dev,
+ 
+ 		ops->set_desc(arg, desc);
+ 		/* Assumes the domain mutex is held! */
+-		ret = irq_domain_alloc_irqs_recursive(domain, virq, 1, arg);
++		ret = irq_domain_alloc_irqs_hierarchy(domain, virq, 1, arg);
+ 		if (ret)
+ 			break;
+ 
+diff --git a/net/bridge/br.c b/net/bridge/br.c
+index 889e5640..7db8a1a5 100644
+--- a/net/bridge/br.c
++++ b/net/bridge/br.c
+@@ -138,14 +138,14 @@ static int br_switchdev_event(struct notifier_block *unused,
+ 	br = p->br;
+ 
+ 	switch (event) {
+-	case SWITCHDEV_FDB_ADD:
++	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+ 		fdb_info = ptr;
+ 		err = br_fdb_external_learn_add(br, p, fdb_info->addr,
+ 						fdb_info->vid);
+ 		if (err)
+ 			err = notifier_from_errno(err);
+ 		break;
+-	case SWITCHDEV_FDB_DEL:
++	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+ 		fdb_info = ptr;
+ 		err = br_fdb_external_learn_del(br, p, fdb_info->addr,
+ 						fdb_info->vid);
+diff --git a/security/keys/Kconfig b/security/keys/Kconfig
+index 0832f636..c5578bd4 100644
+--- a/security/keys/Kconfig
++++ b/security/keys/Kconfig
+@@ -45,6 +45,7 @@ config BIG_KEYS
+ 	bool "Large payload keys"
+ 	depends on KEYS
+ 	depends on TMPFS
++	select CRYPTO
+ 	select CRYPTO_AES
+ 	select CRYPTO_GCM
+ 	help
+@@ -70,6 +71,17 @@ config TRUSTED_KEYS
+ 
+ 	  If you are unsure as to whether this is required, answer N.
+ 
++config SECURE_KEYS
++	tristate "SECURE_KEYS"
++	depends on KEYS && CRYPTO_DEV_FSL_CAAM && CRYPTO_DEV_FSL_CAAM_JR
++	help
++	  This option provide support for creating secure-type key and blobs
++	  in kernel. Secure keys are random number symmetric keys generated
++	  from CAAM. The CAAM creates the blobs for the random key.
++	  Userspace will only be able to see the blob.
++
++	  If you are unsure as to whether this is required, answer N.
++
+ config ENCRYPTED_KEYS
+ 	tristate "ENCRYPTED KEYS"
+ 	depends on KEYS
+diff --git a/security/keys/Makefile b/security/keys/Makefile
+index 1fd4a16e..e7fa6163 100644
+--- a/security/keys/Makefile
++++ b/security/keys/Makefile
+@@ -26,4 +26,9 @@ obj-$(CONFIG_KEY_DH_OPERATIONS) += dh.o
+ #
+ obj-$(CONFIG_BIG_KEYS) += big_key.o
+ obj-$(CONFIG_TRUSTED_KEYS) += trusted.o
++CFLAGS_secure_key.o += -I$(obj)/../../drivers/crypto/caam/
++CFLAGS_securekey_desc.o += -I$(obj)/../../drivers/crypto/caam/
++obj-$(CONFIG_SECURE_KEYS) += securekey.o
++securekey-y := securekey_desc.o \
++	       secure_key.o
+ obj-$(CONFIG_ENCRYPTED_KEYS) += encrypted-keys/
+diff --git a/security/keys/encrypted-keys/Makefile b/security/keys/encrypted-keys/Makefile
+index d6f84332..3840db49 100644
+--- a/security/keys/encrypted-keys/Makefile
++++ b/security/keys/encrypted-keys/Makefile
+@@ -6,5 +6,7 @@ obj-$(CONFIG_ENCRYPTED_KEYS) += encrypted-keys.o
+ 
+ encrypted-keys-y := encrypted.o ecryptfs_format.o
+ masterkey-$(CONFIG_TRUSTED_KEYS) := masterkey_trusted.o
++masterkey-$(CONFIG_SECURE_KEYS) := masterkey_secure.o
+ masterkey-$(CONFIG_TRUSTED_KEYS)-$(CONFIG_ENCRYPTED_KEYS) := masterkey_trusted.o
++masterkey-$(CONFIG_SECURE_KEYS)-$(CONFIG_ENCRYPTED_KEYS) := masterkey_secure.o
+ encrypted-keys-y += $(masterkey-y) $(masterkey-m-m)
+diff --git a/security/keys/encrypted-keys/encrypted.c b/security/keys/encrypted-keys/encrypted.c
+index a871159b..a396ee06 100644
+--- a/security/keys/encrypted-keys/encrypted.c
++++ b/security/keys/encrypted-keys/encrypted.c
+@@ -38,6 +38,7 @@
+ #include "ecryptfs_format.h"
+ 
+ static const char KEY_TRUSTED_PREFIX[] = "trusted:";
++static const char KEY_SECURE_PREFIX[] = "secure:";
+ static const char KEY_USER_PREFIX[] = "user:";
+ static const char hash_alg[] = "sha256";
+ static const char hmac_alg[] = "hmac(sha256)";
+@@ -48,6 +49,7 @@ static unsigned int ivsize;
+ static int blksize;
+ 
+ #define KEY_TRUSTED_PREFIX_LEN (sizeof (KEY_TRUSTED_PREFIX) - 1)
++#define KEY_SECURE_PREFIX_LEN (sizeof(KEY_SECURE_PREFIX) - 1)
+ #define KEY_USER_PREFIX_LEN (sizeof (KEY_USER_PREFIX) - 1)
+ #define KEY_ECRYPTFS_DESC_LEN 16
+ #define HASH_SIZE SHA256_DIGEST_SIZE
+@@ -130,7 +132,7 @@ static int valid_ecryptfs_desc(const char *ecryptfs_desc)
+ /*
+  * valid_master_desc - verify the 'key-type:desc' of a new/updated master-key
+  *
+- * key-type:= "trusted:" | "user:"
++ * key-type:= "trusted:" | "user:" | "secure:"
+  * desc:= master-key description
+  *
+  * Verify that 'key-type' is valid and that 'desc' exists. On key update,
+@@ -141,23 +143,24 @@ static int valid_ecryptfs_desc(const char *ecryptfs_desc)
+  */
+ static int valid_master_desc(const char *new_desc, const char *orig_desc)
+ {
+-	if (!memcmp(new_desc, KEY_TRUSTED_PREFIX, KEY_TRUSTED_PREFIX_LEN)) {
+-		if (strlen(new_desc) == KEY_TRUSTED_PREFIX_LEN)
+-			goto out;
+-		if (orig_desc)
+-			if (memcmp(new_desc, orig_desc, KEY_TRUSTED_PREFIX_LEN))
+-				goto out;
+-	} else if (!memcmp(new_desc, KEY_USER_PREFIX, KEY_USER_PREFIX_LEN)) {
+-		if (strlen(new_desc) == KEY_USER_PREFIX_LEN)
+-			goto out;
+-		if (orig_desc)
+-			if (memcmp(new_desc, orig_desc, KEY_USER_PREFIX_LEN))
+-				goto out;
+-	} else
+-		goto out;
++	int prefix_len;
++
++	if (!strncmp(new_desc, KEY_TRUSTED_PREFIX, KEY_TRUSTED_PREFIX_LEN))
++		prefix_len = KEY_TRUSTED_PREFIX_LEN;
++	else if (!strncmp(new_desc, KEY_SECURE_PREFIX, KEY_SECURE_PREFIX_LEN))
++		prefix_len = KEY_SECURE_PREFIX_LEN;
++	else if (!strncmp(new_desc, KEY_USER_PREFIX, KEY_USER_PREFIX_LEN))
++		prefix_len = KEY_USER_PREFIX_LEN;
++	else
++		return -EINVAL;
++
++	if (!new_desc[prefix_len])
++		return -EINVAL;
++
++	if (orig_desc && strncmp(new_desc, orig_desc, prefix_len))
++		return -EINVAL;
++
+ 	return 0;
+-out:
+-	return -EINVAL;
+ }
+ 
+ /*
+@@ -379,7 +382,7 @@ static int calc_hash(u8 *digest, const u8 *buf, unsigned int buflen)
+ 
+ enum derived_key_type { ENC_KEY, AUTH_KEY };
+ 
+-/* Derive authentication/encryption key from trusted key */
++/* Derive authentication/encryption key from trusted/secure key */
+ static int get_derived_key(u8 *derived_key, enum derived_key_type key_type,
+ 			   const u8 *master_key, size_t master_keylen)
+ {
+@@ -451,6 +454,11 @@ static struct key *request_master_key(struct encrypted_key_payload *epayload,
+ 		mkey = request_trusted_key(epayload->master_desc +
+ 					   KEY_TRUSTED_PREFIX_LEN,
+ 					   master_key, master_keylen);
++	} else if (!strncmp(epayload->master_desc, KEY_SECURE_PREFIX,
++			    KEY_SECURE_PREFIX_LEN)) {
++		mkey = request_secure_key(epayload->master_desc +
++					  KEY_SECURE_PREFIX_LEN,
++					  master_key, master_keylen);
+ 	} else if (!strncmp(epayload->master_desc, KEY_USER_PREFIX,
+ 			    KEY_USER_PREFIX_LEN)) {
+ 		mkey = request_user_key(epayload->master_desc +
+diff --git a/security/keys/encrypted-keys/encrypted.h b/security/keys/encrypted-keys/encrypted.h
+index 47802c0d..74884322 100644
+--- a/security/keys/encrypted-keys/encrypted.h
++++ b/security/keys/encrypted-keys/encrypted.h
+@@ -15,6 +15,19 @@ static inline struct key *request_trusted_key(const char *trusted_desc,
+ }
+ #endif
+ 
++#if defined(CONFIG_SECURE_KEYS)
++extern struct key *request_secure_key(const char *secure_desc,
++				      const u8 **master_key,
++				      size_t *master_keylen);
++#else
++static inline struct key *request_secure_key(const char *secure_desc,
++					     const u8 **master_key,
++					     size_t *master_keylen)
++{
++	return ERR_PTR(-EOPNOTSUPP);
++}
++#endif
++
+ #if ENCRYPTED_DEBUG
+ static inline void dump_master_key(const u8 *master_key, size_t master_keylen)
+ {
+diff --git a/security/keys/encrypted-keys/masterkey_secure.c b/security/keys/encrypted-keys/masterkey_secure.c
+new file mode 100644
+index 00000000..480db286
+--- /dev/null
++++ b/security/keys/encrypted-keys/masterkey_secure.c
+@@ -0,0 +1,37 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Copyright 2018 NXP
++ *
++ */
++
++#include <linux/uaccess.h>
++#include <linux/module.h>
++#include <linux/err.h>
++#include <keys/secure-type.h>
++#include <keys/encrypted-type.h>
++#include "encrypted.h"
++
++/*
++ * request_secure_key - request the secure key
++ *
++ * Secure keys and their blobs are derived from CAAM hardware.
++ * Userspace manages secure  key-type data, but key data is not
++ * visible in plain form. It is presented as blobs.
++ */
++struct key *request_secure_key(const char *secure_desc,
++				const u8 **master_key, size_t *master_keylen)
++{
++	struct secure_key_payload *spayload;
++	struct key *skey;
++
++	skey = request_key(&key_type_secure, secure_desc, NULL);
++	if (IS_ERR(skey))
++		goto error;
++
++	down_read(&skey->sem);
++	spayload = skey->payload.data[0];
++	*master_key = spayload->key;
++	*master_keylen = spayload->key_len;
++error:
++	return skey;
++}
+diff --git a/security/keys/secure_key.c b/security/keys/secure_key.c
+new file mode 100644
+index 00000000..6f7a78fb
+--- /dev/null
++++ b/security/keys/secure_key.c
+@@ -0,0 +1,339 @@
++// SPDX-License-Identifier: GPL-2.0
++/* Copyright 2018 NXP
++ * Secure key is generated using NXP CAAM hardware block. CAAM generates the
++ * random number (used as a key) and creates its blob for the user.
++ */
++
++#include <linux/slab.h>
++#include <linux/parser.h>
++#include <linux/string.h>
++#include <linux/key-type.h>
++#include <linux/rcupdate.h>
++#include <keys/secure-type.h>
++#include <linux/completion.h>
++
++#include "securekey_desc.h"
++
++static const char hmac_alg[] = "hmac(sha1)";
++static const char hash_alg[] = "sha1";
++
++static struct crypto_shash *hashalg;
++static struct crypto_shash *hmacalg;
++
++enum {
++	error = -1,
++	new_key,
++	load_blob,
++};
++
++static const match_table_t key_tokens = {
++	{new_key, "new"},
++	{load_blob, "load"},
++	{error, NULL}
++};
++
++static struct secure_key_payload *secure_payload_alloc(struct key *key)
++{
++	struct secure_key_payload *sec_key = NULL;
++	int ret = 0;
++
++	ret = key_payload_reserve(key, sizeof(*sec_key));
++	if (ret < 0)
++		goto out;
++
++	sec_key = kzalloc(sizeof(*sec_key), GFP_KERNEL);
++	if (!sec_key)
++		goto out;
++
++out:
++	return sec_key;
++}
++
++/*
++ * parse_inputdata - parse the keyctl input data and fill in the
++ *		     payload structure for key or its blob.
++ * param[in]: data pointer to the data to be parsed for creating key.
++ * param[in]: p pointer to secure key payload structure to fill parsed data
++ * On success returns 0, otherwise -EINVAL.
++ */
++static int parse_inputdata(char *data, struct secure_key_payload *p)
++{
++	substring_t args[MAX_OPT_ARGS];
++	long keylen = 0;
++	int ret = -EINVAL;
++	int key_cmd = -EINVAL;
++	char *c = NULL;
++
++	c = strsep(&data, " \t");
++	if (!c) {
++		ret = -EINVAL;
++		goto out;
++	}
++
++	/* Get the keyctl command i.e. new_key or load_blob etc */
++	key_cmd = match_token(c, key_tokens, args);
++
++	switch (key_cmd) {
++	case new_key:
++		/* first argument is key size */
++		c = strsep(&data, " \t");
++		if (!c) {
++			ret = -EINVAL;
++			goto out;
++		}
++
++		ret = kstrtol(c, 10, &keylen);
++		if (ret < 0 || keylen < MIN_KEY_SIZE ||
++						keylen > MAX_KEY_SIZE) {
++			ret = -EINVAL;
++			goto out;
++		}
++
++		p->key_len = keylen;
++		ret = new_key;
++
++		break;
++	case load_blob:
++		/* first argument is blob data for CAAM*/
++		c = strsep(&data, " \t");
++		if (!c) {
++			ret = -EINVAL;
++			goto out;
++		}
++
++		/* Blob_len = No of characters in blob/2 */
++		p->blob_len = strlen(c) / 2;
++		if (p->blob_len > MAX_BLOB_SIZE) {
++			ret = -EINVAL;
++			goto out;
++		}
++
++		ret = hex2bin(p->blob, c, p->blob_len);
++		if (ret < 0) {
++			ret = -EINVAL;
++			goto out;
++		}
++		ret = load_blob;
++
++		break;
++	case error:
++		ret = -EINVAL;
++		break;
++	}
++
++out:
++	return ret;
++}
++
++/*
++ * secure_instantiate - create a new secure type key.
++ * Supports the operation to generate a new key. A random number
++ * is generated from CAAM as key data and the corresponding red blob
++ * is formed and stored as key_blob.
++ * Also supports the operation to load the blob and key is derived using
++ * that blob from CAAM.
++ * On success, return 0. Otherwise return errno.
++ */
++static int secure_instantiate(struct key *key,
++		struct key_preparsed_payload *prep)
++{
++	struct secure_key_payload *payload = NULL;
++	size_t datalen = prep->datalen;
++	char *data = NULL;
++	int key_cmd = 0;
++	int ret = 0;
++	enum sk_req_type sk_op_type;
++	struct device *dev = NULL;
++
++	if (datalen <= 0 || datalen > 32767 || !prep->data) {
++		ret = -EINVAL;
++		goto out;
++	}
++
++	data = kmalloc(datalen + 1, GFP_KERNEL);
++	if (!data) {
++		ret = -ENOMEM;
++		goto out;
++	}
++
++	memcpy(data, prep->data, datalen);
++	data[datalen] = '\0';
++
++	payload = secure_payload_alloc(key);
++	if (!payload) {
++		ret = -ENOMEM;
++		goto out;
++	}
++
++	/* Allocate caam job ring for operation to be performed from CAAM */
++	dev = caam_jr_alloc();
++	if (!dev) {
++		pr_info("caam_jr_alloc failed\n");
++		ret = -ENODEV;
++		goto out;
++	}
++
++	key_cmd = parse_inputdata(data, payload);
++	if (key_cmd < 0) {
++		ret = key_cmd;
++		goto out;
++	}
++
++	switch (key_cmd) {
++	case load_blob:
++		/*
++		 * Red blob decryption to be done for load operation
++		 * to derive the key.
++		 */
++		sk_op_type = sk_red_blob_dec;
++		ret = key_deblob(payload, sk_op_type, dev);
++		if (ret != 0) {
++			pr_info("secure_key: key_blob decap fail (%d)\n", ret);
++			goto out;
++		}
++		break;
++	case new_key:
++		/* Get Random number from caam of the specified length */
++		sk_op_type = sk_get_random;
++		ret = caam_get_random(payload, sk_op_type, dev);
++		if (ret != 0) {
++			pr_info("secure_key: get_random fail (%d)\n", ret);
++			goto out;
++		}
++
++		/* Generate red blob of key random bytes with CAAM */
++		sk_op_type = sk_red_blob_enc;
++		ret = key_blob(payload, sk_op_type, dev);
++		if (ret != 0) {
++			pr_info("secure_key: key_blob encap fail (%d)\n", ret);
++			goto out;
++		}
++		break;
++	default:
++		ret = -EINVAL;
++		goto out;
++	}
++out:
++	if (data)
++		kzfree(data);
++	if (dev)
++		caam_jr_free(dev);
++
++	if (!ret)
++		rcu_assign_keypointer(key, payload);
++	else
++		kzfree(payload);
++
++	return ret;
++}
++
++/*
++ * secure_read - copy the  blob data to userspace in hex.
++ * param[in]: key pointer to key struct
++ * param[in]: buffer pointer to user data for creating key
++ * param[in]: buflen is the length of the buffer
++ * On success, return to userspace the secure key data size.
++ */
++static long secure_read(const struct key *key, char __user *buffer,
++			 size_t buflen)
++{
++	const struct secure_key_payload *p = NULL;
++	char *ascii_buf;
++	char *bufp;
++	int i;
++
++	p = rcu_dereference_key(key);
++	if (!p)
++		return -EINVAL;
++
++	if (buffer && buflen >= 2 * p->blob_len) {
++		ascii_buf = kmalloc(2 * p->blob_len, GFP_KERNEL);
++		if (!ascii_buf)
++			return -ENOMEM;
++
++		bufp = ascii_buf;
++		for (i = 0; i < p->blob_len; i++)
++			bufp = hex_byte_pack(bufp, p->blob[i]);
++		if (copy_to_user(buffer, ascii_buf, 2 * p->blob_len) != 0) {
++			kzfree(ascii_buf);
++			return -EFAULT;
++		}
++		kzfree(ascii_buf);
++	}
++	return 2 * p->blob_len;
++}
++
++/*
++ * secure_destroy - clear and free the key's payload
++ */
++static void secure_destroy(struct key *key)
++{
++	kzfree(key->payload.data[0]);
++}
++
++struct key_type key_type_secure = {
++	.name = "secure",
++	.instantiate = secure_instantiate,
++	.destroy = secure_destroy,
++	.read = secure_read,
++};
++EXPORT_SYMBOL_GPL(key_type_secure);
++
++static void secure_shash_release(void)
++{
++	if (hashalg)
++		crypto_free_shash(hashalg);
++	if (hmacalg)
++		crypto_free_shash(hmacalg);
++}
++
++static int __init secure_shash_alloc(void)
++{
++	int ret;
++
++	hmacalg = crypto_alloc_shash(hmac_alg, 0, CRYPTO_ALG_ASYNC);
++	if (IS_ERR(hmacalg)) {
++		pr_info("secure_key: could not allocate crypto %s\n",
++				hmac_alg);
++		return PTR_ERR(hmacalg);
++	}
++
++	hashalg = crypto_alloc_shash(hash_alg, 0, CRYPTO_ALG_ASYNC);
++	if (IS_ERR(hashalg)) {
++		pr_info("secure_key: could not allocate crypto %s\n",
++				hash_alg);
++		ret = PTR_ERR(hashalg);
++		goto hashalg_fail;
++	}
++
++	return 0;
++
++hashalg_fail:
++	crypto_free_shash(hmacalg);
++	return ret;
++}
++
++static int __init init_secure_key(void)
++{
++	int ret;
++
++	ret = secure_shash_alloc();
++	if (ret < 0)
++		return ret;
++
++	ret = register_key_type(&key_type_secure);
++	if (ret < 0)
++		secure_shash_release();
++	return ret;
++}
++
++static void __exit cleanup_secure_key(void)
++{
++	secure_shash_release();
++	unregister_key_type(&key_type_secure);
++}
++
++late_initcall(init_secure_key);
++module_exit(cleanup_secure_key);
++
++MODULE_LICENSE("GPL");
+diff --git a/security/keys/securekey_desc.c b/security/keys/securekey_desc.c
+new file mode 100644
+index 00000000..b55b68c2
+--- /dev/null
++++ b/security/keys/securekey_desc.c
+@@ -0,0 +1,608 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * Copyright 2018 NXP
++ *
++ */
++
++#include <keys/secure-type.h>
++#include "securekey_desc.h"
++
++/* key modifier for blob encapsulation & decapsulation descriptor */
++u8 key_modifier[] = "SECURE_KEY";
++u32 key_modifier_len = 10;
++
++void caam_sk_rng_desc(struct sk_req *skreq, struct sk_desc *skdesc)
++{
++	struct sk_fetch_rnd_data *fetch_rnd_data = NULL;
++	struct random_desc *rnd_desc = NULL;
++	size_t len = 0;
++	u32 *desc = skreq->hwdesc;
++
++	init_job_desc(desc, 0);
++
++	fetch_rnd_data = &skreq->req_u.sk_fetch_rnd_data;
++	rnd_desc = &skdesc->dma_u.random_descp;
++	len = fetch_rnd_data->key_len;
++
++	/* command 0x82500000 */
++	append_cmd(desc, CMD_OPERATION | OP_TYPE_CLASS1_ALG |
++			OP_ALG_ALGSEL_RNG);
++	/* command 0x60340000 | len */
++	append_cmd(desc, CMD_FIFO_STORE | FIFOST_TYPE_RNGSTORE | len);
++	append_ptr(desc, rnd_desc->rnd_data);
++}
++
++void caam_sk_redblob_encap_desc(struct sk_req *skreq, struct sk_desc *skdesc)
++{
++	struct redblob_encap_desc *red_blob_desc =
++					&skdesc->dma_u.redblob_encapdesc;
++	struct sk_red_blob_encap *red_blob_req =
++					&skreq->req_u.sk_red_blob_encap;
++	u32 *desc = skreq->hwdesc;
++
++	init_job_desc(desc, 0);
++
++	/* Load class 2 key with key modifier. */
++	append_key_as_imm(desc, key_modifier, key_modifier_len,
++			  key_modifier_len, CLASS_2 | KEY_DEST_CLASS_REG);
++
++	/* SEQ IN PTR Command. */
++	append_seq_in_ptr(desc, red_blob_desc->in_data, red_blob_req->data_sz,
++			  0);
++
++	/* SEQ OUT PTR Command. */
++	append_seq_out_ptr(desc, red_blob_desc->redblob,
++			   red_blob_req->redblob_sz, 0);
++
++	/* RedBlob encapsulation PROTOCOL Command. */
++	append_operation(desc, OP_TYPE_ENCAP_PROTOCOL | OP_PCLID_BLOB);
++}
++
++/* void caam_sk_redblob_decap_desc(struct sk_req *skreq, struct sk_desc *skdesc)
++ * brief CAAM Descriptor creator from redblob to plaindata.
++ * param[in] skreq Pointer to secure key request structure
++ * param[in] skdesc Pointer to secure key descriptor structure
++ */
++void caam_sk_redblob_decap_desc(struct sk_req *skreq, struct sk_desc *skdesc)
++{
++	struct redblob_decap_desc *red_blob_desc =
++					&skdesc->dma_u.redblob_decapdesc;
++	struct sk_red_blob_decap *red_blob_req =
++					&skreq->req_u.sk_red_blob_decap;
++	u32 *desc = skreq->hwdesc;
++
++	init_job_desc(desc, 0);
++
++	/* Load class 2 key with key modifier. */
++	append_key_as_imm(desc, key_modifier, key_modifier_len,
++			  key_modifier_len, CLASS_2 | KEY_DEST_CLASS_REG);
++
++	/* SEQ IN PTR Command. */
++	append_seq_in_ptr(desc, red_blob_desc->redblob,
++			  red_blob_req->redblob_sz, 0);
++
++	/* SEQ OUT PTR Command. */
++	append_seq_out_ptr(desc, red_blob_desc->out_data,
++			   red_blob_req->data_sz, 0);
++
++	/* RedBlob decapsulation PROTOCOL Command. */
++	append_operation(desc, OP_TYPE_DECAP_PROTOCOL | OP_PCLID_BLOB);
++}
++
++/* int caam_sk_get_random_map(struct device *dev, struct sk_req *req,
++ *			      struct sk_desc *skdesc)
++ * brief DMA map the buffer virtual pointers to physical address.
++ * param[in] dev Pointer to job ring device structure
++ * param[in] req Pointer to secure key request structure
++ * param[in] skdesc Pointer to secure key descriptor structure
++ * return 0 on success, error value otherwise.
++ */
++int caam_sk_get_random_map(struct device *dev, struct sk_req *req,
++			   struct sk_desc *skdesc)
++{
++	struct sk_fetch_rnd_data *fetch_rnd_data;
++	struct random_desc *rnd_desc;
++
++	fetch_rnd_data = &req->req_u.sk_fetch_rnd_data;
++	rnd_desc = &skdesc->dma_u.random_descp;
++
++	rnd_desc->rnd_data = dma_map_single(dev, fetch_rnd_data->data,
++				fetch_rnd_data->key_len, DMA_FROM_DEVICE);
++
++	if (dma_mapping_error(dev, rnd_desc->rnd_data)) {
++		dev_err(dev, "Unable to map memory\n");
++		goto sk_random_map_fail;
++	}
++	return 0;
++
++sk_random_map_fail:
++	return -ENOMEM;
++}
++
++/* int caam_sk_redblob_encap_map(struct device *dev, struct sk_req *req,
++ *					struct sk_desc *skdesc)
++ * brief DMA map the buffer virtual pointers to physical address.
++ * param[in] dev Pointer to job ring device structure
++ * param[in] req Pointer to secure key request structure
++ * param[in] skdesc Pointer to secure key descriptor structure
++ * return 0 on success, error value otherwise.
++ */
++int caam_sk_redblob_encap_map(struct device *dev, struct sk_req *req,
++			      struct sk_desc *skdesc)
++{
++	struct sk_red_blob_encap *red_blob_encap;
++	struct redblob_encap_desc *red_blob_desc;
++
++	red_blob_encap = &req->req_u.sk_red_blob_encap;
++	red_blob_desc = &skdesc->dma_u.redblob_encapdesc;
++
++	red_blob_desc->in_data = dma_map_single(dev, red_blob_encap->data,
++					red_blob_encap->data_sz, DMA_TO_DEVICE);
++	if (dma_mapping_error(dev, red_blob_desc->in_data)) {
++		dev_err(dev, "Unable to map memory\n");
++		goto sk_data_fail;
++	}
++
++	red_blob_desc->redblob = dma_map_single(dev, red_blob_encap->redblob,
++				red_blob_encap->redblob_sz, DMA_FROM_DEVICE);
++	if (dma_mapping_error(dev, red_blob_desc->redblob)) {
++		dev_err(dev, "Unable to map memory\n");
++		goto sk_redblob_fail;
++	}
++
++	return 0;
++
++sk_redblob_fail:
++	dma_unmap_single(dev, red_blob_desc->in_data, red_blob_encap->data_sz,
++			 DMA_TO_DEVICE);
++sk_data_fail:
++	return -ENOMEM;
++}
++
++/* static int caam_sk_redblob_decap_map(struct device *dev,
++ *					    struct sk_req *req,
++ *					    struct sk_desc *skdesc)
++ * brief DMA map the buffer virtual pointers to physical address.
++ * param[in] dev Pointer to job ring device structure
++ * param[in] req Pointer to secure key request structure
++ * param[in] skdesc Pointer to secure key descriptor structure
++ * return 0 on success, error value otherwise.
++ */
++int caam_sk_redblob_decap_map(struct device *dev, struct sk_req *req,
++			      struct sk_desc *skdesc)
++{
++	struct sk_red_blob_decap *red_blob_decap;
++	struct redblob_decap_desc *red_blob_desc;
++
++	red_blob_decap = &req->req_u.sk_red_blob_decap;
++	red_blob_desc = &skdesc->dma_u.redblob_decapdesc;
++
++	red_blob_desc->redblob = dma_map_single(dev, red_blob_decap->redblob,
++				red_blob_decap->redblob_sz, DMA_TO_DEVICE);
++	if (dma_mapping_error(dev, red_blob_desc->redblob)) {
++		dev_err(dev, "Unable to map memory\n");
++		goto sk_redblob_fail;
++	}
++
++	red_blob_desc->out_data = dma_map_single(dev, red_blob_decap->data,
++				red_blob_decap->data_sz, DMA_FROM_DEVICE);
++	if (dma_mapping_error(dev, red_blob_desc->out_data)) {
++		dev_err(dev, "Unable to map memory\n");
++		goto sk_data_fail;
++	}
++
++	return 0;
++
++sk_data_fail:
++	dma_unmap_single(dev, red_blob_desc->redblob,
++			 red_blob_decap->redblob_sz, DMA_TO_DEVICE);
++sk_redblob_fail:
++	return -ENOMEM;
++}
++
++/* @fn void securekey_unmap(struct device *dev,
++ *			    struct sk_desc *skdesc, struct sk_req *req)
++ * @brief DMA unmap the buffer pointers.
++ * @param[in] dev Pointer to job ring device structure
++ * @param[in] skdesc Pointer to secure key descriptor structure
++ * @param[in] req Pointer to secure key request structure
++ */
++void securekey_unmap(struct device *dev,
++		     struct sk_desc *skdesc, struct sk_req *req)
++{
++
++	switch (req->type) {
++	case sk_get_random:
++		{
++			struct sk_fetch_rnd_data *fetch_rnd_data;
++			struct random_desc *rnd_desc;
++
++			fetch_rnd_data = &req->req_u.sk_fetch_rnd_data;
++			rnd_desc = &skdesc->dma_u.random_descp;
++
++			/* Unmap Descriptor buffer pointers. */
++			dma_unmap_single(dev, rnd_desc->rnd_data,
++					 fetch_rnd_data->key_len,
++					 DMA_FROM_DEVICE);
++			break;
++		}
++	case sk_red_blob_enc:
++		{
++			struct sk_red_blob_encap *red_blob_encap;
++			struct redblob_encap_desc *red_blob_desc;
++
++			red_blob_encap = &req->req_u.sk_red_blob_encap;
++			red_blob_desc = &skdesc->dma_u.redblob_encapdesc;
++
++			/* Unmap Descriptor buffer pointers. */
++			dma_unmap_single(dev, red_blob_desc->in_data,
++					 red_blob_encap->data_sz,
++					 DMA_TO_DEVICE);
++
++			dma_unmap_single(dev, red_blob_desc->redblob,
++					 red_blob_encap->redblob_sz,
++					 DMA_FROM_DEVICE);
++
++			break;
++		}
++	case sk_red_blob_dec:
++		{
++			struct sk_red_blob_decap *red_blob_decap;
++			struct redblob_decap_desc *red_blob_desc;
++
++			red_blob_decap = &req->req_u.sk_red_blob_decap;
++			red_blob_desc = &skdesc->dma_u.redblob_decapdesc;
++
++			/* Unmap Descriptor buffer pointers. */
++			dma_unmap_single(dev, red_blob_desc->redblob,
++					 red_blob_decap->redblob_sz,
++					 DMA_TO_DEVICE);
++
++			dma_unmap_single(dev, red_blob_desc->out_data,
++					 red_blob_decap->data_sz,
++					 DMA_FROM_DEVICE);
++
++			break;
++		}
++	default:
++		dev_err(dev, "Unable to find request type\n");
++		break;
++	}
++	kfree(skdesc);
++}
++
++/*  int caam_securekey_desc_init(struct device *dev, struct sk_req *req)
++ *  brief CAAM Descriptor creator for secure key operations.
++ *  param[in] dev Pointer to job ring device structure
++ *  param[in] req Pointer to secure key request structure
++ *  return 0 on success, error value otherwise.
++ */
++int caam_securekey_desc_init(struct device *dev, struct sk_req *req)
++{
++	struct sk_desc *skdesc = NULL;
++	int ret = 0;
++
++	switch (req->type) {
++	case sk_get_random:
++		{
++			skdesc = kmalloc(sizeof(*skdesc), GFP_DMA);
++			if (!skdesc) {
++				ret = -ENOMEM;
++				goto out;
++			}
++			skdesc->req_type = req->type;
++
++			if (caam_sk_get_random_map(dev, req, skdesc)) {
++				dev_err(dev, "caam get_random map fail\n");
++				ret = -ENOMEM;
++				goto out;
++			}
++			caam_sk_rng_desc(req, skdesc);
++			break;
++		}
++	case sk_red_blob_enc:
++		{
++			skdesc = kmalloc(sizeof(*skdesc), GFP_DMA);
++			if (!skdesc) {
++				ret = -ENOMEM;
++				goto out;
++			}
++
++			skdesc->req_type = req->type;
++
++			if (caam_sk_redblob_encap_map(dev, req, skdesc)) {
++				dev_err(dev, "caam redblob_encap map fail\n");
++				ret = -ENOMEM;
++				goto out;
++			}
++
++			/* Descriptor function to create redblob from data. */
++			caam_sk_redblob_encap_desc(req, skdesc);
++			break;
++		}
++
++	case sk_red_blob_dec:
++		{
++			skdesc = kmalloc(sizeof(*skdesc), GFP_DMA);
++			if (!skdesc) {
++				ret = -ENOMEM;
++				goto out;
++			}
++
++			skdesc->req_type = req->type;
++
++			if (caam_sk_redblob_decap_map(dev, req, skdesc)) {
++				dev_err(dev, "caam redblob_decap map fail\n");
++				ret = -ENOMEM;
++				goto out;
++			}
++
++			/* Descriptor function to decap data from redblob. */
++			caam_sk_redblob_decap_desc(req, skdesc);
++			break;
++		}
++	default:
++		pr_debug("Unknown request type\n");
++		ret = -EINVAL;
++		goto out;
++	}
++
++	req->desc_pointer = (void *)skdesc;
++
++out:
++	return ret;
++}
++
++/* static void caam_op_done (struct device *dev, u32 *desc, u32 ret,
++ *			     void *context)
++ * brief callback function to be called when descriptor executed.
++ * param[in] dev Pointer to device structure
++ * param[in] desc descriptor pointer
++ * param[in] ret return status of Job submitted
++ * param[in] context void pointer
++ */
++static void caam_op_done(struct device *dev, u32 *desc, u32 ret,
++			 void *context)
++{
++	struct sk_req *req = context;
++
++	if (ret) {
++		dev_err(dev, "caam op done err: %x\n", ret);
++		/* print the error source name. */
++		caam_jr_strstatus(dev, ret);
++	}
++	/* Call securekey_unmap function for unmapping the buffer pointers. */
++	securekey_unmap(dev, req->desc_pointer, req);
++
++	req->ret = ret;
++	complete(&req->comp);
++}
++
++
++/*  static int sk_job_submit(struct device *jrdev, struct sk_req *req)
++ *  brief Enqueue a Job descriptor to Job ring and wait until SEC returns.
++ *  param[in] jrdev Pointer to job ring device structure
++ *  param[in] req Pointer to secure key request structure
++ *  return 0 on success, error value otherwise.
++ */
++static int sk_job_submit(struct device *jrdev, struct sk_req *req)
++{
++	int ret;
++
++	init_completion(&req->comp);
++
++	/* caam_jr_enqueue function for Enqueue a job descriptor */
++	ret = caam_jr_enqueue(jrdev, req->hwdesc, caam_op_done, req);
++	if (!ret)
++		wait_for_completion_interruptible(&req->comp);
++
++	ret = req->ret;
++	return ret;
++}
++
++/* caam_get_random(struct secure_key_payload *p,  enum sk_req_type fetch_rnd,
++ *		   struct device *dev)
++ * Create the random number of the specified length using CAAM block
++ * param[in]: out pointer to place the random bytes
++ * param[in]: length for the random data bytes.
++ * param[in]: dev Pointer to job ring device structure
++ * If operation is successful return 0, otherwise error.
++ */
++int caam_get_random(struct secure_key_payload *p,  enum sk_req_type fetch_rnd,
++		    struct device *dev)
++{
++	struct sk_fetch_rnd_data *fetch_rnd_data = NULL;
++	struct sk_req *req = NULL;
++	int ret = 0;
++	void *temp = NULL;
++
++	req = kmalloc(sizeof(struct sk_req), GFP_DMA);
++	if (!req) {
++		ret = -ENOMEM;
++		goto out;
++	}
++
++	req->type = fetch_rnd;
++	fetch_rnd_data = &(req->req_u.sk_fetch_rnd_data);
++
++	/* initialise with key length */
++	fetch_rnd_data->key_len = p->key_len;
++
++	temp = kmalloc(fetch_rnd_data->key_len, GFP_DMA);
++	if (!temp) {
++		ret = -ENOMEM;
++		goto out;
++	}
++	fetch_rnd_data->data = temp;
++
++	ret = caam_securekey_desc_init(dev, req);
++
++	if (ret) {
++		pr_info("caam_securekey_desc_init failed\n");
++		goto out;
++	}
++
++	ret = sk_job_submit(dev, req);
++	if (!ret) {
++		/*Copy output to key buffer. */
++		memcpy(p->key, fetch_rnd_data->data, p->key_len);
++	} else {
++		ret = -EINVAL;
++	}
++
++out:
++	if (req)
++		kfree(req);
++
++	if (temp)
++		kfree(temp);
++
++	return ret;
++}
++EXPORT_SYMBOL(caam_get_random);
++
++/* key_deblob(struct secure_key_payload *p, enum sk_req_type decap_type
++ *		struct device *dev)
++ * Deblobify the blob to get the key data and fill in secure key payload struct
++ * param[in] p pointer to the secure key payload
++ * param[in] decap_type operation to be done.
++ * param[in] dev dev Pointer to job ring device structure
++ * If operation is successful return 0, otherwise error.
++ */
++int key_deblob(struct secure_key_payload *p, enum sk_req_type decap_type,
++	       struct device *dev)
++{
++	unsigned int blob_len;
++	struct sk_red_blob_decap *d_blob;
++	struct sk_req *req = NULL;
++	int total_sz = 0, *temp = NULL, ret = 0;
++
++	req = kmalloc(sizeof(struct sk_req), GFP_DMA);
++	if (!req) {
++		ret = -ENOMEM;
++		goto out;
++	}
++
++	d_blob = &(req->req_u.sk_red_blob_decap);
++	blob_len = p->blob_len;
++	req->type = decap_type;
++
++	/*
++	 * Red blob size is the blob_len filled in payload struct
++	 * Data_sz i.e. key is the blob_len - blob header size
++	 */
++
++	d_blob->redblob_sz = blob_len;
++	d_blob->data_sz = blob_len - (SK_BLOB_KEY_SZ + SK_BLOB_MAC_SZ);
++	total_sz = d_blob->data_sz + d_blob->redblob_sz;
++
++	temp = kmalloc(total_sz, GFP_DMA);
++	if (!temp) {
++		ret = -ENOMEM;
++		goto out;
++	}
++
++	req->mem_pointer = temp;
++	d_blob->redblob = temp;
++	d_blob->data = d_blob->redblob + d_blob->redblob_sz;
++	memcpy(d_blob->redblob, p->blob, blob_len);
++
++	ret = caam_securekey_desc_init(dev, req);
++
++	if (ret) {
++		pr_info("caam_securekey_desc_init: Failed\n");
++		goto out;
++	}
++
++	ret = sk_job_submit(dev, req);
++	if (!ret) {
++		/*Copy output to key buffer. */
++		p->key_len = d_blob->data_sz;
++		memcpy(p->key, d_blob->data, p->key_len);
++	} else {
++		ret = -EINVAL;
++	}
++
++out:
++	if (temp)
++		kfree(temp);
++	if (req)
++		kfree(req);
++	return ret;
++}
++EXPORT_SYMBOL(key_deblob);
++
++/* key_blob(struct secure_key_payload *p, enum sk_req_type encap_type,
++ *		struct device *dev)
++ * To blobify the key data to get the blob. This blob can only be seen by
++ * userspace.
++ * param[in] p pointer to the secure key payload
++ * param[in] decap_type operation to be done.
++ * param[in] dev dev Pointer to job ring device structure
++ * If operation is successful return 0, otherwise error.
++ */
++int key_blob(struct secure_key_payload *p, enum sk_req_type encap_type,
++	     struct device *dev)
++{
++	unsigned int key_len;
++	struct sk_red_blob_encap *k_blob;
++	struct sk_req *req = NULL;
++	int total_sz = 0, *temp = NULL, ret = 0;
++
++	req = kmalloc(sizeof(struct sk_req), GFP_DMA);
++	if (!req) {
++		ret = -ENOMEM;
++		goto out;
++	}
++
++	key_len = p->key_len;
++
++	req->type = encap_type;
++	k_blob = &(req->req_u.sk_red_blob_encap);
++
++	/*
++	 * Data_sz i.e. key len and the corresponding blob_len is
++	 * key_len + BLOB header size.
++	 */
++
++	k_blob->data_sz = key_len;
++	k_blob->redblob_sz = key_len + SK_BLOB_KEY_SZ + SK_BLOB_MAC_SZ;
++	total_sz = k_blob->data_sz + k_blob->redblob_sz;
++
++	temp = kmalloc(total_sz, GFP_DMA);
++	if (!temp) {
++		ret = -ENOMEM;
++		goto out;
++	}
++
++	req->mem_pointer = temp;
++	k_blob->data = temp;
++
++	k_blob->redblob = k_blob->data + k_blob->data_sz;
++	memcpy(k_blob->data, p->key, key_len);
++
++	ret = caam_securekey_desc_init(dev, req);
++
++	if (ret) {
++		pr_info("caam_securekey_desc_init failed\n");
++		goto out;
++	}
++
++	ret = sk_job_submit(dev, req);
++	if (!ret) {
++		/*Copy output to key buffer. */
++		p->blob_len = k_blob->redblob_sz;
++		memcpy(p->blob, k_blob->redblob, p->blob_len);
++	} else {
++		ret = -EINVAL;
++	}
++
++out:
++	if (temp)
++		kfree(req->mem_pointer);
++	if (req)
++		kfree(req);
++	return ret;
++
++}
++EXPORT_SYMBOL(key_blob);
+diff --git a/security/keys/securekey_desc.h b/security/keys/securekey_desc.h
+new file mode 100644
+index 00000000..0ee26e95
+--- /dev/null
++++ b/security/keys/securekey_desc.h
+@@ -0,0 +1,141 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++/*
++ * Copyright 2018 NXP
++ *
++ */
++#ifndef _SECUREKEY_DESC_H_
++#define _SECUREKEY_DESC_H_
++
++#include "compat.h"
++#include "regs.h"
++#include "intern.h"
++#include "desc.h"
++#include "desc_constr.h"
++#include "jr.h"
++#include "error.h"
++#include "pdb.h"
++
++#define SK_BLOB_KEY_SZ		32	/* Blob key size. */
++#define SK_BLOB_MAC_SZ		16	/* Blob MAC size. */
++
++/*
++ * brief defines different kinds of operations supported by this module.
++ */
++enum sk_req_type {
++	sk_get_random,
++	sk_red_blob_enc,
++	sk_red_blob_dec,
++};
++
++
++/*
++ * struct random_des
++ * param[out] rnd_data output buffer for random data.
++ */
++struct random_desc {
++	dma_addr_t rnd_data;
++};
++
++/* struct redblob_encap_desc
++ * details Structure containing dma address for redblob encapsulation.
++ * param[in] in_data input data to redblob encap descriptor.
++ * param[out] redblob output buffer for redblob.
++ */
++struct redblob_encap_desc {
++	dma_addr_t in_data;
++	dma_addr_t redblob;
++};
++
++/* struct redblob_decap_desc
++ * details Structure containing dma address for redblob decapsulation.
++ * param[in] redblob input buffer to redblob decap descriptor.
++ * param[out] out_data output data from redblob decap descriptor.
++ */
++struct redblob_decap_desc {
++	dma_addr_t redblob;
++	dma_addr_t out_data;
++};
++
++/* struct sk_desc
++ * details Structure for securekey descriptor creation.
++ * param[in] req_type operation supported.
++ * param[in] dma_u union of struct for supported operation.
++ */
++struct sk_desc {
++	u32 req_type;
++	union {
++		struct redblob_encap_desc redblob_encapdesc;
++		struct redblob_decap_desc redblob_decapdesc;
++		struct random_desc random_descp;
++	} dma_u;
++};
++
++/* struct sk_fetch_rnd_data
++ * decriptor structure containing key length.
++ */
++struct sk_fetch_rnd_data {
++	void *data;
++	size_t key_len;
++};
++
++/* struct sk_red_blob_encap
++ * details Structure containing buffer pointers for redblob encapsulation.
++ * param[in] data Input data.
++ * param[in] data_sz size of Input data.
++ * param[out] redblob output buffer for redblob.
++ * param[in] redblob_sz size of redblob.
++ */
++struct sk_red_blob_encap {
++	void *data;
++	uint32_t data_sz;
++	void *redblob;
++	uint32_t redblob_sz;
++};
++
++/* struct sk_red_blob_decap
++ * details Structure containing buffer pointers for redblob decapsulation.
++ * param[in] redblob Input redblob.
++ * param[in] redblob_sz size of redblob.
++ * param[out] data output buffer for data.
++ * param[in] data_sz size of output data.
++ */
++struct sk_red_blob_decap {
++	void *redblob;
++	uint32_t redblob_sz;
++	void *data;
++	uint32_t data_sz;
++};
++
++/* struct sk_req
++ * details Structure for securekey request creation.
++ * param[in] type operation supported.
++ * param[in] req_u union of struct for supported operation.
++ * param[out] ret return status of CAAM operation.
++ * param[in] mem_pointer memory pointer for allocated kernel memory.
++ * param[in] desc_pointer Pointer to securekey descriptor creation structure.
++ * param[in] comp struct completion object.
++ * param[in] hwdesc contains descriptor instructions.
++ */
++struct sk_req {
++	enum sk_req_type type;
++	void *arg;
++	union {
++		struct sk_red_blob_encap sk_red_blob_encap;
++		struct sk_red_blob_decap sk_red_blob_decap;
++		struct sk_fetch_rnd_data sk_fetch_rnd_data;
++	} req_u;
++	int ret;
++	void *mem_pointer;
++	void *desc_pointer;
++	struct completion comp;
++	u32 hwdesc[MAX_CAAM_DESCSIZE];
++};
++
++int caam_get_random(struct secure_key_payload *p,  enum sk_req_type fetch_rnd,
++		    struct device *dev);
++int key_blob(struct secure_key_payload *p, enum sk_req_type encap_type,
++	     struct device *dev);
++int key_deblob(struct secure_key_payload *p, enum sk_req_type decap_type,
++	       struct device *dev);
++
++#endif /*_SECUREKEY_DESC_H_*/
+-- 
+2.17.1
+
diff --git a/packages/base/any/kernels/4.9-lts/patches/series.arm64 b/packages/base/any/kernels/4.9-lts/patches/series.arm64
index b1d2fe71..fab1521b 100644
--- a/packages/base/any/kernels/4.9-lts/patches/series.arm64
+++ b/packages/base/any/kernels/4.9-lts/patches/series.arm64
@@ -14,4 +14,4 @@
 0014-staging-fsl_ppfe-eth-calculate-PFE_PKT_SIZE-with-SKB.patch
 0015-driver-mtd-ifc-increase-eccstat-array-size-for-ver-2.patch
 0016-Revert-DPAA-adjust-DPAA-to-adapt-to-Linux-4.9.patch
-
+0017-Sync-LSDK-18.09-lts-4.9-kernel-patch-to-ONL.patch
-- 
2.17.1

